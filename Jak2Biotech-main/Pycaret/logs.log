2023-10-29 20:48:51,405:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-10-29 20:48:51,405:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-10-29 20:48:51,405:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-10-29 20:48:51,405:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-10-29 23:59:39,969:WARNING:/var/folders/f1/19ck_yvd7p355g2p8r2jyzlc0000gn/T/ipykernel_28390/1690932445.py:1: FutureWarning: In a future version of pandas all arguments of concat except for the argument 'objs' will be keyword-only.
  df_train = pd.concat(x_train, x_test)

2023-10-30 00:05:12,155:INFO:PyCaret ClassificationExperiment
2023-10-30 00:05:12,155:INFO:Logging name: clf-default-name
2023-10-30 00:05:12,155:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2023-10-30 00:05:12,155:INFO:version 3.1.0
2023-10-30 00:05:12,155:INFO:Initializing setup()
2023-10-30 00:05:12,155:INFO:self.USI: 5d1e
2023-10-30 00:05:12,155:INFO:self._variable_keys: {'fold_generator', 'y_test', '_available_plots', 'logging_param', 'X', 'fold_groups_param', 'X_test', 'X_train', 'log_plots_param', 'is_multiclass', 'y', 'gpu_n_jobs_param', 'memory', 'n_jobs_param', 'pipeline', 'gpu_param', 'exp_id', 'data', 'USI', 'seed', 'exp_name_log', 'target_param', 'idx', 'y_train', 'html_param', '_ml_usecase', 'fix_imbalance', 'fold_shuffle_param'}
2023-10-30 00:05:12,155:INFO:Checking environment
2023-10-30 00:05:12,155:INFO:python_version: 3.10.8
2023-10-30 00:05:12,155:INFO:python_build: ('main', 'Nov 22 2022 08:25:13')
2023-10-30 00:05:12,155:INFO:machine: arm64
2023-10-30 00:05:12,155:INFO:platform: macOS-14.0-arm64-arm-64bit
2023-10-30 00:05:12,155:INFO:Memory: svmem(total=17179869184, available=5553815552, percent=67.7, used=7574798336, free=118095872, active=5692702720, inactive=5429526528, wired=1882095616)
2023-10-30 00:05:12,155:INFO:Physical Core: 10
2023-10-30 00:05:12,155:INFO:Logical Core: 10
2023-10-30 00:05:12,155:INFO:Checking libraries
2023-10-30 00:05:12,155:INFO:System:
2023-10-30 00:05:12,155:INFO:    python: 3.10.8 | packaged by conda-forge | (main, Nov 22 2022, 08:25:13) [Clang 14.0.6 ]
2023-10-30 00:05:12,155:INFO:executable: /Users/tanmaysharma/projects/Jak2Biotech/.venv/bin/python
2023-10-30 00:05:12,155:INFO:   machine: macOS-14.0-arm64-arm-64bit
2023-10-30 00:05:12,155:INFO:PyCaret required dependencies:
2023-10-30 00:05:13,009:INFO:                 pip: 23.3.1
2023-10-30 00:05:13,009:INFO:          setuptools: 63.2.0
2023-10-30 00:05:13,009:INFO:             pycaret: 3.1.0
2023-10-30 00:05:13,009:INFO:             IPython: 8.16.1
2023-10-30 00:05:13,009:INFO:          ipywidgets: 8.1.1
2023-10-30 00:05:13,009:INFO:                tqdm: 4.66.1
2023-10-30 00:05:13,009:INFO:               numpy: 1.23.5
2023-10-30 00:05:13,009:INFO:              pandas: 1.5.3
2023-10-30 00:05:13,009:INFO:              jinja2: 3.1.2
2023-10-30 00:05:13,009:INFO:               scipy: 1.10.1
2023-10-30 00:05:13,009:INFO:              joblib: 1.3.2
2023-10-30 00:05:13,009:INFO:             sklearn: 1.2.2
2023-10-30 00:05:13,009:INFO:                pyod: 1.1.1
2023-10-30 00:05:13,009:INFO:            imblearn: 0.11.0
2023-10-30 00:05:13,009:INFO:   category_encoders: 2.6.2
2023-10-30 00:05:13,009:INFO:            lightgbm: 4.1.0
2023-10-30 00:05:13,009:INFO:               numba: 0.58.1
2023-10-30 00:05:13,009:INFO:            requests: 2.31.0
2023-10-30 00:05:13,009:INFO:          matplotlib: 3.8.0
2023-10-30 00:05:13,009:INFO:          scikitplot: 0.3.7
2023-10-30 00:05:13,009:INFO:         yellowbrick: 1.5
2023-10-30 00:05:13,009:INFO:              plotly: 5.18.0
2023-10-30 00:05:13,009:INFO:    plotly-resampler: Not installed
2023-10-30 00:05:13,009:INFO:             kaleido: 0.2.1
2023-10-30 00:05:13,009:INFO:           schemdraw: 0.15
2023-10-30 00:05:13,009:INFO:         statsmodels: 0.14.0
2023-10-30 00:05:13,009:INFO:              sktime: 0.21.1
2023-10-30 00:05:13,009:INFO:               tbats: 1.1.3
2023-10-30 00:05:13,009:INFO:            pmdarima: 2.0.4
2023-10-30 00:05:13,009:INFO:              psutil: 5.9.6
2023-10-30 00:05:13,009:INFO:          markupsafe: 2.1.3
2023-10-30 00:05:13,009:INFO:             pickle5: Not installed
2023-10-30 00:05:13,009:INFO:         cloudpickle: 3.0.0
2023-10-30 00:05:13,009:INFO:         deprecation: 2.1.0
2023-10-30 00:05:13,009:INFO:              xxhash: 3.4.1
2023-10-30 00:05:13,009:INFO:           wurlitzer: 3.0.3
2023-10-30 00:05:13,009:INFO:PyCaret optional dependencies:
2023-10-30 00:05:13,018:INFO:                shap: Not installed
2023-10-30 00:05:13,018:INFO:           interpret: Not installed
2023-10-30 00:05:13,018:INFO:                umap: Not installed
2023-10-30 00:05:13,018:INFO:     ydata_profiling: Not installed
2023-10-30 00:05:13,018:INFO:  explainerdashboard: Not installed
2023-10-30 00:05:13,018:INFO:             autoviz: Not installed
2023-10-30 00:05:13,018:INFO:           fairlearn: Not installed
2023-10-30 00:05:13,018:INFO:          deepchecks: Not installed
2023-10-30 00:05:13,018:INFO:             xgboost: Not installed
2023-10-30 00:05:13,018:INFO:            catboost: Not installed
2023-10-30 00:05:13,018:INFO:              kmodes: Not installed
2023-10-30 00:05:13,018:INFO:             mlxtend: Not installed
2023-10-30 00:05:13,018:INFO:       statsforecast: Not installed
2023-10-30 00:05:13,018:INFO:        tune_sklearn: Not installed
2023-10-30 00:05:13,018:INFO:                 ray: Not installed
2023-10-30 00:05:13,018:INFO:            hyperopt: Not installed
2023-10-30 00:05:13,018:INFO:              optuna: Not installed
2023-10-30 00:05:13,018:INFO:               skopt: Not installed
2023-10-30 00:05:13,018:INFO:              mlflow: Not installed
2023-10-30 00:05:13,018:INFO:              gradio: Not installed
2023-10-30 00:05:13,018:INFO:             fastapi: Not installed
2023-10-30 00:05:13,018:INFO:             uvicorn: Not installed
2023-10-30 00:05:13,018:INFO:              m2cgen: Not installed
2023-10-30 00:05:13,018:INFO:           evidently: Not installed
2023-10-30 00:05:13,018:INFO:               fugue: Not installed
2023-10-30 00:05:13,018:INFO:           streamlit: Not installed
2023-10-30 00:05:13,018:INFO:             prophet: Not installed
2023-10-30 00:05:13,018:INFO:None
2023-10-30 00:05:13,018:INFO:Set up data.
2023-10-30 00:05:13,071:INFO:Set up folding strategy.
2023-10-30 00:05:13,071:INFO:Set up train/test split.
2023-10-30 00:05:13,087:INFO:Set up index.
2023-10-30 00:05:13,088:INFO:Assigning column types.
2023-10-30 00:05:13,097:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2023-10-30 00:05:13,116:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-10-30 00:05:13,119:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-10-30 00:05:13,135:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-10-30 00:05:13,135:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-10-30 00:05:13,154:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-10-30 00:05:13,154:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-10-30 00:05:13,166:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-10-30 00:05:13,166:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-10-30 00:05:13,166:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2023-10-30 00:05:13,186:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-10-30 00:05:13,198:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-10-30 00:05:13,198:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-10-30 00:05:13,218:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-10-30 00:05:13,230:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-10-30 00:05:13,230:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-10-30 00:05:13,230:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2023-10-30 00:05:13,261:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-10-30 00:05:13,261:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-10-30 00:05:13,291:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-10-30 00:05:13,291:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-10-30 00:05:13,294:INFO:Preparing preprocessing pipeline...
2023-10-30 00:05:13,295:INFO:Set up simple imputation.
2023-10-30 00:05:13,296:INFO:Set up column name cleaning.
2023-10-30 00:05:13,360:INFO:Finished creating preprocessing pipeline.
2023-10-30 00:05:13,364:INFO:Pipeline: Pipeline(memory=FastMemory(location=/var/folders/f1/19ck_yvd7p355g2p8r2jyzlc0000gn/T/joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['GATS1c', 'SlogP_VSA2', 'SLogP',
                                             'MDEN-23', 'MINdsCH', 'MINssCH2',
                                             'GATS1s', 'GATS2m', 'SaaO',
                                             'BCUTZ-1h', 'SdsN', 'MINaaN',
                                             'MAXssCH2', 'MATS1s', 'GATS5d',
                                             'Xch-5d', 'SMR_VSA7',
                                             'ETA_dEpsilon_B', 'VSA_ESta...
                 TransformerWrapper(exclude=None, include=[],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent',
                                                              verbose='deprecated'))),
                ('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+')))],
         verbose=False)
2023-10-30 00:05:13,364:INFO:Creating final display dataframe.
2023-10-30 00:05:13,544:INFO:Setup _display_container:                     Description             Value
0                    Session id              8003
1                        Target               cls
2                   Target type            Binary
3           Original data shape       (9440, 298)
4        Transformed data shape       (9440, 298)
5   Transformed train set shape       (7552, 298)
6    Transformed test set shape       (1888, 298)
7              Numeric features               297
8      Rows with missing values            100.0%
9                    Preprocess              True
10              Imputation type            simple
11           Numeric imputation              mean
12       Categorical imputation              mode
13               Fold Generator   StratifiedKFold
14                  Fold Number                10
15                     CPU Jobs                -1
16                      Use GPU             False
17               Log Experiment             False
18              Experiment Name  clf-default-name
19                          USI              5d1e
2023-10-30 00:05:13,579:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-10-30 00:05:13,580:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-10-30 00:05:13,611:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-10-30 00:05:13,611:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-10-30 00:05:13,612:INFO:setup() successfully completed in 1.46s...............
2023-10-30 00:07:56,610:INFO:Initializing compare_models()
2023-10-30 00:07:56,610:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x2a35d5cc0>, include=None, fold=None, round=4, cross_validation=True, sort=Prec, n_select=5, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x2a35d5cc0>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'Prec', 'n_select': 5, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>}, exclude=None)
2023-10-30 00:07:56,611:INFO:Checking exceptions
2023-10-30 00:08:33,046:INFO:PyCaret ClassificationExperiment
2023-10-30 00:08:33,047:INFO:Logging name: clf-default-name
2023-10-30 00:08:33,047:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2023-10-30 00:08:33,047:INFO:version 3.1.0
2023-10-30 00:08:33,047:INFO:Initializing setup()
2023-10-30 00:08:33,047:INFO:self.USI: b73e
2023-10-30 00:08:33,047:INFO:self._variable_keys: {'fold_generator', 'y_test', '_available_plots', 'logging_param', 'X', 'fold_groups_param', 'X_test', 'X_train', 'log_plots_param', 'is_multiclass', 'y', 'gpu_n_jobs_param', 'memory', 'n_jobs_param', 'pipeline', 'gpu_param', 'exp_id', 'data', 'USI', 'seed', 'exp_name_log', 'target_param', 'idx', 'y_train', 'html_param', '_ml_usecase', 'fix_imbalance', 'fold_shuffle_param'}
2023-10-30 00:08:33,047:INFO:Checking environment
2023-10-30 00:08:33,047:INFO:python_version: 3.10.8
2023-10-30 00:08:33,047:INFO:python_build: ('main', 'Nov 22 2022 08:25:13')
2023-10-30 00:08:33,047:INFO:machine: arm64
2023-10-30 00:08:33,047:INFO:platform: macOS-14.0-arm64-arm-64bit
2023-10-30 00:08:33,048:INFO:Memory: svmem(total=17179869184, available=5623021568, percent=67.3, used=7537590272, free=55590912, active=5584044032, inactive=5494833152, wired=1953546240)
2023-10-30 00:08:33,048:INFO:Physical Core: 10
2023-10-30 00:08:33,048:INFO:Logical Core: 10
2023-10-30 00:08:33,048:INFO:Checking libraries
2023-10-30 00:08:33,048:INFO:System:
2023-10-30 00:08:33,048:INFO:    python: 3.10.8 | packaged by conda-forge | (main, Nov 22 2022, 08:25:13) [Clang 14.0.6 ]
2023-10-30 00:08:33,048:INFO:executable: /Users/tanmaysharma/projects/Jak2Biotech/.venv/bin/python
2023-10-30 00:08:33,048:INFO:   machine: macOS-14.0-arm64-arm-64bit
2023-10-30 00:08:33,048:INFO:PyCaret required dependencies:
2023-10-30 00:08:33,049:INFO:                 pip: 23.3.1
2023-10-30 00:08:33,049:INFO:          setuptools: 63.2.0
2023-10-30 00:08:33,049:INFO:             pycaret: 3.1.0
2023-10-30 00:08:33,049:INFO:             IPython: 8.16.1
2023-10-30 00:08:33,049:INFO:          ipywidgets: 8.1.1
2023-10-30 00:08:33,049:INFO:                tqdm: 4.66.1
2023-10-30 00:08:33,049:INFO:               numpy: 1.23.5
2023-10-30 00:08:33,049:INFO:              pandas: 1.5.3
2023-10-30 00:08:33,049:INFO:              jinja2: 3.1.2
2023-10-30 00:08:33,049:INFO:               scipy: 1.10.1
2023-10-30 00:08:33,049:INFO:              joblib: 1.3.2
2023-10-30 00:08:33,049:INFO:             sklearn: 1.2.2
2023-10-30 00:08:33,049:INFO:                pyod: 1.1.1
2023-10-30 00:08:33,049:INFO:            imblearn: 0.11.0
2023-10-30 00:08:33,049:INFO:   category_encoders: 2.6.2
2023-10-30 00:08:33,049:INFO:            lightgbm: 4.1.0
2023-10-30 00:08:33,049:INFO:               numba: 0.58.1
2023-10-30 00:08:33,049:INFO:            requests: 2.31.0
2023-10-30 00:08:33,050:INFO:          matplotlib: 3.8.0
2023-10-30 00:08:33,050:INFO:          scikitplot: 0.3.7
2023-10-30 00:08:33,050:INFO:         yellowbrick: 1.5
2023-10-30 00:08:33,050:INFO:              plotly: 5.18.0
2023-10-30 00:08:33,050:INFO:    plotly-resampler: Not installed
2023-10-30 00:08:33,050:INFO:             kaleido: 0.2.1
2023-10-30 00:08:33,050:INFO:           schemdraw: 0.15
2023-10-30 00:08:33,050:INFO:         statsmodels: 0.14.0
2023-10-30 00:08:33,050:INFO:              sktime: 0.21.1
2023-10-30 00:08:33,050:INFO:               tbats: 1.1.3
2023-10-30 00:08:33,050:INFO:            pmdarima: 2.0.4
2023-10-30 00:08:33,050:INFO:              psutil: 5.9.6
2023-10-30 00:08:33,050:INFO:          markupsafe: 2.1.3
2023-10-30 00:08:33,050:INFO:             pickle5: Not installed
2023-10-30 00:08:33,050:INFO:         cloudpickle: 3.0.0
2023-10-30 00:08:33,050:INFO:         deprecation: 2.1.0
2023-10-30 00:08:33,050:INFO:              xxhash: 3.4.1
2023-10-30 00:08:33,050:INFO:           wurlitzer: 3.0.3
2023-10-30 00:08:33,050:INFO:PyCaret optional dependencies:
2023-10-30 00:08:33,050:INFO:                shap: Not installed
2023-10-30 00:08:33,050:INFO:           interpret: Not installed
2023-10-30 00:08:33,050:INFO:                umap: Not installed
2023-10-30 00:08:33,050:INFO:     ydata_profiling: Not installed
2023-10-30 00:08:33,051:INFO:  explainerdashboard: Not installed
2023-10-30 00:08:33,051:INFO:             autoviz: Not installed
2023-10-30 00:08:33,051:INFO:           fairlearn: Not installed
2023-10-30 00:08:33,051:INFO:          deepchecks: Not installed
2023-10-30 00:08:33,051:INFO:             xgboost: Not installed
2023-10-30 00:08:33,051:INFO:            catboost: Not installed
2023-10-30 00:08:33,051:INFO:              kmodes: Not installed
2023-10-30 00:08:33,051:INFO:             mlxtend: Not installed
2023-10-30 00:08:33,051:INFO:       statsforecast: Not installed
2023-10-30 00:08:33,051:INFO:        tune_sklearn: Not installed
2023-10-30 00:08:33,051:INFO:                 ray: Not installed
2023-10-30 00:08:33,051:INFO:            hyperopt: Not installed
2023-10-30 00:08:33,051:INFO:              optuna: Not installed
2023-10-30 00:08:33,051:INFO:               skopt: Not installed
2023-10-30 00:08:33,051:INFO:              mlflow: Not installed
2023-10-30 00:08:33,051:INFO:              gradio: Not installed
2023-10-30 00:08:33,051:INFO:             fastapi: Not installed
2023-10-30 00:08:33,051:INFO:             uvicorn: Not installed
2023-10-30 00:08:33,051:INFO:              m2cgen: Not installed
2023-10-30 00:08:33,051:INFO:           evidently: Not installed
2023-10-30 00:08:33,051:INFO:               fugue: Not installed
2023-10-30 00:08:33,051:INFO:           streamlit: Not installed
2023-10-30 00:08:33,051:INFO:             prophet: Not installed
2023-10-30 00:08:33,051:INFO:None
2023-10-30 00:08:33,052:INFO:Set up data.
2023-10-30 00:08:33,180:INFO:Set up folding strategy.
2023-10-30 00:08:33,180:INFO:Set up train/test split.
2023-10-30 00:08:33,204:INFO:Set up index.
2023-10-30 00:08:33,205:INFO:Assigning column types.
2023-10-30 00:08:33,216:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2023-10-30 00:08:33,237:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-10-30 00:08:33,238:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-10-30 00:08:33,252:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-10-30 00:08:33,252:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-10-30 00:08:33,274:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-10-30 00:08:33,274:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-10-30 00:08:33,287:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-10-30 00:08:33,287:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-10-30 00:08:33,287:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2023-10-30 00:08:33,308:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-10-30 00:08:33,321:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-10-30 00:08:33,321:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-10-30 00:08:33,342:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-10-30 00:08:33,355:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-10-30 00:08:33,356:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-10-30 00:08:33,356:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2023-10-30 00:08:33,389:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-10-30 00:08:33,389:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-10-30 00:08:33,422:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-10-30 00:08:33,422:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-10-30 00:08:33,424:INFO:Preparing preprocessing pipeline...
2023-10-30 00:08:33,425:INFO:Set up simple imputation.
2023-10-30 00:08:33,426:INFO:Set up column name cleaning.
2023-10-30 00:08:33,538:INFO:Finished creating preprocessing pipeline.
2023-10-30 00:08:33,557:INFO:Pipeline: Pipeline(memory=FastMemory(location=/var/folders/f1/19ck_yvd7p355g2p8r2jyzlc0000gn/T/joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['GATS1c', 'SlogP_VSA2', 'SLogP',
                                             'MDEN-23', 'MINdsCH', 'MINssCH2',
                                             'GATS1s', 'GATS2m', 'SaaO',
                                             'BCUTZ-1h', 'SdsN', 'MINaaN',
                                             'MAXssCH2', 'MATS1s', 'GATS5d',
                                             'Xch-5d', 'SMR_VSA7',
                                             'ETA_dEpsilon_B', 'VSA_ESta...
                 TransformerWrapper(exclude=None, include=[],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent',
                                                              verbose='deprecated'))),
                ('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+')))],
         verbose=False)
2023-10-30 00:08:33,557:INFO:Creating final display dataframe.
2023-10-30 00:08:33,984:INFO:Setup _display_container:                     Description             Value
0                    Session id               623
1                        Target               cls
2                   Target type            Binary
3           Original data shape       (9440, 298)
4        Transformed data shape       (9440, 298)
5   Transformed train set shape       (7552, 298)
6    Transformed test set shape       (1888, 298)
7              Numeric features               297
8      Rows with missing values            100.0%
9                    Preprocess              True
10              Imputation type            simple
11           Numeric imputation              mean
12       Categorical imputation              mode
13               Fold Generator   StratifiedKFold
14                  Fold Number                10
15                     CPU Jobs                -1
16                      Use GPU             False
17               Log Experiment             False
18              Experiment Name  clf-default-name
19                          USI              b73e
2023-10-30 00:08:34,034:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-10-30 00:08:34,034:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-10-30 00:08:34,068:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-10-30 00:08:34,069:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-10-30 00:08:34,070:INFO:setup() successfully completed in 1.03s...............
2023-10-30 00:08:34,076:INFO:Initializing compare_models()
2023-10-30 00:08:34,076:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x28f7762f0>, include=None, fold=None, round=4, cross_validation=True, sort=Prec, n_select=5, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x28f7762f0>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'Prec', 'n_select': 5, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>}, exclude=None)
2023-10-30 00:08:34,076:INFO:Checking exceptions
2023-10-30 00:08:58,904:INFO:Initializing compare_models()
2023-10-30 00:08:58,904:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x28f7762f0>, include=None, fold=None, round=4, cross_validation=True, sort=Precision, n_select=5, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x28f7762f0>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'Precision', 'n_select': 5, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>}, exclude=None)
2023-10-30 00:08:58,904:INFO:Checking exceptions
2023-10-30 00:08:58,913:INFO:Preparing display monitor
2023-10-30 00:08:58,968:INFO:Initializing Logistic Regression
2023-10-30 00:08:58,969:INFO:Total runtime is 4.780292510986328e-06 minutes
2023-10-30 00:08:58,973:INFO:SubProcess create_model() called ==================================
2023-10-30 00:08:58,974:INFO:Initializing create_model()
2023-10-30 00:08:58,975:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x28f7762f0>, estimator=lr, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x2a99610f0>, model_only=True, return_train_score=False, kwargs={})
2023-10-30 00:08:58,975:INFO:Checking exceptions
2023-10-30 00:08:58,975:INFO:Importing libraries
2023-10-30 00:08:58,976:INFO:Copying training dataset
2023-10-30 00:08:58,996:INFO:Defining folds
2023-10-30 00:08:58,996:INFO:Declaring metric variables
2023-10-30 00:08:59,003:INFO:Importing untrained model
2023-10-30 00:08:59,012:INFO:Logistic Regression Imported successfully
2023-10-30 00:08:59,023:INFO:Starting cross validation
2023-10-30 00:08:59,029:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-10-30 00:50:14,636:INFO:Initializing ensemble_model()
2023-10-30 00:50:14,636:INFO:ensemble_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x28f7762f0>, estimator=<pycaret.classification.oop.ClassificationExperiment object at 0x28f7762f0>, method=Boosting, fold=None, n_estimators=10, round=4, choose_better=False, optimize=Accuracy, fit_kwargs=None, groups=None, probability_threshold=None, verbose=True, return_train_score=False)
2023-10-30 00:50:14,636:INFO:Checking exceptions
2023-10-30 00:51:07,006:INFO:Initializing create_model()
2023-10-30 00:51:07,007:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x28f7762f0>, estimator=dt, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-10-30 00:51:07,007:INFO:Checking exceptions
2023-10-30 00:51:07,028:INFO:Importing libraries
2023-10-30 00:51:07,028:INFO:Copying training dataset
2023-10-30 00:51:07,059:INFO:Defining folds
2023-10-30 00:51:07,059:INFO:Declaring metric variables
2023-10-30 00:51:07,064:INFO:Importing untrained model
2023-10-30 00:51:07,068:INFO:Decision Tree Classifier Imported successfully
2023-10-30 00:51:07,079:INFO:Starting cross validation
2023-10-30 00:51:07,085:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-10-30 00:52:09,703:INFO:Initializing create_model()
2023-10-30 00:52:09,704:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x28f7762f0>, estimator=dt, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-10-30 00:52:09,704:INFO:Checking exceptions
2023-10-30 00:52:09,725:INFO:Importing libraries
2023-10-30 00:52:09,725:INFO:Copying training dataset
2023-10-30 00:52:09,752:INFO:Defining folds
2023-10-30 00:52:09,753:INFO:Declaring metric variables
2023-10-30 00:52:09,757:INFO:Importing untrained model
2023-10-30 00:52:09,763:INFO:Decision Tree Classifier Imported successfully
2023-10-30 00:52:09,772:INFO:Starting cross validation
2023-10-30 00:52:09,778:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-10-30 00:55:22,745:INFO:PyCaret ClassificationExperiment
2023-10-30 00:55:22,745:INFO:Logging name: clf-default-name
2023-10-30 00:55:22,745:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2023-10-30 00:55:22,745:INFO:version 3.1.0
2023-10-30 00:55:22,745:INFO:Initializing setup()
2023-10-30 00:55:22,745:INFO:self.USI: e67f
2023-10-30 00:55:22,745:INFO:self._variable_keys: {'fold_generator', 'y_test', '_available_plots', 'logging_param', 'X', 'fold_groups_param', 'X_test', 'X_train', 'log_plots_param', 'is_multiclass', 'y', 'gpu_n_jobs_param', 'memory', 'n_jobs_param', 'pipeline', 'gpu_param', 'exp_id', 'data', 'USI', 'seed', 'exp_name_log', 'target_param', 'idx', 'y_train', 'html_param', '_ml_usecase', 'fix_imbalance', 'fold_shuffle_param'}
2023-10-30 00:55:22,745:INFO:Checking environment
2023-10-30 00:55:22,746:INFO:python_version: 3.10.8
2023-10-30 00:55:22,746:INFO:python_build: ('main', 'Nov 22 2022 08:25:13')
2023-10-30 00:55:22,746:INFO:machine: arm64
2023-10-30 00:55:22,746:INFO:platform: macOS-14.0-arm64-arm-64bit
2023-10-30 00:55:22,746:INFO:Memory: svmem(total=17179869184, available=4979441664, percent=71.0, used=6907887616, free=185057280, active=4802265088, inactive=4779589632, wired=2105622528)
2023-10-30 00:55:22,746:INFO:Physical Core: 10
2023-10-30 00:55:22,746:INFO:Logical Core: 10
2023-10-30 00:55:22,746:INFO:Checking libraries
2023-10-30 00:55:22,746:INFO:System:
2023-10-30 00:55:22,746:INFO:    python: 3.10.8 | packaged by conda-forge | (main, Nov 22 2022, 08:25:13) [Clang 14.0.6 ]
2023-10-30 00:55:22,746:INFO:executable: /Users/tanmaysharma/projects/Jak2Biotech/.venv/bin/python
2023-10-30 00:55:22,746:INFO:   machine: macOS-14.0-arm64-arm-64bit
2023-10-30 00:55:22,746:INFO:PyCaret required dependencies:
2023-10-30 00:55:22,746:INFO:                 pip: 23.3.1
2023-10-30 00:55:22,746:INFO:          setuptools: 63.2.0
2023-10-30 00:55:22,746:INFO:             pycaret: 3.1.0
2023-10-30 00:55:22,747:INFO:             IPython: 8.16.1
2023-10-30 00:55:22,747:INFO:          ipywidgets: 8.1.1
2023-10-30 00:55:22,747:INFO:                tqdm: 4.66.1
2023-10-30 00:55:22,747:INFO:               numpy: 1.23.5
2023-10-30 00:55:22,747:INFO:              pandas: 1.5.3
2023-10-30 00:55:22,747:INFO:              jinja2: 3.1.2
2023-10-30 00:55:22,747:INFO:               scipy: 1.10.1
2023-10-30 00:55:22,747:INFO:              joblib: 1.3.2
2023-10-30 00:55:22,747:INFO:             sklearn: 1.2.2
2023-10-30 00:55:22,747:INFO:                pyod: 1.1.1
2023-10-30 00:55:22,747:INFO:            imblearn: 0.11.0
2023-10-30 00:55:22,747:INFO:   category_encoders: 2.6.2
2023-10-30 00:55:22,747:INFO:            lightgbm: 4.1.0
2023-10-30 00:55:22,747:INFO:               numba: 0.58.1
2023-10-30 00:55:22,747:INFO:            requests: 2.31.0
2023-10-30 00:55:22,747:INFO:          matplotlib: 3.8.0
2023-10-30 00:55:22,747:INFO:          scikitplot: 0.3.7
2023-10-30 00:55:22,747:INFO:         yellowbrick: 1.5
2023-10-30 00:55:22,747:INFO:              plotly: 5.18.0
2023-10-30 00:55:22,747:INFO:    plotly-resampler: Not installed
2023-10-30 00:55:22,747:INFO:             kaleido: 0.2.1
2023-10-30 00:55:22,747:INFO:           schemdraw: 0.15
2023-10-30 00:55:22,747:INFO:         statsmodels: 0.14.0
2023-10-30 00:55:22,748:INFO:              sktime: 0.21.1
2023-10-30 00:55:22,748:INFO:               tbats: 1.1.3
2023-10-30 00:55:22,748:INFO:            pmdarima: 2.0.4
2023-10-30 00:55:22,748:INFO:              psutil: 5.9.6
2023-10-30 00:55:22,748:INFO:          markupsafe: 2.1.3
2023-10-30 00:55:22,748:INFO:             pickle5: Not installed
2023-10-30 00:55:22,748:INFO:         cloudpickle: 3.0.0
2023-10-30 00:55:22,748:INFO:         deprecation: 2.1.0
2023-10-30 00:55:22,748:INFO:              xxhash: 3.4.1
2023-10-30 00:55:22,748:INFO:           wurlitzer: 3.0.3
2023-10-30 00:55:22,748:INFO:PyCaret optional dependencies:
2023-10-30 00:55:22,748:INFO:                shap: Not installed
2023-10-30 00:55:22,748:INFO:           interpret: Not installed
2023-10-30 00:55:22,748:INFO:                umap: Not installed
2023-10-30 00:55:22,748:INFO:     ydata_profiling: Not installed
2023-10-30 00:55:22,748:INFO:  explainerdashboard: Not installed
2023-10-30 00:55:22,748:INFO:             autoviz: Not installed
2023-10-30 00:55:22,748:INFO:           fairlearn: Not installed
2023-10-30 00:55:22,748:INFO:          deepchecks: Not installed
2023-10-30 00:55:22,748:INFO:             xgboost: Not installed
2023-10-30 00:55:22,748:INFO:            catboost: Not installed
2023-10-30 00:55:22,748:INFO:              kmodes: Not installed
2023-10-30 00:55:22,749:INFO:             mlxtend: Not installed
2023-10-30 00:55:22,749:INFO:       statsforecast: Not installed
2023-10-30 00:55:22,749:INFO:        tune_sklearn: Not installed
2023-10-30 00:55:22,749:INFO:                 ray: Not installed
2023-10-30 00:55:22,749:INFO:            hyperopt: Not installed
2023-10-30 00:55:22,749:INFO:              optuna: Not installed
2023-10-30 00:55:22,749:INFO:               skopt: Not installed
2023-10-30 00:55:22,749:INFO:              mlflow: Not installed
2023-10-30 00:55:22,749:INFO:              gradio: Not installed
2023-10-30 00:55:22,749:INFO:             fastapi: Not installed
2023-10-30 00:55:22,749:INFO:             uvicorn: Not installed
2023-10-30 00:55:22,749:INFO:              m2cgen: Not installed
2023-10-30 00:55:22,749:INFO:           evidently: Not installed
2023-10-30 00:55:22,749:INFO:               fugue: Not installed
2023-10-30 00:55:22,749:INFO:           streamlit: Not installed
2023-10-30 00:55:22,749:INFO:             prophet: Not installed
2023-10-30 00:55:22,749:INFO:None
2023-10-30 00:55:22,749:INFO:Set up data.
2023-10-30 00:59:25,611:INFO:PyCaret ClassificationExperiment
2023-10-30 00:59:25,611:INFO:Logging name: clf-default-name
2023-10-30 00:59:25,611:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2023-10-30 00:59:25,611:INFO:version 3.1.0
2023-10-30 00:59:25,611:INFO:Initializing setup()
2023-10-30 00:59:25,611:INFO:self.USI: 54bc
2023-10-30 00:59:25,611:INFO:self._variable_keys: {'fold_generator', 'y_test', '_available_plots', 'logging_param', 'X', 'fold_groups_param', 'X_test', 'X_train', 'log_plots_param', 'is_multiclass', 'y', 'gpu_n_jobs_param', 'memory', 'n_jobs_param', 'pipeline', 'gpu_param', 'exp_id', 'data', 'USI', 'seed', 'exp_name_log', 'target_param', 'idx', 'y_train', 'html_param', '_ml_usecase', 'fix_imbalance', 'fold_shuffle_param'}
2023-10-30 00:59:25,611:INFO:Checking environment
2023-10-30 00:59:25,611:INFO:python_version: 3.10.8
2023-10-30 00:59:25,611:INFO:python_build: ('main', 'Nov 22 2022 08:25:13')
2023-10-30 00:59:25,611:INFO:machine: arm64
2023-10-30 00:59:25,611:INFO:platform: macOS-14.0-arm64-arm-64bit
2023-10-30 00:59:25,612:INFO:Memory: svmem(total=17179869184, available=4709007360, percent=72.6, used=6845546496, free=61325312, active=4662706176, inactive=4635639808, wired=2182840320)
2023-10-30 00:59:25,612:INFO:Physical Core: 10
2023-10-30 00:59:25,612:INFO:Logical Core: 10
2023-10-30 00:59:25,612:INFO:Checking libraries
2023-10-30 00:59:25,612:INFO:System:
2023-10-30 00:59:25,612:INFO:    python: 3.10.8 | packaged by conda-forge | (main, Nov 22 2022, 08:25:13) [Clang 14.0.6 ]
2023-10-30 00:59:25,612:INFO:executable: /Users/tanmaysharma/projects/Jak2Biotech/.venv/bin/python
2023-10-30 00:59:25,612:INFO:   machine: macOS-14.0-arm64-arm-64bit
2023-10-30 00:59:25,612:INFO:PyCaret required dependencies:
2023-10-30 00:59:25,612:INFO:                 pip: 23.3.1
2023-10-30 00:59:25,612:INFO:          setuptools: 63.2.0
2023-10-30 00:59:25,612:INFO:             pycaret: 3.1.0
2023-10-30 00:59:25,612:INFO:             IPython: 8.16.1
2023-10-30 00:59:25,612:INFO:          ipywidgets: 8.1.1
2023-10-30 00:59:25,612:INFO:                tqdm: 4.66.1
2023-10-30 00:59:25,612:INFO:               numpy: 1.23.5
2023-10-30 00:59:25,612:INFO:              pandas: 1.5.3
2023-10-30 00:59:25,612:INFO:              jinja2: 3.1.2
2023-10-30 00:59:25,612:INFO:               scipy: 1.10.1
2023-10-30 00:59:25,613:INFO:              joblib: 1.3.2
2023-10-30 00:59:25,613:INFO:             sklearn: 1.2.2
2023-10-30 00:59:25,613:INFO:                pyod: 1.1.1
2023-10-30 00:59:25,613:INFO:            imblearn: 0.11.0
2023-10-30 00:59:25,613:INFO:   category_encoders: 2.6.2
2023-10-30 00:59:25,613:INFO:            lightgbm: 4.1.0
2023-10-30 00:59:25,613:INFO:               numba: 0.58.1
2023-10-30 00:59:25,613:INFO:            requests: 2.31.0
2023-10-30 00:59:25,613:INFO:          matplotlib: 3.8.0
2023-10-30 00:59:25,613:INFO:          scikitplot: 0.3.7
2023-10-30 00:59:25,613:INFO:         yellowbrick: 1.5
2023-10-30 00:59:25,613:INFO:              plotly: 5.18.0
2023-10-30 00:59:25,613:INFO:    plotly-resampler: Not installed
2023-10-30 00:59:25,613:INFO:             kaleido: 0.2.1
2023-10-30 00:59:25,613:INFO:           schemdraw: 0.15
2023-10-30 00:59:25,613:INFO:         statsmodels: 0.14.0
2023-10-30 00:59:25,613:INFO:              sktime: 0.21.1
2023-10-30 00:59:25,613:INFO:               tbats: 1.1.3
2023-10-30 00:59:25,613:INFO:            pmdarima: 2.0.4
2023-10-30 00:59:25,613:INFO:              psutil: 5.9.6
2023-10-30 00:59:25,613:INFO:          markupsafe: 2.1.3
2023-10-30 00:59:25,613:INFO:             pickle5: Not installed
2023-10-30 00:59:25,613:INFO:         cloudpickle: 3.0.0
2023-10-30 00:59:25,613:INFO:         deprecation: 2.1.0
2023-10-30 00:59:25,614:INFO:              xxhash: 3.4.1
2023-10-30 00:59:25,614:INFO:           wurlitzer: 3.0.3
2023-10-30 00:59:25,614:INFO:PyCaret optional dependencies:
2023-10-30 00:59:25,614:INFO:                shap: Not installed
2023-10-30 00:59:25,614:INFO:           interpret: Not installed
2023-10-30 00:59:25,614:INFO:                umap: Not installed
2023-10-30 00:59:25,614:INFO:     ydata_profiling: Not installed
2023-10-30 00:59:25,614:INFO:  explainerdashboard: Not installed
2023-10-30 00:59:25,614:INFO:             autoviz: Not installed
2023-10-30 00:59:25,614:INFO:           fairlearn: Not installed
2023-10-30 00:59:25,614:INFO:          deepchecks: Not installed
2023-10-30 00:59:25,614:INFO:             xgboost: Not installed
2023-10-30 00:59:25,614:INFO:            catboost: Not installed
2023-10-30 00:59:25,614:INFO:              kmodes: Not installed
2023-10-30 00:59:25,614:INFO:             mlxtend: Not installed
2023-10-30 00:59:25,614:INFO:       statsforecast: Not installed
2023-10-30 00:59:25,614:INFO:        tune_sklearn: Not installed
2023-10-30 00:59:25,614:INFO:                 ray: Not installed
2023-10-30 00:59:25,614:INFO:            hyperopt: Not installed
2023-10-30 00:59:25,614:INFO:              optuna: Not installed
2023-10-30 00:59:25,614:INFO:               skopt: Not installed
2023-10-30 00:59:25,614:INFO:              mlflow: Not installed
2023-10-30 00:59:25,615:INFO:              gradio: Not installed
2023-10-30 00:59:25,615:INFO:             fastapi: Not installed
2023-10-30 00:59:25,615:INFO:             uvicorn: Not installed
2023-10-30 00:59:25,615:INFO:              m2cgen: Not installed
2023-10-30 00:59:25,615:INFO:           evidently: Not installed
2023-10-30 00:59:25,615:INFO:               fugue: Not installed
2023-10-30 00:59:25,615:INFO:           streamlit: Not installed
2023-10-30 00:59:25,615:INFO:             prophet: Not installed
2023-10-30 00:59:25,615:INFO:None
2023-10-30 00:59:25,615:INFO:Set up data.
2023-10-30 00:59:25,730:INFO:Set up folding strategy.
2023-10-30 00:59:25,731:INFO:Set up train/test split.
2023-10-30 00:59:25,748:INFO:Set up index.
2023-10-30 00:59:25,748:INFO:Assigning column types.
2023-10-30 00:59:25,755:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2023-10-30 00:59:25,776:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-10-30 00:59:25,776:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-10-30 00:59:25,790:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-10-30 00:59:25,790:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-10-30 00:59:25,810:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-10-30 00:59:25,810:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-10-30 00:59:25,823:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-10-30 00:59:25,823:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-10-30 00:59:25,824:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2023-10-30 00:59:25,844:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-10-30 00:59:25,857:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-10-30 00:59:25,857:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-10-30 00:59:25,878:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-10-30 00:59:25,890:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-10-30 00:59:25,890:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-10-30 00:59:25,891:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2023-10-30 00:59:25,924:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-10-30 00:59:25,924:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-10-30 00:59:25,957:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-10-30 00:59:25,957:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-10-30 00:59:25,958:INFO:Preparing preprocessing pipeline...
2023-10-30 00:59:25,959:INFO:Set up label encoding.
2023-10-30 00:59:25,959:INFO:Set up simple imputation.
2023-10-30 00:59:25,960:INFO:Set up column name cleaning.
2023-10-30 00:59:26,069:INFO:Finished creating preprocessing pipeline.
2023-10-30 00:59:26,088:INFO:Pipeline: Pipeline(memory=FastMemory(location=/var/folders/f1/19ck_yvd7p355g2p8r2jyzlc0000gn/T/joblib),
         steps=[('label_encoding',
                 TransformerWrapperWithInverse(exclude=None, include=None,
                                               transformer=LabelEncoder())),
                ('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['GATS1c', 'SlogP_VSA2', 'SLogP',
                                             'MDEN-23', 'MINdsCH', 'MINssCH2',
                                             'GATS1s', 'GATS2m', 'SaaO'...
                 TransformerWrapper(exclude=None, include=[],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent',
                                                              verbose='deprecated'))),
                ('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+')))],
         verbose=False)
2023-10-30 00:59:26,088:INFO:Creating final display dataframe.
2023-10-30 00:59:26,522:INFO:Setup _display_container:                     Description             Value
0                    Session id              3620
1                        Target               cls
2                   Target type            Binary
3                Target mapping   -1.0: 0, 1.0: 1
4           Original data shape       (9440, 298)
5        Transformed data shape       (9440, 298)
6   Transformed train set shape       (7552, 298)
7    Transformed test set shape       (1888, 298)
8              Numeric features               297
9      Rows with missing values            100.0%
10                   Preprocess              True
11              Imputation type            simple
12           Numeric imputation              mean
13       Categorical imputation              mode
14               Fold Generator   StratifiedKFold
15                  Fold Number                10
16                     CPU Jobs                -1
17                      Use GPU             False
18               Log Experiment             False
19              Experiment Name  clf-default-name
20                          USI              54bc
2023-10-30 00:59:26,569:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-10-30 00:59:26,569:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-10-30 00:59:26,604:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-10-30 00:59:26,605:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-10-30 00:59:26,606:INFO:setup() successfully completed in 1.0s...............
2023-10-30 01:00:16,599:INFO:Initializing create_model()
2023-10-30 01:00:16,600:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x16194d510>, estimator=dt, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-10-30 01:00:16,600:INFO:Checking exceptions
2023-10-30 01:00:16,618:INFO:Importing libraries
2023-10-30 01:00:16,618:INFO:Copying training dataset
2023-10-30 01:00:16,637:INFO:Defining folds
2023-10-30 01:00:16,637:INFO:Declaring metric variables
2023-10-30 01:00:16,642:INFO:Importing untrained model
2023-10-30 01:00:16,648:INFO:Decision Tree Classifier Imported successfully
2023-10-30 01:00:16,658:INFO:Starting cross validation
2023-10-30 01:00:16,664:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-10-30 01:05:57,817:INFO:Initializing create_model()
2023-10-30 01:05:57,817:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x16194d510>, estimator=dt, fold=4, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=True, kwargs={})
2023-10-30 01:05:57,817:INFO:Checking exceptions
2023-10-30 01:05:57,836:INFO:Importing libraries
2023-10-30 01:05:57,837:INFO:Copying training dataset
2023-10-30 01:05:57,857:INFO:Defining folds
2023-10-30 01:05:57,857:INFO:Declaring metric variables
2023-10-30 01:05:57,862:INFO:Importing untrained model
2023-10-30 01:05:57,867:INFO:Decision Tree Classifier Imported successfully
2023-10-30 01:05:57,877:INFO:Starting cross validation
2023-10-30 01:05:57,884:INFO:Cross validating with StratifiedKFold(n_splits=4, random_state=None, shuffle=False), n_jobs=-1
2023-10-30 01:24:52,388:INFO:Initializing create_model()
2023-10-30 01:24:52,388:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x16194d510>, estimator=dt, fold=4, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=True, kwargs={})
2023-10-30 01:24:52,388:INFO:Checking exceptions
2023-10-30 01:24:52,409:INFO:Importing libraries
2023-10-30 01:24:52,410:INFO:Copying training dataset
2023-10-30 01:24:52,430:INFO:Defining folds
2023-10-30 01:24:52,430:INFO:Declaring metric variables
2023-10-30 01:24:52,435:INFO:Importing untrained model
2023-10-30 01:24:52,440:INFO:Decision Tree Classifier Imported successfully
2023-10-30 01:24:52,450:INFO:Starting cross validation
2023-10-30 01:24:52,457:INFO:Cross validating with StratifiedKFold(n_splits=4, random_state=None, shuffle=False), n_jobs=-1
2023-10-30 01:26:08,894:INFO:Initializing create_model()
2023-10-30 01:26:08,894:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x16194d510>, estimator=dt, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-10-30 01:26:08,894:INFO:Checking exceptions
2023-10-30 01:26:08,914:INFO:Importing libraries
2023-10-30 01:26:08,914:INFO:Copying training dataset
2023-10-30 01:26:08,929:INFO:Defining folds
2023-10-30 01:26:08,929:INFO:Declaring metric variables
2023-10-30 01:26:08,934:INFO:Importing untrained model
2023-10-30 01:26:08,939:INFO:Decision Tree Classifier Imported successfully
2023-10-30 01:26:08,947:INFO:Starting cross validation
2023-10-30 01:26:08,954:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-10-30 01:26:24,396:INFO:Initializing create_model()
2023-10-30 01:26:24,396:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x16194d510>, estimator=dt, fold=4, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-10-30 01:26:24,396:INFO:Checking exceptions
2023-10-30 01:26:24,424:INFO:Importing libraries
2023-10-30 01:26:24,425:INFO:Copying training dataset
2023-10-30 01:26:24,443:INFO:Defining folds
2023-10-30 01:26:24,443:INFO:Declaring metric variables
2023-10-30 01:26:24,448:INFO:Importing untrained model
2023-10-30 01:26:24,453:INFO:Decision Tree Classifier Imported successfully
2023-10-30 01:26:24,463:INFO:Starting cross validation
2023-10-30 01:26:24,469:INFO:Cross validating with StratifiedKFold(n_splits=4, random_state=None, shuffle=False), n_jobs=-1
2023-10-30 01:39:52,644:INFO:PyCaret ClassificationExperiment
2023-10-30 01:39:52,644:INFO:Logging name: clf-default-name
2023-10-30 01:39:52,644:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2023-10-30 01:39:52,644:INFO:version 3.1.0
2023-10-30 01:39:52,644:INFO:Initializing setup()
2023-10-30 01:39:52,644:INFO:self.USI: 5023
2023-10-30 01:39:52,644:INFO:self._variable_keys: {'fold_generator', 'y_test', '_available_plots', 'logging_param', 'X', 'fold_groups_param', 'X_test', 'X_train', 'log_plots_param', 'is_multiclass', 'y', 'gpu_n_jobs_param', 'memory', 'n_jobs_param', 'pipeline', 'gpu_param', 'exp_id', 'data', 'USI', 'seed', 'exp_name_log', 'target_param', 'idx', 'y_train', 'html_param', '_ml_usecase', 'fix_imbalance', 'fold_shuffle_param'}
2023-10-30 01:39:52,645:INFO:Checking environment
2023-10-30 01:39:52,645:INFO:python_version: 3.10.8
2023-10-30 01:39:52,645:INFO:python_build: ('main', 'Nov 22 2022 08:25:13')
2023-10-30 01:39:52,645:INFO:machine: arm64
2023-10-30 01:39:52,645:INFO:platform: macOS-14.0-arm64-arm-64bit
2023-10-30 01:39:52,645:INFO:Memory: svmem(total=17179869184, available=4228923392, percent=75.4, used=6286557184, free=29917184, active=4201709568, inactive=4190683136, wired=2084847616)
2023-10-30 01:39:52,645:INFO:Physical Core: 10
2023-10-30 01:39:52,645:INFO:Logical Core: 10
2023-10-30 01:39:52,645:INFO:Checking libraries
2023-10-30 01:39:52,645:INFO:System:
2023-10-30 01:39:52,645:INFO:    python: 3.10.8 | packaged by conda-forge | (main, Nov 22 2022, 08:25:13) [Clang 14.0.6 ]
2023-10-30 01:39:52,645:INFO:executable: /Users/tanmaysharma/projects/Jak2Biotech/.venv/bin/python
2023-10-30 01:39:52,645:INFO:   machine: macOS-14.0-arm64-arm-64bit
2023-10-30 01:39:52,645:INFO:PyCaret required dependencies:
2023-10-30 01:39:52,646:INFO:                 pip: 23.3.1
2023-10-30 01:39:52,646:INFO:          setuptools: 63.2.0
2023-10-30 01:39:52,646:INFO:             pycaret: 3.1.0
2023-10-30 01:39:52,646:INFO:             IPython: 8.16.1
2023-10-30 01:39:52,646:INFO:          ipywidgets: 8.1.1
2023-10-30 01:39:52,646:INFO:                tqdm: 4.66.1
2023-10-30 01:39:52,646:INFO:               numpy: 1.23.5
2023-10-30 01:39:52,646:INFO:              pandas: 1.5.3
2023-10-30 01:39:52,646:INFO:              jinja2: 3.1.2
2023-10-30 01:39:52,646:INFO:               scipy: 1.10.1
2023-10-30 01:39:52,646:INFO:              joblib: 1.3.2
2023-10-30 01:39:52,646:INFO:             sklearn: 1.2.2
2023-10-30 01:39:52,646:INFO:                pyod: 1.1.1
2023-10-30 01:39:52,646:INFO:            imblearn: 0.11.0
2023-10-30 01:39:52,646:INFO:   category_encoders: 2.6.2
2023-10-30 01:39:52,646:INFO:            lightgbm: 4.1.0
2023-10-30 01:39:52,646:INFO:               numba: 0.58.1
2023-10-30 01:39:52,646:INFO:            requests: 2.31.0
2023-10-30 01:39:52,646:INFO:          matplotlib: 3.8.0
2023-10-30 01:39:52,646:INFO:          scikitplot: 0.3.7
2023-10-30 01:39:52,647:INFO:         yellowbrick: 1.5
2023-10-30 01:39:52,647:INFO:              plotly: 5.18.0
2023-10-30 01:39:52,647:INFO:    plotly-resampler: Not installed
2023-10-30 01:39:52,647:INFO:             kaleido: 0.2.1
2023-10-30 01:39:52,647:INFO:           schemdraw: 0.15
2023-10-30 01:39:52,647:INFO:         statsmodels: 0.14.0
2023-10-30 01:39:52,647:INFO:              sktime: 0.21.1
2023-10-30 01:39:52,647:INFO:               tbats: 1.1.3
2023-10-30 01:39:52,647:INFO:            pmdarima: 2.0.4
2023-10-30 01:39:52,647:INFO:              psutil: 5.9.6
2023-10-30 01:39:52,647:INFO:          markupsafe: 2.1.3
2023-10-30 01:39:52,647:INFO:             pickle5: Not installed
2023-10-30 01:39:52,647:INFO:         cloudpickle: 3.0.0
2023-10-30 01:39:52,647:INFO:         deprecation: 2.1.0
2023-10-30 01:39:52,647:INFO:              xxhash: 3.4.1
2023-10-30 01:39:52,647:INFO:           wurlitzer: 3.0.3
2023-10-30 01:39:52,647:INFO:PyCaret optional dependencies:
2023-10-30 01:39:52,647:INFO:                shap: Not installed
2023-10-30 01:39:52,647:INFO:           interpret: Not installed
2023-10-30 01:39:52,648:INFO:                umap: Not installed
2023-10-30 01:39:52,648:INFO:     ydata_profiling: Not installed
2023-10-30 01:39:52,648:INFO:  explainerdashboard: Not installed
2023-10-30 01:39:52,648:INFO:             autoviz: Not installed
2023-10-30 01:39:52,648:INFO:           fairlearn: Not installed
2023-10-30 01:39:52,648:INFO:          deepchecks: Not installed
2023-10-30 01:39:52,648:INFO:             xgboost: Not installed
2023-10-30 01:39:52,648:INFO:            catboost: Not installed
2023-10-30 01:39:52,648:INFO:              kmodes: Not installed
2023-10-30 01:39:52,648:INFO:             mlxtend: Not installed
2023-10-30 01:39:52,648:INFO:       statsforecast: Not installed
2023-10-30 01:39:52,648:INFO:        tune_sklearn: Not installed
2023-10-30 01:39:52,648:INFO:                 ray: Not installed
2023-10-30 01:39:52,648:INFO:            hyperopt: Not installed
2023-10-30 01:39:52,648:INFO:              optuna: Not installed
2023-10-30 01:39:52,648:INFO:               skopt: Not installed
2023-10-30 01:39:52,648:INFO:              mlflow: Not installed
2023-10-30 01:39:52,648:INFO:              gradio: Not installed
2023-10-30 01:39:52,648:INFO:             fastapi: Not installed
2023-10-30 01:39:52,648:INFO:             uvicorn: Not installed
2023-10-30 01:39:52,648:INFO:              m2cgen: Not installed
2023-10-30 01:39:52,648:INFO:           evidently: Not installed
2023-10-30 01:39:52,648:INFO:               fugue: Not installed
2023-10-30 01:39:52,648:INFO:           streamlit: Not installed
2023-10-30 01:39:52,648:INFO:             prophet: Not installed
2023-10-30 01:39:52,649:INFO:None
2023-10-30 01:39:52,649:INFO:Set up data.
2023-10-30 01:39:52,789:INFO:Set up folding strategy.
2023-10-30 01:39:52,789:INFO:Set up train/test split.
2023-10-30 01:39:52,807:INFO:Set up index.
2023-10-30 01:39:52,807:INFO:Assigning column types.
2023-10-30 01:39:52,816:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2023-10-30 01:39:52,836:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-10-30 01:39:52,837:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-10-30 01:39:52,850:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-10-30 01:39:52,850:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-10-30 01:39:52,871:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-10-30 01:39:52,872:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-10-30 01:39:52,885:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-10-30 01:39:52,885:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-10-30 01:39:52,885:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2023-10-30 01:39:52,906:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-10-30 01:39:52,919:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-10-30 01:39:52,919:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-10-30 01:39:52,940:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-10-30 01:39:52,953:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-10-30 01:39:52,953:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-10-30 01:39:52,953:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2023-10-30 01:39:52,987:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-10-30 01:39:52,987:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-10-30 01:39:53,021:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-10-30 01:39:53,021:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-10-30 01:39:53,022:INFO:Preparing preprocessing pipeline...
2023-10-30 01:39:53,025:INFO:Set up simple imputation.
2023-10-30 01:39:53,025:INFO:Set up variance threshold.
2023-10-30 01:39:53,026:INFO:Set up removing multicollinearity.
2023-10-30 01:39:53,026:INFO:Set up removing outliers.
2023-10-30 01:40:07,953:INFO:PyCaret ClassificationExperiment
2023-10-30 01:40:07,953:INFO:Logging name: clf-default-name
2023-10-30 01:40:07,953:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2023-10-30 01:40:07,953:INFO:version 3.1.0
2023-10-30 01:40:07,953:INFO:Initializing setup()
2023-10-30 01:40:07,954:INFO:self.USI: 8f14
2023-10-30 01:40:07,954:INFO:self._variable_keys: {'fold_generator', 'y_test', '_available_plots', 'logging_param', 'X', 'fold_groups_param', 'X_test', 'X_train', 'log_plots_param', 'is_multiclass', 'y', 'gpu_n_jobs_param', 'memory', 'n_jobs_param', 'pipeline', 'gpu_param', 'exp_id', 'data', 'USI', 'seed', 'exp_name_log', 'target_param', 'idx', 'y_train', 'html_param', '_ml_usecase', 'fix_imbalance', 'fold_shuffle_param'}
2023-10-30 01:40:07,954:INFO:Checking environment
2023-10-30 01:40:07,954:INFO:python_version: 3.10.8
2023-10-30 01:40:07,954:INFO:python_build: ('main', 'Nov 22 2022 08:25:13')
2023-10-30 01:40:07,954:INFO:machine: arm64
2023-10-30 01:40:07,954:INFO:platform: macOS-14.0-arm64-arm-64bit
2023-10-30 01:40:07,954:INFO:Memory: svmem(total=17179869184, available=4278583296, percent=75.1, used=6314098688, free=117145600, active=4174659584, inactive=4158537728, wired=2139439104)
2023-10-30 01:40:07,954:INFO:Physical Core: 10
2023-10-30 01:40:07,954:INFO:Logical Core: 10
2023-10-30 01:40:07,954:INFO:Checking libraries
2023-10-30 01:40:07,954:INFO:System:
2023-10-30 01:40:07,954:INFO:    python: 3.10.8 | packaged by conda-forge | (main, Nov 22 2022, 08:25:13) [Clang 14.0.6 ]
2023-10-30 01:40:07,954:INFO:executable: /Users/tanmaysharma/projects/Jak2Biotech/.venv/bin/python
2023-10-30 01:40:07,954:INFO:   machine: macOS-14.0-arm64-arm-64bit
2023-10-30 01:40:07,954:INFO:PyCaret required dependencies:
2023-10-30 01:40:07,954:INFO:                 pip: 23.3.1
2023-10-30 01:40:07,955:INFO:          setuptools: 63.2.0
2023-10-30 01:40:07,955:INFO:             pycaret: 3.1.0
2023-10-30 01:40:07,955:INFO:             IPython: 8.16.1
2023-10-30 01:40:07,955:INFO:          ipywidgets: 8.1.1
2023-10-30 01:40:07,955:INFO:                tqdm: 4.66.1
2023-10-30 01:40:07,955:INFO:               numpy: 1.23.5
2023-10-30 01:40:07,955:INFO:              pandas: 1.5.3
2023-10-30 01:40:07,955:INFO:              jinja2: 3.1.2
2023-10-30 01:40:07,955:INFO:               scipy: 1.10.1
2023-10-30 01:40:07,955:INFO:              joblib: 1.3.2
2023-10-30 01:40:07,955:INFO:             sklearn: 1.2.2
2023-10-30 01:40:07,955:INFO:                pyod: 1.1.1
2023-10-30 01:40:07,955:INFO:            imblearn: 0.11.0
2023-10-30 01:40:07,955:INFO:   category_encoders: 2.6.2
2023-10-30 01:40:07,955:INFO:            lightgbm: 4.1.0
2023-10-30 01:40:07,955:INFO:               numba: 0.58.1
2023-10-30 01:40:07,955:INFO:            requests: 2.31.0
2023-10-30 01:40:07,955:INFO:          matplotlib: 3.8.0
2023-10-30 01:40:07,955:INFO:          scikitplot: 0.3.7
2023-10-30 01:40:07,955:INFO:         yellowbrick: 1.5
2023-10-30 01:40:07,955:INFO:              plotly: 5.18.0
2023-10-30 01:40:07,955:INFO:    plotly-resampler: Not installed
2023-10-30 01:40:07,955:INFO:             kaleido: 0.2.1
2023-10-30 01:40:07,955:INFO:           schemdraw: 0.15
2023-10-30 01:40:07,955:INFO:         statsmodels: 0.14.0
2023-10-30 01:40:07,955:INFO:              sktime: 0.21.1
2023-10-30 01:40:07,956:INFO:               tbats: 1.1.3
2023-10-30 01:40:07,956:INFO:            pmdarima: 2.0.4
2023-10-30 01:40:07,956:INFO:              psutil: 5.9.6
2023-10-30 01:40:07,956:INFO:          markupsafe: 2.1.3
2023-10-30 01:40:07,956:INFO:             pickle5: Not installed
2023-10-30 01:40:07,956:INFO:         cloudpickle: 3.0.0
2023-10-30 01:40:07,956:INFO:         deprecation: 2.1.0
2023-10-30 01:40:07,956:INFO:              xxhash: 3.4.1
2023-10-30 01:40:07,956:INFO:           wurlitzer: 3.0.3
2023-10-30 01:40:07,956:INFO:PyCaret optional dependencies:
2023-10-30 01:40:07,956:INFO:                shap: Not installed
2023-10-30 01:40:07,956:INFO:           interpret: Not installed
2023-10-30 01:40:07,956:INFO:                umap: Not installed
2023-10-30 01:40:07,956:INFO:     ydata_profiling: Not installed
2023-10-30 01:40:07,956:INFO:  explainerdashboard: Not installed
2023-10-30 01:40:07,956:INFO:             autoviz: Not installed
2023-10-30 01:40:07,956:INFO:           fairlearn: Not installed
2023-10-30 01:40:07,956:INFO:          deepchecks: Not installed
2023-10-30 01:40:07,956:INFO:             xgboost: Not installed
2023-10-30 01:40:07,956:INFO:            catboost: Not installed
2023-10-30 01:40:07,956:INFO:              kmodes: Not installed
2023-10-30 01:40:07,956:INFO:             mlxtend: Not installed
2023-10-30 01:40:07,956:INFO:       statsforecast: Not installed
2023-10-30 01:40:07,957:INFO:        tune_sklearn: Not installed
2023-10-30 01:40:07,957:INFO:                 ray: Not installed
2023-10-30 01:40:07,957:INFO:            hyperopt: Not installed
2023-10-30 01:40:07,957:INFO:              optuna: Not installed
2023-10-30 01:40:07,957:INFO:               skopt: Not installed
2023-10-30 01:40:07,957:INFO:              mlflow: Not installed
2023-10-30 01:40:07,957:INFO:              gradio: Not installed
2023-10-30 01:40:07,957:INFO:             fastapi: Not installed
2023-10-30 01:40:07,957:INFO:             uvicorn: Not installed
2023-10-30 01:40:07,957:INFO:              m2cgen: Not installed
2023-10-30 01:40:07,957:INFO:           evidently: Not installed
2023-10-30 01:40:07,957:INFO:               fugue: Not installed
2023-10-30 01:40:07,957:INFO:           streamlit: Not installed
2023-10-30 01:40:07,957:INFO:             prophet: Not installed
2023-10-30 01:40:07,957:INFO:None
2023-10-30 01:40:07,957:INFO:Set up data.
2023-10-30 01:40:08,079:INFO:Set up folding strategy.
2023-10-30 01:40:08,079:INFO:Set up train/test split.
2023-10-30 01:40:08,092:INFO:Set up index.
2023-10-30 01:40:08,093:INFO:Assigning column types.
2023-10-30 01:40:08,100:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2023-10-30 01:40:08,121:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-10-30 01:40:08,122:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-10-30 01:40:08,135:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-10-30 01:40:08,135:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-10-30 01:40:08,156:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-10-30 01:40:08,156:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-10-30 01:40:08,169:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-10-30 01:40:08,169:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-10-30 01:40:08,170:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2023-10-30 01:40:08,190:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-10-30 01:40:08,203:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-10-30 01:40:08,203:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-10-30 01:40:08,224:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-10-30 01:40:08,237:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-10-30 01:40:08,237:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-10-30 01:40:08,237:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2023-10-30 01:40:08,271:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-10-30 01:40:08,271:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-10-30 01:40:08,304:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-10-30 01:40:08,305:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-10-30 01:40:08,305:INFO:Preparing preprocessing pipeline...
2023-10-30 01:40:08,307:INFO:Set up simple imputation.
2023-10-30 01:40:08,307:INFO:Set up variance threshold.
2023-10-30 01:40:08,308:INFO:Set up removing multicollinearity.
2023-10-30 01:40:08,308:INFO:Set up removing outliers.
2023-10-30 01:40:08,308:INFO:Set up PCA.
2023-10-30 01:40:08,309:INFO:Set up feature selection.
2023-10-30 01:40:08,342:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-10-30 01:40:08,342:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-10-30 01:40:08,346:INFO:Set up column name cleaning.
2023-10-30 01:53:07,143:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-10-30 01:53:07,144:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-10-30 01:53:07,144:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-10-30 01:53:07,144:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-10-30 01:53:10,364:INFO:PyCaret ClassificationExperiment
2023-10-30 01:53:10,364:INFO:Logging name: clf-default-name
2023-10-30 01:53:10,364:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2023-10-30 01:53:10,364:INFO:version 3.1.0
2023-10-30 01:53:10,364:INFO:Initializing setup()
2023-10-30 01:53:10,364:INFO:self.USI: fe1f
2023-10-30 01:53:10,364:INFO:self._variable_keys: {'_available_plots', 'X', 'n_jobs_param', 'fold_groups_param', 'fold_shuffle_param', 'gpu_param', 'exp_name_log', 'is_multiclass', 'target_param', 'pipeline', 'logging_param', 'USI', 'idx', 'X_train', 'y', 'y_test', '_ml_usecase', 'fix_imbalance', 'gpu_n_jobs_param', 'X_test', 'memory', 'seed', 'exp_id', 'data', 'log_plots_param', 'html_param', 'y_train', 'fold_generator'}
2023-10-30 01:53:10,364:INFO:Checking environment
2023-10-30 01:53:10,364:INFO:python_version: 3.10.8
2023-10-30 01:53:10,364:INFO:python_build: ('main', 'Nov 22 2022 08:25:13')
2023-10-30 01:53:10,364:INFO:machine: arm64
2023-10-30 01:53:10,364:INFO:platform: macOS-14.0-arm64-arm-64bit
2023-10-30 01:53:10,364:INFO:Memory: svmem(total=17179869184, available=7351894016, percent=57.2, used=8742780928, free=97468416, active=7266500608, inactive=7111557120, wired=1476280320)
2023-10-30 01:53:10,365:INFO:Physical Core: 10
2023-10-30 01:53:10,365:INFO:Logical Core: 10
2023-10-30 01:53:10,365:INFO:Checking libraries
2023-10-30 01:53:10,365:INFO:System:
2023-10-30 01:53:10,365:INFO:    python: 3.10.8 | packaged by conda-forge | (main, Nov 22 2022, 08:25:13) [Clang 14.0.6 ]
2023-10-30 01:53:10,365:INFO:executable: /Users/tanmaysharma/projects/Jak2Biotech/.venv/bin/python
2023-10-30 01:53:10,365:INFO:   machine: macOS-14.0-arm64-arm-64bit
2023-10-30 01:53:10,365:INFO:PyCaret required dependencies:
2023-10-30 01:53:10,393:INFO:                 pip: 23.3.1
2023-10-30 01:53:10,393:INFO:          setuptools: 63.2.0
2023-10-30 01:53:10,393:INFO:             pycaret: 3.1.0
2023-10-30 01:53:10,393:INFO:             IPython: 8.16.1
2023-10-30 01:53:10,393:INFO:          ipywidgets: 8.1.1
2023-10-30 01:53:10,393:INFO:                tqdm: 4.66.1
2023-10-30 01:53:10,393:INFO:               numpy: 1.23.5
2023-10-30 01:53:10,393:INFO:              pandas: 1.5.3
2023-10-30 01:53:10,393:INFO:              jinja2: 3.1.2
2023-10-30 01:53:10,393:INFO:               scipy: 1.10.1
2023-10-30 01:53:10,393:INFO:              joblib: 1.3.2
2023-10-30 01:53:10,393:INFO:             sklearn: 1.2.2
2023-10-30 01:53:10,393:INFO:                pyod: 1.1.1
2023-10-30 01:53:10,393:INFO:            imblearn: 0.11.0
2023-10-30 01:53:10,393:INFO:   category_encoders: 2.6.2
2023-10-30 01:53:10,393:INFO:            lightgbm: 4.1.0
2023-10-30 01:53:10,393:INFO:               numba: 0.58.1
2023-10-30 01:53:10,393:INFO:            requests: 2.31.0
2023-10-30 01:53:10,393:INFO:          matplotlib: 3.8.0
2023-10-30 01:53:10,393:INFO:          scikitplot: 0.3.7
2023-10-30 01:53:10,393:INFO:         yellowbrick: 1.5
2023-10-30 01:53:10,393:INFO:              plotly: 5.18.0
2023-10-30 01:53:10,393:INFO:    plotly-resampler: Not installed
2023-10-30 01:53:10,393:INFO:             kaleido: 0.2.1
2023-10-30 01:53:10,393:INFO:           schemdraw: 0.15
2023-10-30 01:53:10,393:INFO:         statsmodels: 0.14.0
2023-10-30 01:53:10,393:INFO:              sktime: 0.21.1
2023-10-30 01:53:10,393:INFO:               tbats: 1.1.3
2023-10-30 01:53:10,393:INFO:            pmdarima: 2.0.4
2023-10-30 01:53:10,393:INFO:              psutil: 5.9.6
2023-10-30 01:53:10,393:INFO:          markupsafe: 2.1.3
2023-10-30 01:53:10,393:INFO:             pickle5: Not installed
2023-10-30 01:53:10,393:INFO:         cloudpickle: 3.0.0
2023-10-30 01:53:10,393:INFO:         deprecation: 2.1.0
2023-10-30 01:53:10,393:INFO:              xxhash: 3.4.1
2023-10-30 01:53:10,393:INFO:           wurlitzer: 3.0.3
2023-10-30 01:53:10,393:INFO:PyCaret optional dependencies:
2023-10-30 01:53:10,401:INFO:                shap: Not installed
2023-10-30 01:53:10,401:INFO:           interpret: Not installed
2023-10-30 01:53:10,401:INFO:                umap: Not installed
2023-10-30 01:53:10,401:INFO:     ydata_profiling: Not installed
2023-10-30 01:53:10,401:INFO:  explainerdashboard: Not installed
2023-10-30 01:53:10,401:INFO:             autoviz: Not installed
2023-10-30 01:53:10,401:INFO:           fairlearn: Not installed
2023-10-30 01:53:10,401:INFO:          deepchecks: Not installed
2023-10-30 01:53:10,401:INFO:             xgboost: Not installed
2023-10-30 01:53:10,401:INFO:            catboost: Not installed
2023-10-30 01:53:10,401:INFO:              kmodes: Not installed
2023-10-30 01:53:10,401:INFO:             mlxtend: Not installed
2023-10-30 01:53:10,401:INFO:       statsforecast: Not installed
2023-10-30 01:53:10,401:INFO:        tune_sklearn: Not installed
2023-10-30 01:53:10,401:INFO:                 ray: Not installed
2023-10-30 01:53:10,401:INFO:            hyperopt: Not installed
2023-10-30 01:53:10,401:INFO:              optuna: Not installed
2023-10-30 01:53:10,401:INFO:               skopt: Not installed
2023-10-30 01:53:10,401:INFO:              mlflow: Not installed
2023-10-30 01:53:10,401:INFO:              gradio: Not installed
2023-10-30 01:53:10,401:INFO:             fastapi: Not installed
2023-10-30 01:53:10,401:INFO:             uvicorn: Not installed
2023-10-30 01:53:10,401:INFO:              m2cgen: Not installed
2023-10-30 01:53:10,401:INFO:           evidently: Not installed
2023-10-30 01:53:10,401:INFO:               fugue: Not installed
2023-10-30 01:53:10,401:INFO:           streamlit: Not installed
2023-10-30 01:53:10,401:INFO:             prophet: Not installed
2023-10-30 01:53:10,401:INFO:None
2023-10-30 01:53:10,401:INFO:Set up data.
2023-10-30 01:53:10,491:INFO:Set up folding strategy.
2023-10-30 01:53:10,492:INFO:Set up train/test split.
2023-10-30 01:53:10,507:INFO:Set up index.
2023-10-30 01:53:10,508:INFO:Assigning column types.
2023-10-30 01:53:10,514:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2023-10-30 01:53:10,532:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-10-30 01:53:10,533:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-10-30 01:53:10,548:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-10-30 01:53:10,548:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-10-30 01:53:10,566:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-10-30 01:53:10,567:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-10-30 01:53:10,578:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-10-30 01:53:10,578:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-10-30 01:53:10,578:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2023-10-30 01:53:10,598:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-10-30 01:53:10,609:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-10-30 01:53:10,609:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-10-30 01:53:10,628:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-10-30 01:53:10,640:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-10-30 01:53:10,640:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-10-30 01:53:10,640:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2023-10-30 01:53:10,670:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-10-30 01:53:10,670:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-10-30 01:53:10,701:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-10-30 01:53:10,701:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-10-30 01:53:10,703:INFO:Preparing preprocessing pipeline...
2023-10-30 01:53:10,705:INFO:Set up simple imputation.
2023-10-30 01:53:10,705:INFO:Set up variance threshold.
2023-10-30 01:53:10,705:INFO:Set up removing multicollinearity.
2023-10-30 01:53:10,705:INFO:Set up removing outliers.
2023-10-30 01:53:10,705:INFO:Set up PCA.
2023-10-30 01:53:10,706:INFO:Set up feature selection.
2023-10-30 01:53:10,736:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-10-30 01:53:10,736:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-10-30 01:53:10,739:INFO:Set up column name cleaning.
2023-10-30 02:04:46,034:INFO:Finished creating preprocessing pipeline.
2023-10-30 02:04:46,044:INFO:Pipeline: Pipeline(memory=FastMemory(location=/var/folders/f1/19ck_yvd7p355g2p8r2jyzlc0000gn/T/joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['GATS1c', 'SlogP_VSA2', 'SLogP',
                                             'MDEN-23', 'MINdsCH', 'MINssCH2',
                                             'GATS1s', 'GATS2m', 'SaaO',
                                             'BCUTZ-1h', 'SdsN', 'MINaaN',
                                             'MAXssCH2', 'MATS1s', 'GATS5d',
                                             'Xch-5d', 'SMR_VSA7',
                                             'ETA_dEpsilon_B', 'VSA_ESta...
                                                                                         objective=None,
                                                                                         random_state=None,
                                                                                         reg_alpha=0.0,
                                                                                         reg_lambda=0.0,
                                                                                         subsample=1.0,
                                                                                         subsample_for_bin=200000,
                                                                                         subsample_freq=0),
                                                                importance_getter='auto',
                                                                max_features=59,
                                                                norm_order=1,
                                                                prefit=False,
                                                                threshold=-inf))),
                ('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+')))],
         verbose=False)
2023-10-30 02:04:46,044:INFO:Creating final display dataframe.
2023-10-30 02:13:42,631:INFO:Setup _display_container:                     Description             Value
0                    Session id              1381
1                        Target               cls
2                   Target type            Binary
3           Original data shape       (9440, 298)
4        Transformed data shape        (9086, 60)
5   Transformed train set shape        (6726, 60)
6    Transformed test set shape        (2360, 60)
7              Numeric features               297
8      Rows with missing values            100.0%
9                    Preprocess              True
10              Imputation type            simple
11           Numeric imputation            median
12       Categorical imputation              mode
13       Low variance threshold              0.05
14     Remove multicollinearity              True
15  Multicollinearity threshold               0.8
16              Remove outliers              True
17           Outliers threshold              0.05
18                          PCA              True
19                   PCA method            kernel
20               PCA components              None
21            Feature selection              True
22     Feature selection method           classic
23  Feature selection estimator          lightgbm
24  Number of features selected               0.2
25               Fold Generator   StratifiedKFold
26                  Fold Number                10
27                     CPU Jobs                -1
28                      Use GPU             False
29               Log Experiment             False
30              Experiment Name  clf-default-name
31                          USI              fe1f
2023-10-30 02:13:42,669:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-10-30 02:13:42,670:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-10-30 02:13:42,703:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-10-30 02:13:42,704:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-10-30 02:13:42,704:INFO:setup() successfully completed in 1232.34s...............
2023-10-30 02:13:42,709:INFO:Initializing create_model()
2023-10-30 02:13:42,710:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x28c176dd0>, estimator=catboost, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-10-30 02:13:42,710:INFO:Checking exceptions
2023-10-30 02:18:16,359:INFO:Initializing create_model()
2023-10-30 02:18:16,359:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x28c176dd0>, estimator=ada, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-10-30 02:18:16,360:INFO:Checking exceptions
2023-10-30 02:18:16,401:INFO:Importing libraries
2023-10-30 02:18:16,402:INFO:Copying training dataset
2023-10-30 02:18:16,423:INFO:Defining folds
2023-10-30 02:18:16,423:INFO:Declaring metric variables
2023-10-30 02:18:16,426:INFO:Importing untrained model
2023-10-30 02:18:16,430:INFO:Ada Boost Classifier Imported successfully
2023-10-30 02:18:16,434:INFO:Starting cross validation
2023-10-30 02:18:16,469:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-10-30 02:19:36,364:INFO:[LightGBM] [Info] Number of positive: 2964, number of negative: 3089
2023-10-30 02:19:36,578:INFO:[LightGBM] [Info] Number of positive: 2933, number of negative: 3120
2023-10-30 02:19:36,757:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.392590 seconds.
2023-10-30 02:19:36,757:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-30 02:19:36,764:INFO:[LightGBM] [Info] Total Bins 1299735
2023-10-30 02:19:36,967:INFO:[LightGBM] [Info] Number of data points in the train set: 6053, number of used features: 5097
2023-10-30 02:19:36,986:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.489675 -> initscore=-0.041308
2023-10-30 02:19:36,986:INFO:[LightGBM] [Info] Start training from score -0.041308
2023-10-30 02:19:37,044:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.452551 seconds.
2023-10-30 02:19:37,045:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-30 02:19:37,052:INFO:[LightGBM] [Info] Total Bins 1291830
2023-10-30 02:19:37,202:INFO:[LightGBM] [Info] Number of data points in the train set: 6053, number of used features: 5066
2023-10-30 02:19:37,214:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.484553 -> initscore=-0.061807
2023-10-30 02:19:37,214:INFO:[LightGBM] [Info] Start training from score -0.061807
2023-10-30 02:19:38,055:INFO:[LightGBM] [Info] Number of positive: 2924, number of negative: 3129
2023-10-30 02:19:38,716:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.658496 seconds.
2023-10-30 02:19:38,716:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-30 02:19:38,723:INFO:[LightGBM] [Info] Total Bins 1297440
2023-10-30 02:19:38,878:INFO:[LightGBM] [Info] Number of data points in the train set: 6053, number of used features: 5088
2023-10-30 02:19:38,897:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.483066 -> initscore=-0.067761
2023-10-30 02:19:38,897:INFO:[LightGBM] [Info] Start training from score -0.067761
2023-10-30 02:19:40,038:INFO:[LightGBM] [Info] Number of positive: 2935, number of negative: 3119
2023-10-30 02:19:40,521:INFO:[LightGBM] [Info] Number of positive: 2935, number of negative: 3118
2023-10-30 02:19:40,612:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.572025 seconds.
2023-10-30 02:19:40,612:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-30 02:19:40,628:INFO:[LightGBM] [Info] Total Bins 1290810
2023-10-30 02:19:40,769:INFO:[LightGBM] [Info] Number of data points in the train set: 6054, number of used features: 5062
2023-10-30 02:19:40,779:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.484803 -> initscore=-0.060805
2023-10-30 02:19:40,782:INFO:[LightGBM] [Info] Start training from score -0.060805
2023-10-30 02:19:41,254:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.727791 seconds.
2023-10-30 02:19:41,254:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-30 02:19:41,268:INFO:[LightGBM] [Info] Total Bins 1294380
2023-10-30 02:19:41,455:INFO:[LightGBM] [Info] Number of data points in the train set: 6053, number of used features: 5076
2023-10-30 02:19:41,468:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.484884 -> initscore=-0.060484
2023-10-30 02:19:41,470:INFO:[LightGBM] [Info] Start training from score -0.060484
2023-10-30 02:19:44,132:INFO:[LightGBM] [Info] Number of positive: 2939, number of negative: 3114
2023-10-30 02:19:44,340:INFO:[LightGBM] [Info] Number of positive: 2963, number of negative: 3090
2023-10-30 02:19:44,687:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.552316 seconds.
2023-10-30 02:19:44,687:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-30 02:19:44,694:INFO:[LightGBM] [Info] Total Bins 1293360
2023-10-30 02:19:44,903:INFO:[LightGBM] [Info] Number of data points in the train set: 6053, number of used features: 5072
2023-10-30 02:19:44,915:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.485544 -> initscore=-0.057839
2023-10-30 02:19:44,915:INFO:[LightGBM] [Info] Start training from score -0.057839
2023-10-30 02:19:45,162:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.803181 seconds.
2023-10-30 02:19:45,162:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-30 02:19:45,169:INFO:[LightGBM] [Info] Total Bins 1297695
2023-10-30 02:19:45,363:INFO:[LightGBM] [Info] Number of data points in the train set: 6053, number of used features: 5089
2023-10-30 02:19:45,383:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.489509 -> initscore=-0.041969
2023-10-30 02:19:45,384:INFO:[LightGBM] [Info] Start training from score -0.041969
2023-10-30 02:19:46,935:INFO:[LightGBM] [Info] Number of positive: 2964, number of negative: 3089
2023-10-30 02:19:47,452:INFO:[LightGBM] [Info] Number of positive: 2930, number of negative: 3123
2023-10-30 02:19:47,623:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.677357 seconds.
2023-10-30 02:19:47,623:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-30 02:19:47,639:INFO:[LightGBM] [Info] Total Bins 1297440
2023-10-30 02:19:47,859:INFO:[LightGBM] [Info] Number of data points in the train set: 6053, number of used features: 5088
2023-10-30 02:19:47,877:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.489675 -> initscore=-0.041308
2023-10-30 02:19:47,878:INFO:[LightGBM] [Info] Start training from score -0.041308
2023-10-30 02:19:48,245:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.787939 seconds.
2023-10-30 02:19:48,245:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-30 02:19:48,263:INFO:[LightGBM] [Info] Total Bins 1290810
2023-10-30 02:19:48,411:INFO:[LightGBM] [Info] Number of positive: 2929, number of negative: 3123
2023-10-30 02:19:48,482:INFO:[LightGBM] [Info] Number of data points in the train set: 6053, number of used features: 5062
2023-10-30 02:19:48,510:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.484057 -> initscore=-0.063792
2023-10-30 02:19:48,510:INFO:[LightGBM] [Info] Start training from score -0.063792
2023-10-30 02:19:49,112:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.684359 seconds.
2023-10-30 02:19:49,112:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-30 02:19:49,121:INFO:[LightGBM] [Info] Total Bins 1292850
2023-10-30 02:19:49,366:INFO:[LightGBM] [Info] Number of data points in the train set: 6052, number of used features: 5070
2023-10-30 02:19:49,378:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.483972 -> initscore=-0.064133
2023-10-30 02:19:49,378:INFO:[LightGBM] [Info] Start training from score -0.064133
2023-10-30 02:22:01,702:INFO:Calculating mean and std
2023-10-30 02:22:01,706:INFO:Creating metrics dataframe
2023-10-30 02:22:01,714:INFO:Finalizing model
2023-10-30 02:32:41,511:INFO:[LightGBM] [Info] Number of positive: 3291, number of negative: 3435
2023-10-30 02:32:41,769:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.256376 seconds.
2023-10-30 02:32:41,769:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-30 02:32:41,776:INFO:[LightGBM] [Info] Total Bins 1415505
2023-10-30 02:32:41,828:INFO:[LightGBM] [Info] Number of data points in the train set: 6726, number of used features: 5551
2023-10-30 02:32:41,834:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.489295 -> initscore=-0.042825
2023-10-30 02:32:41,834:INFO:[LightGBM] [Info] Start training from score -0.042825
2023-10-30 02:33:17,102:INFO:Uploading results into container
2023-10-30 02:33:17,103:INFO:Uploading model into container now
2023-10-30 02:33:17,109:INFO:_master_model_container: 1
2023-10-30 02:33:17,109:INFO:_display_container: 2
2023-10-30 02:33:17,109:INFO:AdaBoostClassifier(algorithm='SAMME.R', base_estimator='deprecated',
                   estimator=None, learning_rate=1.0, n_estimators=50,
                   random_state=1381)
2023-10-30 02:33:17,110:INFO:create_model() successfully completed......................................
2023-10-30 02:33:17,197:INFO:Initializing create_model()
2023-10-30 02:33:17,198:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x28c176dd0>, estimator=xgboost, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-10-30 02:33:17,198:INFO:Checking exceptions
2023-10-30 02:33:59,111:INFO:Initializing create_model()
2023-10-30 02:33:59,111:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x28c176dd0>, estimator=gbc, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-10-30 02:33:59,112:INFO:Checking exceptions
2023-10-30 02:33:59,121:INFO:Importing libraries
2023-10-30 02:33:59,121:INFO:Copying training dataset
2023-10-30 02:33:59,139:INFO:Defining folds
2023-10-30 02:33:59,139:INFO:Declaring metric variables
2023-10-30 02:33:59,141:INFO:Importing untrained model
2023-10-30 02:33:59,142:INFO:Gradient Boosting Classifier Imported successfully
2023-10-30 02:33:59,146:INFO:Starting cross validation
2023-10-30 02:33:59,219:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-10-30 02:35:21,081:INFO:[LightGBM] [Info] Number of positive: 2929, number of negative: 3123
2023-10-30 02:35:21,514:INFO:[LightGBM] [Info] Number of positive: 2933, number of negative: 3120
2023-10-30 02:35:21,569:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.488981 seconds.
2023-10-30 02:35:21,569:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-30 02:35:21,576:INFO:[LightGBM] [Info] Total Bins 1292850
2023-10-30 02:35:21,674:INFO:[LightGBM] [Info] Number of data points in the train set: 6052, number of used features: 5070
2023-10-30 02:35:21,681:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.483972 -> initscore=-0.064133
2023-10-30 02:35:21,681:INFO:[LightGBM] [Info] Start training from score -0.064133
2023-10-30 02:35:22,192:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.674460 seconds.
2023-10-30 02:35:22,192:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-30 02:35:22,198:INFO:[LightGBM] [Info] Total Bins 1291830
2023-10-30 02:35:22,341:INFO:[LightGBM] [Info] Number of data points in the train set: 6053, number of used features: 5066
2023-10-30 02:35:22,352:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.484553 -> initscore=-0.061807
2023-10-30 02:35:22,352:INFO:[LightGBM] [Info] Start training from score -0.061807
2023-10-30 02:35:23,039:INFO:[LightGBM] [Info] Number of positive: 2964, number of negative: 3089
2023-10-30 02:35:23,045:INFO:[LightGBM] [Info] Number of positive: 2963, number of negative: 3090
2023-10-30 02:35:23,607:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.565738 seconds.
2023-10-30 02:35:23,608:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-30 02:35:23,612:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.564942 seconds.
2023-10-30 02:35:23,613:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-30 02:35:23,614:INFO:[LightGBM] [Info] Total Bins 1297440
2023-10-30 02:35:23,620:INFO:[LightGBM] [Info] Total Bins 1297695
2023-10-30 02:35:23,956:INFO:[LightGBM] [Info] Number of data points in the train set: 6053, number of used features: 5089
2023-10-30 02:35:23,963:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.489509 -> initscore=-0.041969
2023-10-30 02:35:23,964:INFO:[LightGBM] [Info] Start training from score -0.041969
2023-10-30 02:35:23,965:INFO:[LightGBM] [Info] Number of data points in the train set: 6053, number of used features: 5088
2023-10-30 02:35:23,972:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.489675 -> initscore=-0.041308
2023-10-30 02:35:23,972:INFO:[LightGBM] [Info] Start training from score -0.041308
2023-10-30 02:35:24,260:INFO:[LightGBM] [Info] Number of positive: 2924, number of negative: 3129
2023-10-30 02:35:24,950:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.686948 seconds.
2023-10-30 02:35:24,950:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-30 02:35:24,957:INFO:[LightGBM] [Info] Total Bins 1297440
2023-10-30 02:35:25,110:INFO:[LightGBM] [Info] Number of data points in the train set: 6053, number of used features: 5088
2023-10-30 02:35:25,118:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.483066 -> initscore=-0.067761
2023-10-30 02:35:25,118:INFO:[LightGBM] [Info] Start training from score -0.067761
2023-10-30 02:35:26,190:INFO:[LightGBM] [Info] Number of positive: 2935, number of negative: 3119
2023-10-30 02:35:26,597:INFO:[LightGBM] [Info] Number of positive: 2935, number of negative: 3118
2023-10-30 02:35:26,756:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.562994 seconds.
2023-10-30 02:35:26,757:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-30 02:35:26,765:INFO:[LightGBM] [Info] Total Bins 1290810
2023-10-30 02:35:26,919:INFO:[LightGBM] [Info] Number of data points in the train set: 6054, number of used features: 5062
2023-10-30 02:35:26,941:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.484803 -> initscore=-0.060805
2023-10-30 02:35:26,942:INFO:[LightGBM] [Info] Start training from score -0.060805
2023-10-30 02:35:27,054:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.455752 seconds.
2023-10-30 02:35:27,055:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-30 02:35:27,064:INFO:[LightGBM] [Info] Total Bins 1294380
2023-10-30 02:35:27,229:INFO:[LightGBM] [Info] Number of data points in the train set: 6053, number of used features: 5076
2023-10-30 02:35:27,242:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.484884 -> initscore=-0.060484
2023-10-30 02:35:27,242:INFO:[LightGBM] [Info] Start training from score -0.060484
2023-10-30 02:35:28,777:INFO:[LightGBM] [Info] Number of positive: 2964, number of negative: 3089
2023-10-30 02:35:28,956:INFO:[LightGBM] [Info] Number of positive: 2930, number of negative: 3123
2023-10-30 02:35:29,355:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.571642 seconds.
2023-10-30 02:35:29,355:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-30 02:35:29,380:INFO:[LightGBM] [Info] Total Bins 1299735
2023-10-30 02:35:29,405:INFO:[LightGBM] [Info] Number of positive: 2939, number of negative: 3114
2023-10-30 02:35:29,516:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.559965 seconds.
2023-10-30 02:35:29,520:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-30 02:35:29,547:INFO:[LightGBM] [Info] Total Bins 1290810
2023-10-30 02:35:29,611:INFO:[LightGBM] [Info] Number of data points in the train set: 6053, number of used features: 5097
2023-10-30 02:35:29,639:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.489675 -> initscore=-0.041308
2023-10-30 02:35:29,639:INFO:[LightGBM] [Info] Start training from score -0.041308
2023-10-30 02:35:29,761:INFO:[LightGBM] [Info] Number of data points in the train set: 6053, number of used features: 5062
2023-10-30 02:35:29,767:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.484057 -> initscore=-0.063792
2023-10-30 02:35:29,767:INFO:[LightGBM] [Info] Start training from score -0.063792
2023-10-30 02:35:30,161:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.748041 seconds.
2023-10-30 02:35:30,161:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-30 02:35:30,172:INFO:[LightGBM] [Info] Total Bins 1293360
2023-10-30 02:35:30,400:INFO:[LightGBM] [Info] Number of data points in the train set: 6053, number of used features: 5072
2023-10-30 02:35:30,413:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.485544 -> initscore=-0.057839
2023-10-30 02:35:30,415:INFO:[LightGBM] [Info] Start training from score -0.057839
2023-10-30 02:38:02,394:INFO:Calculating mean and std
2023-10-30 02:38:02,397:INFO:Creating metrics dataframe
2023-10-30 02:38:02,405:INFO:Finalizing model
2023-10-30 02:48:56,527:INFO:[LightGBM] [Info] Number of positive: 3291, number of negative: 3435
2023-10-30 02:48:56,828:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.297219 seconds.
2023-10-30 02:48:56,828:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-30 02:48:56,836:INFO:[LightGBM] [Info] Total Bins 1415505
2023-10-30 02:48:56,911:INFO:[LightGBM] [Info] Number of data points in the train set: 6726, number of used features: 5551
2023-10-30 02:48:56,917:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.489295 -> initscore=-0.042825
2023-10-30 02:48:56,917:INFO:[LightGBM] [Info] Start training from score -0.042825
2023-10-30 02:49:39,984:INFO:Uploading results into container
2023-10-30 02:49:39,986:INFO:Uploading model into container now
2023-10-30 02:49:39,991:INFO:_master_model_container: 2
2023-10-30 02:49:39,991:INFO:_display_container: 3
2023-10-30 02:49:39,991:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=1381, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2023-10-30 02:49:39,991:INFO:create_model() successfully completed......................................
2023-10-30 02:57:51,734:INFO:Initializing create_model()
2023-10-30 02:57:51,734:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x28c176dd0>, estimator=xgboost, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-10-30 02:57:51,735:INFO:Checking exceptions
2023-10-30 02:58:00,830:INFO:Initializing create_model()
2023-10-30 02:58:00,830:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x28c176dd0>, estimator=xgboost, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-10-30 02:58:00,830:INFO:Checking exceptions
2023-10-30 02:58:16,304:INFO:Initializing create_model()
2023-10-30 02:58:16,304:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x28c176dd0>, estimator=xgboost, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-10-30 02:58:16,304:INFO:Checking exceptions
2023-10-30 02:59:15,245:INFO:Initializing interpret_model()
2023-10-30 02:59:15,245:INFO:interpret_model(estimator=GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=1381, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False), use_train_data=False, X_new_sample=None, y_new_sample=None, feature=None, kwargs={}, observation=None, plot=summary, save=False, self=<pycaret.classification.oop.ClassificationExperiment object at 0x28c176dd0>)
2023-10-30 02:59:15,245:INFO:Checking exceptions
2023-10-30 02:59:15,245:ERROR:
'shap' is a soft dependency and not included in the pycaret installation. Please run: `pip install shap` to install.
Alternately, you can install this by running `pip install pycaret[analysis]`
NoneType: None
2023-10-30 02:59:31,312:INFO:Initializing interpret_model()
2023-10-30 02:59:31,312:INFO:interpret_model(estimator=GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=1381, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False), use_train_data=False, X_new_sample=None, y_new_sample=None, feature=None, kwargs={}, observation=None, plot=summary, save=False, self=<pycaret.classification.oop.ClassificationExperiment object at 0x28c176dd0>)
2023-10-30 02:59:31,312:INFO:Checking exceptions
2023-10-30 02:59:31,313:ERROR:
'shap' is a soft dependency and not included in the pycaret installation. Please run: `pip install shap` to install.
Alternately, you can install this by running `pip install pycaret[analysis]`
NoneType: None
2023-10-30 17:07:44,649:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-10-30 17:07:44,649:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-10-30 17:07:44,649:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-10-30 17:07:44,649:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-10-30 17:07:54,593:INFO:PyCaret ClassificationExperiment
2023-10-30 17:07:54,593:INFO:Logging name: clf-default-name
2023-10-30 17:07:54,593:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2023-10-30 17:07:54,594:INFO:version 3.1.0
2023-10-30 17:07:54,594:INFO:Initializing setup()
2023-10-30 17:07:54,594:INFO:self.USI: 612f
2023-10-30 17:07:54,594:INFO:self._variable_keys: {'data', 'USI', 'fold_groups_param', 'gpu_param', 'log_plots_param', 'exp_id', 'X_train', 'pipeline', 'X', 'X_test', '_available_plots', 'exp_name_log', 'fold_shuffle_param', 'is_multiclass', 'target_param', 'gpu_n_jobs_param', 'seed', 'fix_imbalance', 'logging_param', 'y_train', '_ml_usecase', 'y_test', 'y', 'n_jobs_param', 'idx', 'memory', 'html_param', 'fold_generator'}
2023-10-30 17:07:54,594:INFO:Checking environment
2023-10-30 17:07:54,594:INFO:python_version: 3.10.8
2023-10-30 17:07:54,594:INFO:python_build: ('main', 'Nov 22 2022 08:25:13')
2023-10-30 17:07:54,594:INFO:machine: arm64
2023-10-30 17:07:54,594:INFO:platform: macOS-14.0-arm64-arm-64bit
2023-10-30 17:07:54,594:INFO:Memory: svmem(total=17179869184, available=6256017408, percent=63.6, used=8045838336, free=91832320, active=6181175296, inactive=6161448960, wired=1864663040)
2023-10-30 17:07:54,594:INFO:Physical Core: 10
2023-10-30 17:07:54,594:INFO:Logical Core: 10
2023-10-30 17:07:54,594:INFO:Checking libraries
2023-10-30 17:07:54,594:INFO:System:
2023-10-30 17:07:54,594:INFO:    python: 3.10.8 | packaged by conda-forge | (main, Nov 22 2022, 08:25:13) [Clang 14.0.6 ]
2023-10-30 17:07:54,594:INFO:executable: /Users/tanmaysharma/projects/Jak2Biotech/.venv/bin/python
2023-10-30 17:07:54,594:INFO:   machine: macOS-14.0-arm64-arm-64bit
2023-10-30 17:07:54,594:INFO:PyCaret required dependencies:
2023-10-30 17:07:55,226:INFO:                 pip: 23.3.1
2023-10-30 17:07:55,226:INFO:          setuptools: 63.2.0
2023-10-30 17:07:55,226:INFO:             pycaret: 3.1.0
2023-10-30 17:07:55,226:INFO:             IPython: 8.16.1
2023-10-30 17:07:55,226:INFO:          ipywidgets: 8.1.1
2023-10-30 17:07:55,226:INFO:                tqdm: 4.66.1
2023-10-30 17:07:55,226:INFO:               numpy: 1.23.5
2023-10-30 17:07:55,226:INFO:              pandas: 1.5.3
2023-10-30 17:07:55,226:INFO:              jinja2: 3.1.2
2023-10-30 17:07:55,226:INFO:               scipy: 1.10.1
2023-10-30 17:07:55,226:INFO:              joblib: 1.3.2
2023-10-30 17:07:55,226:INFO:             sklearn: 1.2.2
2023-10-30 17:07:55,226:INFO:                pyod: 1.1.1
2023-10-30 17:07:55,226:INFO:            imblearn: 0.11.0
2023-10-30 17:07:55,226:INFO:   category_encoders: 2.6.2
2023-10-30 17:07:55,226:INFO:            lightgbm: 4.1.0
2023-10-30 17:07:55,226:INFO:               numba: 0.58.1
2023-10-30 17:07:55,226:INFO:            requests: 2.31.0
2023-10-30 17:07:55,226:INFO:          matplotlib: 3.6.0
2023-10-30 17:07:55,226:INFO:          scikitplot: 0.3.7
2023-10-30 17:07:55,226:INFO:         yellowbrick: 1.5
2023-10-30 17:07:55,226:INFO:              plotly: 5.18.0
2023-10-30 17:07:55,226:INFO:    plotly-resampler: Not installed
2023-10-30 17:07:55,226:INFO:             kaleido: 0.2.1
2023-10-30 17:07:55,226:INFO:           schemdraw: 0.15
2023-10-30 17:07:55,226:INFO:         statsmodels: 0.14.0
2023-10-30 17:07:55,226:INFO:              sktime: 0.21.1
2023-10-30 17:07:55,226:INFO:               tbats: 1.1.3
2023-10-30 17:07:55,226:INFO:            pmdarima: 2.0.4
2023-10-30 17:07:55,226:INFO:              psutil: 5.9.6
2023-10-30 17:07:55,226:INFO:          markupsafe: 2.1.3
2023-10-30 17:07:55,226:INFO:             pickle5: Not installed
2023-10-30 17:07:55,226:INFO:         cloudpickle: 2.2.1
2023-10-30 17:07:55,226:INFO:         deprecation: 2.1.0
2023-10-30 17:07:55,226:INFO:              xxhash: 3.4.1
2023-10-30 17:07:55,226:INFO:           wurlitzer: 3.0.3
2023-10-30 17:07:55,226:INFO:PyCaret optional dependencies:
2023-10-30 17:07:55,826:WARNING:/Users/tanmaysharma/projects/Jak2Biotech/.venv/lib/python3.10/site-packages/trio/_core/_multierror.py:412: RuntimeWarning: IPython detected, but you already have a custom exception handler installed. I'll skip installing Trio's custom handler, but this means exception groups will not show full tracebacks.
  warnings.warn(

2023-10-30 17:08:06,645:INFO:                shap: 0.43.0
2023-10-30 17:08:06,645:INFO:           interpret: 0.4.4
2023-10-30 17:08:06,645:INFO:                umap: 0.5.4
2023-10-30 17:08:06,645:INFO:     ydata_profiling: 4.6.0
2023-10-30 17:08:06,645:INFO:  explainerdashboard: 0.4.3
2023-10-30 17:08:06,645:INFO:             autoviz: Not installed
2023-10-30 17:08:06,645:INFO:           fairlearn: 0.7.0
2023-10-30 17:08:06,645:INFO:          deepchecks: Not installed
2023-10-30 17:08:06,645:INFO:             xgboost: Not installed
2023-10-30 17:08:06,645:INFO:            catboost: 1.1.1
2023-10-30 17:08:06,645:INFO:              kmodes: 0.12.2
2023-10-30 17:08:06,645:INFO:             mlxtend: 0.23.0
2023-10-30 17:08:06,645:INFO:       statsforecast: 1.5.0
2023-10-30 17:08:06,645:INFO:        tune_sklearn: 0.4.6
2023-10-30 17:08:06,645:INFO:                 ray: 2.7.1
2023-10-30 17:08:06,645:INFO:            hyperopt: 0.2.7
2023-10-30 17:08:06,645:INFO:              optuna: 3.4.0
2023-10-30 17:08:06,645:INFO:               skopt: 0.9.0
2023-10-30 17:08:06,645:INFO:              mlflow: 1.30.1
2023-10-30 17:08:06,645:INFO:              gradio: 3.50.2
2023-10-30 17:08:06,645:INFO:             fastapi: 0.104.0
2023-10-30 17:08:06,645:INFO:             uvicorn: 0.23.2
2023-10-30 17:08:06,645:INFO:              m2cgen: 0.10.0
2023-10-30 17:08:06,645:INFO:           evidently: 0.2.8
2023-10-30 17:08:06,645:INFO:               fugue: 0.8.6
2023-10-30 17:08:06,645:INFO:           streamlit: Not installed
2023-10-30 17:08:06,645:INFO:             prophet: Not installed
2023-10-30 17:08:06,645:INFO:None
2023-10-30 17:08:06,645:INFO:Set up data.
2023-10-30 17:08:06,690:INFO:Set up folding strategy.
2023-10-30 17:08:06,690:INFO:Set up train/test split.
2023-10-30 17:08:06,703:INFO:Set up index.
2023-10-30 17:08:06,704:INFO:Assigning column types.
2023-10-30 17:08:06,710:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2023-10-30 17:08:06,729:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-10-30 17:08:06,730:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-10-30 17:08:06,746:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-10-30 17:08:06,747:INFO:Soft dependency imported: catboost: 1.1.1
2023-10-30 17:08:08,472:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-10-30 17:08:08,472:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-10-30 17:08:08,487:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-10-30 17:08:08,487:INFO:Soft dependency imported: catboost: 1.1.1
2023-10-30 17:08:08,487:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2023-10-30 17:08:08,506:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-10-30 17:08:08,519:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-10-30 17:08:08,519:INFO:Soft dependency imported: catboost: 1.1.1
2023-10-30 17:08:08,538:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-10-30 17:08:08,551:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-10-30 17:08:08,551:INFO:Soft dependency imported: catboost: 1.1.1
2023-10-30 17:08:08,551:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2023-10-30 17:08:08,583:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-10-30 17:08:08,583:INFO:Soft dependency imported: catboost: 1.1.1
2023-10-30 17:08:08,616:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-10-30 17:08:08,616:INFO:Soft dependency imported: catboost: 1.1.1
2023-10-30 17:08:08,618:INFO:Preparing preprocessing pipeline...
2023-10-30 17:08:08,620:INFO:Set up simple imputation.
2023-10-30 17:08:08,620:INFO:Set up variance threshold.
2023-10-30 17:08:08,620:INFO:Set up removing multicollinearity.
2023-10-30 17:08:08,620:INFO:Set up removing outliers.
2023-10-30 17:08:08,620:INFO:Set up PCA.
2023-10-30 17:08:08,621:INFO:Set up feature selection.
2023-10-30 17:08:08,653:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-10-30 17:08:08,653:INFO:Soft dependency imported: catboost: 1.1.1
2023-10-30 17:08:08,657:INFO:Set up column name cleaning.
2023-10-30 17:17:56,448:INFO:Finished creating preprocessing pipeline.
2023-10-30 17:17:56,457:INFO:Pipeline: Pipeline(memory=FastMemory(location=/var/folders/f1/19ck_yvd7p355g2p8r2jyzlc0000gn/T/joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['GATS1c', 'SlogP_VSA2', 'SLogP',
                                             'MDEN-23', 'MINdsCH', 'MINssCH2',
                                             'GATS1s', 'GATS2m', 'SaaO',
                                             'BCUTZ-1h', 'SdsN', 'MINaaN',
                                             'MAXssCH2', 'MATS1s', 'GATS5d',
                                             'Xch-5d', 'SMR_VSA7',
                                             'ETA_dEpsilon_B', 'VSA_ESta...
                                                                                         objective=None,
                                                                                         random_state=None,
                                                                                         reg_alpha=0.0,
                                                                                         reg_lambda=0.0,
                                                                                         subsample=1.0,
                                                                                         subsample_for_bin=200000,
                                                                                         subsample_freq=0),
                                                                importance_getter='auto',
                                                                max_features=59,
                                                                norm_order=1,
                                                                prefit=False,
                                                                threshold=-inf))),
                ('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+')))],
         verbose=False)
2023-10-30 17:17:56,457:INFO:Creating final display dataframe.
2023-10-30 17:26:48,483:INFO:Setup _display_container:                     Description             Value
0                    Session id              1784
1                        Target               cls
2                   Target type            Binary
3           Original data shape       (9440, 298)
4        Transformed data shape        (9086, 60)
5   Transformed train set shape        (6726, 60)
6    Transformed test set shape        (2360, 60)
7              Numeric features               297
8      Rows with missing values            100.0%
9                    Preprocess              True
10              Imputation type            simple
11           Numeric imputation            median
12       Categorical imputation              mode
13       Low variance threshold              0.05
14     Remove multicollinearity              True
15  Multicollinearity threshold               0.8
16              Remove outliers              True
17           Outliers threshold              0.05
18                          PCA              True
19                   PCA method            kernel
20               PCA components              None
21            Feature selection              True
22     Feature selection method           classic
23  Feature selection estimator          lightgbm
24  Number of features selected               0.2
25               Fold Generator   StratifiedKFold
26                  Fold Number                10
27                     CPU Jobs                -1
28                      Use GPU             False
29               Log Experiment             False
30              Experiment Name  clf-default-name
31                          USI              612f
2023-10-30 17:26:48,527:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-10-30 17:26:48,527:INFO:Soft dependency imported: catboost: 1.1.1
2023-10-30 17:26:48,562:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-10-30 17:26:48,563:INFO:Soft dependency imported: catboost: 1.1.1
2023-10-30 17:26:48,563:INFO:setup() successfully completed in 1133.97s...............
2023-10-30 17:26:48,568:INFO:Initializing create_model()
2023-10-30 17:26:48,568:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x2a279fcd0>, estimator=xgboost, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-10-30 17:26:48,568:INFO:Checking exceptions
2023-10-30 17:27:38,615:INFO:Initializing create_model()
2023-10-30 17:27:38,616:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x2a279fcd0>, estimator=xgboost, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-10-30 17:27:38,616:INFO:Checking exceptions
2023-10-30 17:31:18,345:INFO:Initializing compare_models()
2023-10-30 17:31:18,346:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x2a279fcd0>, include=None, fold=None, round=4, cross_validation=True, sort=Precision, n_select=5, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x2a279fcd0>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'Precision', 'n_select': 5, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>}, exclude=None)
2023-10-30 17:31:18,346:INFO:Checking exceptions
2023-10-30 17:31:18,358:INFO:Preparing display monitor
2023-10-30 17:31:18,392:INFO:Initializing Logistic Regression
2023-10-30 17:31:18,393:INFO:Total runtime is 4.34716542561849e-06 minutes
2023-10-30 17:31:18,394:INFO:SubProcess create_model() called ==================================
2023-10-30 17:31:18,395:INFO:Initializing create_model()
2023-10-30 17:31:18,395:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x2a279fcd0>, estimator=lr, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x2b258bb20>, model_only=True, return_train_score=False, kwargs={})
2023-10-30 17:31:18,395:INFO:Checking exceptions
2023-10-30 17:31:18,395:INFO:Importing libraries
2023-10-30 17:31:18,395:INFO:Copying training dataset
2023-10-30 17:31:18,414:INFO:Defining folds
2023-10-30 17:31:18,414:INFO:Declaring metric variables
2023-10-30 17:31:18,416:INFO:Importing untrained model
2023-10-30 17:31:18,418:INFO:Logistic Regression Imported successfully
2023-10-30 17:31:18,421:INFO:Starting cross validation
2023-10-30 17:31:18,455:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-10-30 17:32:44,492:INFO:[LightGBM] [Info] Number of positive: 2942, number of negative: 3110
2023-10-30 17:32:45,088:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.595627 seconds.
2023-10-30 17:32:45,088:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-30 17:32:45,091:INFO:[LightGBM] [Info] Total Bins 1294635
2023-10-30 17:32:45,264:INFO:[LightGBM] [Info] Number of data points in the train set: 6052, number of used features: 5077
2023-10-30 17:32:45,272:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.486120 -> initscore=-0.055533
2023-10-30 17:32:45,272:INFO:[LightGBM] [Info] Start training from score -0.055533
2023-10-30 17:32:47,338:INFO:[LightGBM] [Info] Number of positive: 2975, number of negative: 3077
2023-10-30 17:32:47,481:INFO:[LightGBM] [Info] Number of positive: 2929, number of negative: 3124
2023-10-30 17:32:47,859:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.517771 seconds.
2023-10-30 17:32:47,859:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-30 17:32:47,869:INFO:[LightGBM] [Info] Total Bins 1289280
2023-10-30 17:32:47,927:INFO:[LightGBM] [Info] Number of positive: 2952, number of negative: 3101
2023-10-30 17:32:48,167:INFO:[LightGBM] [Info] Number of data points in the train set: 6052, number of used features: 5056
2023-10-30 17:32:48,180:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.491573 -> initscore=-0.033711
2023-10-30 17:32:48,180:INFO:[LightGBM] [Info] Start training from score -0.033711
2023-10-30 17:32:48,256:INFO:[LightGBM] [Info] Number of positive: 2954, number of negative: 3099
2023-10-30 17:32:48,336:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.840087 seconds.
2023-10-30 17:32:48,336:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-30 17:32:48,342:INFO:[LightGBM] [Info] Total Bins 1291830
2023-10-30 17:32:48,513:INFO:[LightGBM] [Info] Number of data points in the train set: 6053, number of used features: 5066
2023-10-30 17:32:48,522:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.483892 -> initscore=-0.064453
2023-10-30 17:32:48,522:INFO:[LightGBM] [Info] Start training from score -0.064453
2023-10-30 17:32:48,727:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.779471 seconds.
2023-10-30 17:32:48,727:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-30 17:32:48,735:INFO:[LightGBM] [Info] Total Bins 1286730
2023-10-30 17:32:48,820:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.559500 seconds.
2023-10-30 17:32:48,821:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-30 17:32:48,836:INFO:[LightGBM] [Info] Total Bins 1290810
2023-10-30 17:32:49,007:INFO:[LightGBM] [Info] Number of data points in the train set: 6053, number of used features: 5046
2023-10-30 17:32:49,021:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.487692 -> initscore=-0.049242
2023-10-30 17:32:49,021:INFO:[LightGBM] [Info] Start training from score -0.049242
2023-10-30 17:32:49,123:INFO:[LightGBM] [Info] Number of data points in the train set: 6053, number of used features: 5062
2023-10-30 17:32:49,143:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.488022 -> initscore=-0.047919
2023-10-30 17:32:49,143:INFO:[LightGBM] [Info] Start training from score -0.047919
2023-10-30 17:32:49,261:INFO:[LightGBM] [Info] Number of positive: 2921, number of negative: 3132
2023-10-30 17:32:49,526:INFO:[LightGBM] [Info] Number of positive: 2946, number of negative: 3107
2023-10-30 17:32:49,712:INFO:[LightGBM] [Info] Number of positive: 2956, number of negative: 3097
2023-10-30 17:32:50,046:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.782539 seconds.
2023-10-30 17:32:50,047:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-30 17:32:50,055:INFO:[LightGBM] [Info] Total Bins 1294890
2023-10-30 17:32:50,281:INFO:[LightGBM] [Info] Number of data points in the train set: 6053, number of used features: 5078
2023-10-30 17:32:50,295:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.482571 -> initscore=-0.069746
2023-10-30 17:32:50,295:INFO:[LightGBM] [Info] Start training from score -0.069746
2023-10-30 17:32:50,314:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.569752 seconds.
2023-10-30 17:32:50,314:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-30 17:32:50,322:INFO:[LightGBM] [Info] Total Bins 1289790
2023-10-30 17:32:50,370:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.839909 seconds.
2023-10-30 17:32:50,370:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-30 17:32:50,384:INFO:[LightGBM] [Info] Total Bins 1294890
2023-10-30 17:32:50,553:INFO:[LightGBM] [Info] Number of data points in the train set: 6053, number of used features: 5058
2023-10-30 17:32:50,582:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.488353 -> initscore=-0.046597
2023-10-30 17:32:50,582:INFO:[LightGBM] [Info] Start training from score -0.046597
2023-10-30 17:32:50,628:INFO:[LightGBM] [Info] Number of data points in the train set: 6053, number of used features: 5078
2023-10-30 17:32:50,640:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.486701 -> initscore=-0.053209
2023-10-30 17:32:50,640:INFO:[LightGBM] [Info] Start training from score -0.053209
2023-10-30 17:32:51,431:INFO:[LightGBM] [Info] Number of positive: 2944, number of negative: 3109
2023-10-30 17:32:52,089:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.647619 seconds.
2023-10-30 17:32:52,090:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-30 17:32:52,102:INFO:[LightGBM] [Info] Total Bins 1292085
2023-10-30 17:32:52,330:INFO:[LightGBM] [Info] Number of data points in the train set: 6053, number of used features: 5067
2023-10-30 17:32:52,359:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.486370 -> initscore=-0.054532
2023-10-30 17:32:52,360:INFO:[LightGBM] [Info] Start training from score -0.054532
2023-10-30 17:34:45,797:INFO:[LightGBM] [Info] Number of positive: 2940, number of negative: 3113
2023-10-30 17:34:46,427:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.615218 seconds.
2023-10-30 17:34:46,429:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-30 17:34:46,438:INFO:[LightGBM] [Info] Total Bins 1291830
2023-10-30 17:34:46,671:INFO:[LightGBM] [Info] Number of data points in the train set: 6053, number of used features: 5066
2023-10-30 17:34:46,695:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.485710 -> initscore=-0.057177
2023-10-30 17:34:46,696:INFO:[LightGBM] [Info] Start training from score -0.057177
2023-10-30 17:35:50,610:INFO:Calculating mean and std
2023-10-30 17:35:50,625:INFO:Creating metrics dataframe
2023-10-30 17:35:50,663:INFO:Uploading results into container
2023-10-30 17:35:50,665:INFO:Uploading model into container now
2023-10-30 17:35:50,667:INFO:_master_model_container: 1
2023-10-30 17:35:50,667:INFO:_display_container: 2
2023-10-30 17:35:50,669:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=1784, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2023-10-30 17:35:50,669:INFO:create_model() successfully completed......................................
2023-10-30 17:35:50,937:INFO:SubProcess create_model() end ==================================
2023-10-30 17:35:50,937:INFO:Creating metrics dataframe
2023-10-30 17:35:50,941:INFO:Initializing K Neighbors Classifier
2023-10-30 17:35:50,941:INFO:Total runtime is 4.542484529813131 minutes
2023-10-30 17:35:50,943:INFO:SubProcess create_model() called ==================================
2023-10-30 17:35:50,943:INFO:Initializing create_model()
2023-10-30 17:35:50,943:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x2a279fcd0>, estimator=knn, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x2b258bb20>, model_only=True, return_train_score=False, kwargs={})
2023-10-30 17:35:50,943:INFO:Checking exceptions
2023-10-30 17:35:50,943:INFO:Importing libraries
2023-10-30 17:35:50,943:INFO:Copying training dataset
2023-10-30 17:35:50,962:INFO:Defining folds
2023-10-30 17:35:50,962:INFO:Declaring metric variables
2023-10-30 17:35:50,963:INFO:Importing untrained model
2023-10-30 17:35:50,966:INFO:K Neighbors Classifier Imported successfully
2023-10-30 17:35:50,968:INFO:Starting cross validation
2023-10-30 17:35:51,067:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-10-30 17:40:19,628:INFO:Calculating mean and std
2023-10-30 17:40:19,630:INFO:Creating metrics dataframe
2023-10-30 17:40:19,646:INFO:Uploading results into container
2023-10-30 17:40:19,646:INFO:Uploading model into container now
2023-10-30 17:40:19,647:INFO:_master_model_container: 2
2023-10-30 17:40:19,647:INFO:_display_container: 2
2023-10-30 17:40:19,648:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2023-10-30 17:40:19,648:INFO:create_model() successfully completed......................................
2023-10-30 17:40:19,765:INFO:SubProcess create_model() end ==================================
2023-10-30 17:40:19,765:INFO:Creating metrics dataframe
2023-10-30 17:40:19,770:INFO:Initializing Naive Bayes
2023-10-30 17:40:19,771:INFO:Total runtime is 9.022968884309133 minutes
2023-10-30 17:40:19,772:INFO:SubProcess create_model() called ==================================
2023-10-30 17:40:19,772:INFO:Initializing create_model()
2023-10-30 17:40:19,772:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x2a279fcd0>, estimator=nb, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x2b258bb20>, model_only=True, return_train_score=False, kwargs={})
2023-10-30 17:40:19,773:INFO:Checking exceptions
2023-10-30 17:40:19,773:INFO:Importing libraries
2023-10-30 17:40:19,773:INFO:Copying training dataset
2023-10-30 17:40:19,790:INFO:Defining folds
2023-10-30 17:40:19,790:INFO:Declaring metric variables
2023-10-30 17:40:19,792:INFO:Importing untrained model
2023-10-30 17:40:19,793:INFO:Naive Bayes Imported successfully
2023-10-30 17:40:19,796:INFO:Starting cross validation
2023-10-30 17:40:19,863:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-10-30 17:43:41,166:INFO:[LightGBM] [Info] Number of positive: 2940, number of negative: 3113
2023-10-30 17:43:41,782:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.603873 seconds.
2023-10-30 17:43:41,782:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-30 17:43:41,792:INFO:[LightGBM] [Info] Total Bins 1291830
2023-10-30 17:43:42,020:INFO:[LightGBM] [Info] Number of data points in the train set: 6053, number of used features: 5066
2023-10-30 17:43:42,033:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.485710 -> initscore=-0.057177
2023-10-30 17:43:42,035:INFO:[LightGBM] [Info] Start training from score -0.057177
2023-10-30 17:44:46,823:INFO:Calculating mean and std
2023-10-30 17:44:46,826:INFO:Creating metrics dataframe
2023-10-30 17:44:46,848:INFO:Uploading results into container
2023-10-30 17:44:46,848:INFO:Uploading model into container now
2023-10-30 17:44:46,849:INFO:_master_model_container: 3
2023-10-30 17:44:46,849:INFO:_display_container: 2
2023-10-30 17:44:46,849:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2023-10-30 17:44:46,849:INFO:create_model() successfully completed......................................
2023-10-30 17:44:46,956:INFO:SubProcess create_model() end ==================================
2023-10-30 17:44:46,956:INFO:Creating metrics dataframe
2023-10-30 17:44:46,962:INFO:Initializing Decision Tree Classifier
2023-10-30 17:44:46,962:INFO:Total runtime is 13.476159981886546 minutes
2023-10-30 17:44:46,964:INFO:SubProcess create_model() called ==================================
2023-10-30 17:44:46,964:INFO:Initializing create_model()
2023-10-30 17:44:46,964:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x2a279fcd0>, estimator=dt, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x2b258bb20>, model_only=True, return_train_score=False, kwargs={})
2023-10-30 17:44:46,964:INFO:Checking exceptions
2023-10-30 17:44:46,964:INFO:Importing libraries
2023-10-30 17:44:46,964:INFO:Copying training dataset
2023-10-30 17:44:46,978:INFO:Defining folds
2023-10-30 17:44:46,978:INFO:Declaring metric variables
2023-10-30 17:44:46,980:INFO:Importing untrained model
2023-10-30 17:44:46,981:INFO:Decision Tree Classifier Imported successfully
2023-10-30 17:44:46,984:INFO:Starting cross validation
2023-10-30 17:44:47,108:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-10-30 17:51:44,325:INFO:Calculating mean and std
2023-10-30 17:51:44,329:INFO:Creating metrics dataframe
2023-10-30 17:51:44,352:INFO:Uploading results into container
2023-10-30 17:51:44,352:INFO:Uploading model into container now
2023-10-30 17:51:44,353:INFO:_master_model_container: 4
2023-10-30 17:51:44,353:INFO:_display_container: 2
2023-10-30 17:51:44,354:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       random_state=1784, splitter='best')
2023-10-30 17:51:44,354:INFO:create_model() successfully completed......................................
2023-10-30 17:51:44,474:INFO:SubProcess create_model() end ==================================
2023-10-30 17:51:44,475:INFO:Creating metrics dataframe
2023-10-30 17:51:44,480:INFO:Initializing SVM - Linear Kernel
2023-10-30 17:51:44,481:INFO:Total runtime is 20.43480381568273 minutes
2023-10-30 17:51:44,483:INFO:SubProcess create_model() called ==================================
2023-10-30 17:51:44,483:INFO:Initializing create_model()
2023-10-30 17:51:44,483:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x2a279fcd0>, estimator=svm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x2b258bb20>, model_only=True, return_train_score=False, kwargs={})
2023-10-30 17:51:44,483:INFO:Checking exceptions
2023-10-30 17:51:44,483:INFO:Importing libraries
2023-10-30 17:51:44,483:INFO:Copying training dataset
2023-10-30 17:51:44,499:INFO:Defining folds
2023-10-30 17:51:44,499:INFO:Declaring metric variables
2023-10-30 17:51:44,501:INFO:Importing untrained model
2023-10-30 17:51:44,503:INFO:SVM - Linear Kernel Imported successfully
2023-10-30 17:51:44,507:INFO:Starting cross validation
2023-10-30 17:51:44,585:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-10-30 17:55:43,737:INFO:[LightGBM] [Info] Number of positive: 2940, number of negative: 3113
2023-10-30 17:55:44,343:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.597060 seconds.
2023-10-30 17:55:44,351:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-30 17:55:44,355:INFO:[LightGBM] [Info] Total Bins 1291830
2023-10-30 17:55:44,570:INFO:[LightGBM] [Info] Number of data points in the train set: 6053, number of used features: 5066
2023-10-30 17:55:44,588:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.485710 -> initscore=-0.057177
2023-10-30 17:55:44,588:INFO:[LightGBM] [Info] Start training from score -0.057177
2023-10-30 17:56:14,610:WARNING:/Users/tanmaysharma/projects/Jak2Biotech/.venv/lib/python3.10/site-packages/pycaret/internal/metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/Users/tanmaysharma/projects/Jak2Biotech/.venv/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/tanmaysharma/projects/Jak2Biotech/.venv/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 182, in _score
    return super()._score(
  File "/Users/tanmaysharma/projects/Jak2Biotech/.venv/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "/Users/tanmaysharma/projects/Jak2Biotech/.venv/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "/Users/tanmaysharma/projects/Jak2Biotech/.venv/lib/python3.10/site-packages/sklearn/utils/_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "/Users/tanmaysharma/projects/Jak2Biotech/.venv/lib/python3.10/site-packages/sklearn/linear_model/_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'. Did you mean: '_predict_proba_lr'?

  warnings.warn(

2023-10-30 17:56:18,563:WARNING:/Users/tanmaysharma/projects/Jak2Biotech/.venv/lib/python3.10/site-packages/pycaret/internal/metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/Users/tanmaysharma/projects/Jak2Biotech/.venv/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/tanmaysharma/projects/Jak2Biotech/.venv/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 182, in _score
    return super()._score(
  File "/Users/tanmaysharma/projects/Jak2Biotech/.venv/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "/Users/tanmaysharma/projects/Jak2Biotech/.venv/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "/Users/tanmaysharma/projects/Jak2Biotech/.venv/lib/python3.10/site-packages/sklearn/utils/_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "/Users/tanmaysharma/projects/Jak2Biotech/.venv/lib/python3.10/site-packages/sklearn/linear_model/_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'. Did you mean: '_predict_proba_lr'?

  warnings.warn(

2023-10-30 17:56:18,913:WARNING:/Users/tanmaysharma/projects/Jak2Biotech/.venv/lib/python3.10/site-packages/pycaret/internal/metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/Users/tanmaysharma/projects/Jak2Biotech/.venv/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/tanmaysharma/projects/Jak2Biotech/.venv/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 182, in _score
    return super()._score(
  File "/Users/tanmaysharma/projects/Jak2Biotech/.venv/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "/Users/tanmaysharma/projects/Jak2Biotech/.venv/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "/Users/tanmaysharma/projects/Jak2Biotech/.venv/lib/python3.10/site-packages/sklearn/utils/_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "/Users/tanmaysharma/projects/Jak2Biotech/.venv/lib/python3.10/site-packages/sklearn/linear_model/_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'. Did you mean: '_predict_proba_lr'?

  warnings.warn(

2023-10-30 17:56:19,004:WARNING:/Users/tanmaysharma/projects/Jak2Biotech/.venv/lib/python3.10/site-packages/pycaret/internal/metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/Users/tanmaysharma/projects/Jak2Biotech/.venv/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/tanmaysharma/projects/Jak2Biotech/.venv/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 182, in _score
    return super()._score(
  File "/Users/tanmaysharma/projects/Jak2Biotech/.venv/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "/Users/tanmaysharma/projects/Jak2Biotech/.venv/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "/Users/tanmaysharma/projects/Jak2Biotech/.venv/lib/python3.10/site-packages/sklearn/utils/_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "/Users/tanmaysharma/projects/Jak2Biotech/.venv/lib/python3.10/site-packages/sklearn/linear_model/_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'. Did you mean: '_predict_proba_lr'?

  warnings.warn(

2023-10-30 17:56:19,603:WARNING:/Users/tanmaysharma/projects/Jak2Biotech/.venv/lib/python3.10/site-packages/pycaret/internal/metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/Users/tanmaysharma/projects/Jak2Biotech/.venv/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/tanmaysharma/projects/Jak2Biotech/.venv/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 182, in _score
    return super()._score(
  File "/Users/tanmaysharma/projects/Jak2Biotech/.venv/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "/Users/tanmaysharma/projects/Jak2Biotech/.venv/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "/Users/tanmaysharma/projects/Jak2Biotech/.venv/lib/python3.10/site-packages/sklearn/utils/_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "/Users/tanmaysharma/projects/Jak2Biotech/.venv/lib/python3.10/site-packages/sklearn/linear_model/_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'. Did you mean: '_predict_proba_lr'?

  warnings.warn(

2023-10-30 17:56:19,737:WARNING:/Users/tanmaysharma/projects/Jak2Biotech/.venv/lib/python3.10/site-packages/pycaret/internal/metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/Users/tanmaysharma/projects/Jak2Biotech/.venv/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/tanmaysharma/projects/Jak2Biotech/.venv/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 182, in _score
    return super()._score(
  File "/Users/tanmaysharma/projects/Jak2Biotech/.venv/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "/Users/tanmaysharma/projects/Jak2Biotech/.venv/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "/Users/tanmaysharma/projects/Jak2Biotech/.venv/lib/python3.10/site-packages/sklearn/utils/_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "/Users/tanmaysharma/projects/Jak2Biotech/.venv/lib/python3.10/site-packages/sklearn/linear_model/_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'. Did you mean: '_predict_proba_lr'?

  warnings.warn(

2023-10-30 17:56:19,876:WARNING:/Users/tanmaysharma/projects/Jak2Biotech/.venv/lib/python3.10/site-packages/pycaret/internal/metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/Users/tanmaysharma/projects/Jak2Biotech/.venv/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/tanmaysharma/projects/Jak2Biotech/.venv/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 182, in _score
    return super()._score(
  File "/Users/tanmaysharma/projects/Jak2Biotech/.venv/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "/Users/tanmaysharma/projects/Jak2Biotech/.venv/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "/Users/tanmaysharma/projects/Jak2Biotech/.venv/lib/python3.10/site-packages/sklearn/utils/_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "/Users/tanmaysharma/projects/Jak2Biotech/.venv/lib/python3.10/site-packages/sklearn/linear_model/_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'. Did you mean: '_predict_proba_lr'?

  warnings.warn(

2023-10-30 17:56:19,885:WARNING:/Users/tanmaysharma/projects/Jak2Biotech/.venv/lib/python3.10/site-packages/pycaret/internal/metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/Users/tanmaysharma/projects/Jak2Biotech/.venv/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/tanmaysharma/projects/Jak2Biotech/.venv/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 182, in _score
    return super()._score(
  File "/Users/tanmaysharma/projects/Jak2Biotech/.venv/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "/Users/tanmaysharma/projects/Jak2Biotech/.venv/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "/Users/tanmaysharma/projects/Jak2Biotech/.venv/lib/python3.10/site-packages/sklearn/utils/_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "/Users/tanmaysharma/projects/Jak2Biotech/.venv/lib/python3.10/site-packages/sklearn/linear_model/_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'. Did you mean: '_predict_proba_lr'?

  warnings.warn(

2023-10-30 17:56:20,403:WARNING:/Users/tanmaysharma/projects/Jak2Biotech/.venv/lib/python3.10/site-packages/pycaret/internal/metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/Users/tanmaysharma/projects/Jak2Biotech/.venv/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/tanmaysharma/projects/Jak2Biotech/.venv/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 182, in _score
    return super()._score(
  File "/Users/tanmaysharma/projects/Jak2Biotech/.venv/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "/Users/tanmaysharma/projects/Jak2Biotech/.venv/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "/Users/tanmaysharma/projects/Jak2Biotech/.venv/lib/python3.10/site-packages/sklearn/utils/_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "/Users/tanmaysharma/projects/Jak2Biotech/.venv/lib/python3.10/site-packages/sklearn/linear_model/_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'. Did you mean: '_predict_proba_lr'?

  warnings.warn(

2023-10-30 17:56:41,251:WARNING:/Users/tanmaysharma/projects/Jak2Biotech/.venv/lib/python3.10/site-packages/pycaret/internal/metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/Users/tanmaysharma/projects/Jak2Biotech/.venv/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/tanmaysharma/projects/Jak2Biotech/.venv/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 182, in _score
    return super()._score(
  File "/Users/tanmaysharma/projects/Jak2Biotech/.venv/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "/Users/tanmaysharma/projects/Jak2Biotech/.venv/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "/Users/tanmaysharma/projects/Jak2Biotech/.venv/lib/python3.10/site-packages/sklearn/utils/_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "/Users/tanmaysharma/projects/Jak2Biotech/.venv/lib/python3.10/site-packages/sklearn/linear_model/_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'. Did you mean: '_predict_proba_lr'?

  warnings.warn(

2023-10-30 17:56:41,368:INFO:Calculating mean and std
2023-10-30 17:56:41,370:INFO:Creating metrics dataframe
2023-10-30 17:56:41,382:INFO:Uploading results into container
2023-10-30 17:56:41,383:INFO:Uploading model into container now
2023-10-30 17:56:41,383:INFO:_master_model_container: 5
2023-10-30 17:56:41,383:INFO:_display_container: 2
2023-10-30 17:56:41,384:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=1784, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2023-10-30 17:56:41,384:INFO:create_model() successfully completed......................................
2023-10-30 17:56:41,489:INFO:SubProcess create_model() end ==================================
2023-10-30 17:56:41,489:INFO:Creating metrics dataframe
2023-10-30 17:56:41,495:INFO:Initializing Ridge Classifier
2023-10-30 17:56:41,495:INFO:Total runtime is 25.385047467549644 minutes
2023-10-30 17:56:41,497:INFO:SubProcess create_model() called ==================================
2023-10-30 17:56:41,497:INFO:Initializing create_model()
2023-10-30 17:56:41,497:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x2a279fcd0>, estimator=ridge, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x2b258bb20>, model_only=True, return_train_score=False, kwargs={})
2023-10-30 17:56:41,497:INFO:Checking exceptions
2023-10-30 17:56:41,497:INFO:Importing libraries
2023-10-30 17:56:41,497:INFO:Copying training dataset
2023-10-30 17:56:41,511:INFO:Defining folds
2023-10-30 17:56:41,512:INFO:Declaring metric variables
2023-10-30 17:56:41,514:INFO:Importing untrained model
2023-10-30 17:56:41,515:INFO:Ridge Classifier Imported successfully
2023-10-30 17:56:41,519:INFO:Starting cross validation
2023-10-30 17:56:41,581:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-10-30 18:00:26,243:WARNING:/Users/tanmaysharma/projects/Jak2Biotech/.venv/lib/python3.10/site-packages/pycaret/internal/metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/Users/tanmaysharma/projects/Jak2Biotech/.venv/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/tanmaysharma/projects/Jak2Biotech/.venv/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 182, in _score
    return super()._score(
  File "/Users/tanmaysharma/projects/Jak2Biotech/.venv/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "/Users/tanmaysharma/projects/Jak2Biotech/.venv/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'. Did you mean: '_predict_proba_lr'?

  warnings.warn(

2023-10-30 18:00:30,170:WARNING:/Users/tanmaysharma/projects/Jak2Biotech/.venv/lib/python3.10/site-packages/pycaret/internal/metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/Users/tanmaysharma/projects/Jak2Biotech/.venv/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/tanmaysharma/projects/Jak2Biotech/.venv/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 182, in _score
    return super()._score(
  File "/Users/tanmaysharma/projects/Jak2Biotech/.venv/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "/Users/tanmaysharma/projects/Jak2Biotech/.venv/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'. Did you mean: '_predict_proba_lr'?

  warnings.warn(

2023-10-30 18:00:30,428:WARNING:/Users/tanmaysharma/projects/Jak2Biotech/.venv/lib/python3.10/site-packages/pycaret/internal/metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/Users/tanmaysharma/projects/Jak2Biotech/.venv/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/tanmaysharma/projects/Jak2Biotech/.venv/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 182, in _score
    return super()._score(
  File "/Users/tanmaysharma/projects/Jak2Biotech/.venv/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "/Users/tanmaysharma/projects/Jak2Biotech/.venv/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'. Did you mean: '_predict_proba_lr'?

  warnings.warn(

2023-10-30 18:00:30,504:WARNING:/Users/tanmaysharma/projects/Jak2Biotech/.venv/lib/python3.10/site-packages/pycaret/internal/metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/Users/tanmaysharma/projects/Jak2Biotech/.venv/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/tanmaysharma/projects/Jak2Biotech/.venv/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 182, in _score
    return super()._score(
  File "/Users/tanmaysharma/projects/Jak2Biotech/.venv/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "/Users/tanmaysharma/projects/Jak2Biotech/.venv/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'. Did you mean: '_predict_proba_lr'?

  warnings.warn(

2023-10-30 18:00:31,169:WARNING:/Users/tanmaysharma/projects/Jak2Biotech/.venv/lib/python3.10/site-packages/pycaret/internal/metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/Users/tanmaysharma/projects/Jak2Biotech/.venv/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/tanmaysharma/projects/Jak2Biotech/.venv/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 182, in _score
    return super()._score(
  File "/Users/tanmaysharma/projects/Jak2Biotech/.venv/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "/Users/tanmaysharma/projects/Jak2Biotech/.venv/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'. Did you mean: '_predict_proba_lr'?

  warnings.warn(

2023-10-30 18:00:31,420:WARNING:/Users/tanmaysharma/projects/Jak2Biotech/.venv/lib/python3.10/site-packages/pycaret/internal/metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/Users/tanmaysharma/projects/Jak2Biotech/.venv/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/tanmaysharma/projects/Jak2Biotech/.venv/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 182, in _score
    return super()._score(
  File "/Users/tanmaysharma/projects/Jak2Biotech/.venv/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "/Users/tanmaysharma/projects/Jak2Biotech/.venv/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'. Did you mean: '_predict_proba_lr'?

  warnings.warn(

2023-10-30 18:00:31,706:WARNING:/Users/tanmaysharma/projects/Jak2Biotech/.venv/lib/python3.10/site-packages/pycaret/internal/metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/Users/tanmaysharma/projects/Jak2Biotech/.venv/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/tanmaysharma/projects/Jak2Biotech/.venv/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 182, in _score
    return super()._score(
  File "/Users/tanmaysharma/projects/Jak2Biotech/.venv/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "/Users/tanmaysharma/projects/Jak2Biotech/.venv/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'. Did you mean: '_predict_proba_lr'?

  warnings.warn(

2023-10-30 18:00:32,321:WARNING:/Users/tanmaysharma/projects/Jak2Biotech/.venv/lib/python3.10/site-packages/pycaret/internal/metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/Users/tanmaysharma/projects/Jak2Biotech/.venv/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/tanmaysharma/projects/Jak2Biotech/.venv/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 182, in _score
    return super()._score(
  File "/Users/tanmaysharma/projects/Jak2Biotech/.venv/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "/Users/tanmaysharma/projects/Jak2Biotech/.venv/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'. Did you mean: '_predict_proba_lr'?

  warnings.warn(

2023-10-30 18:00:32,701:WARNING:/Users/tanmaysharma/projects/Jak2Biotech/.venv/lib/python3.10/site-packages/pycaret/internal/metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/Users/tanmaysharma/projects/Jak2Biotech/.venv/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/tanmaysharma/projects/Jak2Biotech/.venv/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 182, in _score
    return super()._score(
  File "/Users/tanmaysharma/projects/Jak2Biotech/.venv/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "/Users/tanmaysharma/projects/Jak2Biotech/.venv/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'. Did you mean: '_predict_proba_lr'?

  warnings.warn(

2023-10-30 18:00:54,021:WARNING:/Users/tanmaysharma/projects/Jak2Biotech/.venv/lib/python3.10/site-packages/pycaret/internal/metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/Users/tanmaysharma/projects/Jak2Biotech/.venv/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/tanmaysharma/projects/Jak2Biotech/.venv/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 182, in _score
    return super()._score(
  File "/Users/tanmaysharma/projects/Jak2Biotech/.venv/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "/Users/tanmaysharma/projects/Jak2Biotech/.venv/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'. Did you mean: '_predict_proba_lr'?

  warnings.warn(

2023-10-30 18:00:54,040:INFO:Calculating mean and std
2023-10-30 18:00:54,042:INFO:Creating metrics dataframe
2023-10-30 18:00:54,058:INFO:Uploading results into container
2023-10-30 18:00:54,059:INFO:Uploading model into container now
2023-10-30 18:00:54,059:INFO:_master_model_container: 6
2023-10-30 18:00:54,059:INFO:_display_container: 2
2023-10-30 18:00:54,060:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=1784, solver='auto',
                tol=0.0001)
2023-10-30 18:00:54,060:INFO:create_model() successfully completed......................................
2023-10-30 18:00:54,162:INFO:SubProcess create_model() end ==================================
2023-10-30 18:00:54,163:INFO:Creating metrics dataframe
2023-10-30 18:00:54,167:INFO:Initializing Random Forest Classifier
2023-10-30 18:00:54,167:INFO:Total runtime is 29.596249846617383 minutes
2023-10-30 18:00:54,169:INFO:SubProcess create_model() called ==================================
2023-10-30 18:00:54,169:INFO:Initializing create_model()
2023-10-30 18:00:54,169:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x2a279fcd0>, estimator=rf, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x2b258bb20>, model_only=True, return_train_score=False, kwargs={})
2023-10-30 18:00:54,169:INFO:Checking exceptions
2023-10-30 18:00:54,169:INFO:Importing libraries
2023-10-30 18:00:54,169:INFO:Copying training dataset
2023-10-30 18:00:54,184:INFO:Defining folds
2023-10-30 18:00:54,184:INFO:Declaring metric variables
2023-10-30 18:00:54,185:INFO:Importing untrained model
2023-10-30 18:00:54,187:INFO:Random Forest Classifier Imported successfully
2023-10-30 18:00:54,190:INFO:Starting cross validation
2023-10-30 18:00:54,255:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-10-30 18:05:36,006:INFO:Calculating mean and std
2023-10-30 18:05:36,009:INFO:Creating metrics dataframe
2023-10-30 18:05:36,027:INFO:Uploading results into container
2023-10-30 18:05:36,028:INFO:Uploading model into container now
2023-10-30 18:05:36,029:INFO:_master_model_container: 7
2023-10-30 18:05:36,029:INFO:_display_container: 2
2023-10-30 18:05:36,030:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=100, n_jobs=-1, oob_score=False,
                       random_state=1784, verbose=0, warm_start=False)
2023-10-30 18:05:36,030:INFO:create_model() successfully completed......................................
2023-10-30 18:05:36,163:INFO:SubProcess create_model() end ==================================
2023-10-30 18:05:36,163:INFO:Creating metrics dataframe
2023-10-30 18:05:36,169:INFO:Initializing Quadratic Discriminant Analysis
2023-10-30 18:05:36,169:INFO:Total runtime is 34.296271963914236 minutes
2023-10-30 18:05:36,170:INFO:SubProcess create_model() called ==================================
2023-10-30 18:05:36,170:INFO:Initializing create_model()
2023-10-30 18:05:36,170:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x2a279fcd0>, estimator=qda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x2b258bb20>, model_only=True, return_train_score=False, kwargs={})
2023-10-30 18:05:36,170:INFO:Checking exceptions
2023-10-30 18:05:36,170:INFO:Importing libraries
2023-10-30 18:05:36,170:INFO:Copying training dataset
2023-10-30 18:05:36,185:INFO:Defining folds
2023-10-30 18:05:36,185:INFO:Declaring metric variables
2023-10-30 18:05:36,187:INFO:Importing untrained model
2023-10-30 18:05:36,188:INFO:Quadratic Discriminant Analysis Imported successfully
2023-10-30 18:05:36,191:INFO:Starting cross validation
2023-10-30 18:05:36,282:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-10-30 18:10:08,539:INFO:Calculating mean and std
2023-10-30 18:10:08,543:INFO:Creating metrics dataframe
2023-10-30 18:10:08,563:INFO:Uploading results into container
2023-10-30 18:10:08,564:INFO:Uploading model into container now
2023-10-30 18:10:08,564:INFO:_master_model_container: 8
2023-10-30 18:10:08,564:INFO:_display_container: 2
2023-10-30 18:10:08,565:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2023-10-30 18:10:08,565:INFO:create_model() successfully completed......................................
2023-10-30 18:10:08,677:INFO:SubProcess create_model() end ==================================
2023-10-30 18:10:08,677:INFO:Creating metrics dataframe
2023-10-30 18:10:08,682:INFO:Initializing Ada Boost Classifier
2023-10-30 18:10:08,683:INFO:Total runtime is 38.83816910187404 minutes
2023-10-30 18:10:08,684:INFO:SubProcess create_model() called ==================================
2023-10-30 18:10:08,684:INFO:Initializing create_model()
2023-10-30 18:10:08,684:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x2a279fcd0>, estimator=ada, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x2b258bb20>, model_only=True, return_train_score=False, kwargs={})
2023-10-30 18:10:08,684:INFO:Checking exceptions
2023-10-30 18:10:08,684:INFO:Importing libraries
2023-10-30 18:10:08,684:INFO:Copying training dataset
2023-10-30 18:10:08,702:INFO:Defining folds
2023-10-30 18:10:08,702:INFO:Declaring metric variables
2023-10-30 18:10:08,703:INFO:Importing untrained model
2023-10-30 18:10:08,705:INFO:Ada Boost Classifier Imported successfully
2023-10-30 18:10:08,708:INFO:Starting cross validation
2023-10-30 18:10:08,781:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-10-30 18:15:15,993:INFO:Calculating mean and std
2023-10-30 18:15:16,005:INFO:Creating metrics dataframe
2023-10-30 18:15:16,025:INFO:Uploading results into container
2023-10-30 18:15:16,027:INFO:Uploading model into container now
2023-10-30 18:15:16,028:INFO:_master_model_container: 9
2023-10-30 18:15:16,029:INFO:_display_container: 2
2023-10-30 18:15:16,031:INFO:AdaBoostClassifier(algorithm='SAMME.R', base_estimator='deprecated',
                   estimator=None, learning_rate=1.0, n_estimators=50,
                   random_state=1784)
2023-10-30 18:15:16,031:INFO:create_model() successfully completed......................................
2023-10-30 18:15:16,260:INFO:SubProcess create_model() end ==================================
2023-10-30 18:15:16,261:INFO:Creating metrics dataframe
2023-10-30 18:15:16,267:INFO:Initializing Gradient Boosting Classifier
2023-10-30 18:15:16,267:INFO:Total runtime is 43.96457581520081 minutes
2023-10-30 18:15:16,269:INFO:SubProcess create_model() called ==================================
2023-10-30 18:15:16,269:INFO:Initializing create_model()
2023-10-30 18:15:16,269:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x2a279fcd0>, estimator=gbc, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x2b258bb20>, model_only=True, return_train_score=False, kwargs={})
2023-10-30 18:15:16,269:INFO:Checking exceptions
2023-10-30 18:15:16,269:INFO:Importing libraries
2023-10-30 18:15:16,269:INFO:Copying training dataset
2023-10-30 18:15:16,287:INFO:Defining folds
2023-10-30 18:15:16,287:INFO:Declaring metric variables
2023-10-30 18:15:16,289:INFO:Importing untrained model
2023-10-30 18:15:16,291:INFO:Gradient Boosting Classifier Imported successfully
2023-10-30 18:15:16,294:INFO:Starting cross validation
2023-10-30 18:15:16,465:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-10-30 18:20:15,894:INFO:Calculating mean and std
2023-10-30 18:20:15,897:INFO:Creating metrics dataframe
2023-10-30 18:20:15,907:INFO:Uploading results into container
2023-10-30 18:20:15,908:INFO:Uploading model into container now
2023-10-30 18:20:15,908:INFO:_master_model_container: 10
2023-10-30 18:20:15,908:INFO:_display_container: 2
2023-10-30 18:20:15,910:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=1784, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2023-10-30 18:20:15,910:INFO:create_model() successfully completed......................................
2023-10-30 18:20:16,039:INFO:SubProcess create_model() end ==================================
2023-10-30 18:20:16,039:INFO:Creating metrics dataframe
2023-10-30 18:20:16,045:INFO:Initializing Linear Discriminant Analysis
2023-10-30 18:20:16,045:INFO:Total runtime is 48.9608824133873 minutes
2023-10-30 18:20:16,048:INFO:SubProcess create_model() called ==================================
2023-10-30 18:20:16,048:INFO:Initializing create_model()
2023-10-30 18:20:16,048:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x2a279fcd0>, estimator=lda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x2b258bb20>, model_only=True, return_train_score=False, kwargs={})
2023-10-30 18:20:16,048:INFO:Checking exceptions
2023-10-30 18:20:16,048:INFO:Importing libraries
2023-10-30 18:20:16,048:INFO:Copying training dataset
2023-10-30 18:20:16,068:INFO:Defining folds
2023-10-30 18:20:16,068:INFO:Declaring metric variables
2023-10-30 18:20:16,070:INFO:Importing untrained model
2023-10-30 18:20:16,072:INFO:Linear Discriminant Analysis Imported successfully
2023-10-30 18:20:16,075:INFO:Starting cross validation
2023-10-30 18:20:16,183:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-10-30 18:24:16,321:INFO:Calculating mean and std
2023-10-30 18:24:16,326:INFO:Creating metrics dataframe
2023-10-30 18:24:16,353:INFO:Uploading results into container
2023-10-30 18:24:16,353:INFO:Uploading model into container now
2023-10-30 18:24:16,354:INFO:_master_model_container: 11
2023-10-30 18:24:16,354:INFO:_display_container: 2
2023-10-30 18:24:16,354:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2023-10-30 18:24:16,354:INFO:create_model() successfully completed......................................
2023-10-30 18:24:16,456:INFO:SubProcess create_model() end ==================================
2023-10-30 18:24:16,456:INFO:Creating metrics dataframe
2023-10-30 18:24:16,463:INFO:Initializing Extra Trees Classifier
2023-10-30 18:24:16,463:INFO:Total runtime is 52.967840063571934 minutes
2023-10-30 18:24:16,464:INFO:SubProcess create_model() called ==================================
2023-10-30 18:24:16,464:INFO:Initializing create_model()
2023-10-30 18:24:16,464:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x2a279fcd0>, estimator=et, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x2b258bb20>, model_only=True, return_train_score=False, kwargs={})
2023-10-30 18:24:16,464:INFO:Checking exceptions
2023-10-30 18:24:16,464:INFO:Importing libraries
2023-10-30 18:24:16,465:INFO:Copying training dataset
2023-10-30 18:24:16,479:INFO:Defining folds
2023-10-30 18:24:16,479:INFO:Declaring metric variables
2023-10-30 18:24:16,481:INFO:Importing untrained model
2023-10-30 18:24:16,482:INFO:Extra Trees Classifier Imported successfully
2023-10-30 18:24:16,485:INFO:Starting cross validation
2023-10-30 18:24:16,546:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-10-30 18:28:33,961:INFO:Calculating mean and std
2023-10-30 18:28:33,964:INFO:Creating metrics dataframe
2023-10-30 18:28:33,981:INFO:Uploading results into container
2023-10-30 18:28:33,982:INFO:Uploading model into container now
2023-10-30 18:28:33,982:INFO:_master_model_container: 12
2023-10-30 18:28:33,982:INFO:_display_container: 2
2023-10-30 18:28:33,983:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     n_estimators=100, n_jobs=-1, oob_score=False,
                     random_state=1784, verbose=0, warm_start=False)
2023-10-30 18:28:33,983:INFO:create_model() successfully completed......................................
2023-10-30 18:28:34,111:INFO:SubProcess create_model() end ==================================
2023-10-30 18:28:34,111:INFO:Creating metrics dataframe
2023-10-30 18:28:34,117:INFO:Initializing Light Gradient Boosting Machine
2023-10-30 18:28:34,118:INFO:Total runtime is 57.262085199356086 minutes
2023-10-30 18:28:34,119:INFO:SubProcess create_model() called ==================================
2023-10-30 18:28:34,119:INFO:Initializing create_model()
2023-10-30 18:28:34,119:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x2a279fcd0>, estimator=lightgbm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x2b258bb20>, model_only=True, return_train_score=False, kwargs={})
2023-10-30 18:28:34,119:INFO:Checking exceptions
2023-10-30 18:28:34,119:INFO:Importing libraries
2023-10-30 18:28:34,119:INFO:Copying training dataset
2023-10-30 18:28:34,135:INFO:Defining folds
2023-10-30 18:28:34,136:INFO:Declaring metric variables
2023-10-30 18:28:34,137:INFO:Importing untrained model
2023-10-30 18:28:34,139:INFO:Light Gradient Boosting Machine Imported successfully
2023-10-30 18:28:34,142:INFO:Starting cross validation
2023-10-30 18:28:34,213:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-10-30 18:33:14,158:INFO:Calculating mean and std
2023-10-30 18:33:14,162:INFO:Creating metrics dataframe
2023-10-30 18:33:14,193:INFO:Uploading results into container
2023-10-30 18:33:14,194:INFO:Uploading model into container now
2023-10-30 18:33:14,194:INFO:_master_model_container: 13
2023-10-30 18:33:14,194:INFO:_display_container: 2
2023-10-30 18:33:14,195:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=1784, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2023-10-30 18:33:14,195:INFO:create_model() successfully completed......................................
2023-10-30 18:33:14,305:INFO:SubProcess create_model() end ==================================
2023-10-30 18:33:14,305:INFO:Creating metrics dataframe
2023-10-30 18:33:14,312:INFO:Initializing CatBoost Classifier
2023-10-30 18:33:14,313:INFO:Total runtime is 61.93200254837673 minutes
2023-10-30 18:33:14,314:INFO:SubProcess create_model() called ==================================
2023-10-30 18:33:14,314:INFO:Initializing create_model()
2023-10-30 18:33:14,314:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x2a279fcd0>, estimator=catboost, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x2b258bb20>, model_only=True, return_train_score=False, kwargs={})
2023-10-30 18:33:14,314:INFO:Checking exceptions
2023-10-30 18:33:14,314:INFO:Importing libraries
2023-10-30 18:33:14,314:INFO:Copying training dataset
2023-10-30 18:33:14,330:INFO:Defining folds
2023-10-30 18:33:14,330:INFO:Declaring metric variables
2023-10-30 18:33:14,331:INFO:Importing untrained model
2023-10-30 18:33:14,337:INFO:CatBoost Classifier Imported successfully
2023-10-30 18:33:14,340:INFO:Starting cross validation
2023-10-30 18:33:14,410:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-10-30 18:37:50,647:INFO:Calculating mean and std
2023-10-30 18:37:50,650:INFO:Creating metrics dataframe
2023-10-30 18:37:50,671:INFO:Uploading results into container
2023-10-30 18:37:50,671:INFO:Uploading model into container now
2023-10-30 18:37:50,672:INFO:_master_model_container: 14
2023-10-30 18:37:50,672:INFO:_display_container: 2
2023-10-30 18:37:50,672:INFO:<catboost.core.CatBoostClassifier object at 0x2b3b547c0>
2023-10-30 18:37:50,672:INFO:create_model() successfully completed......................................
2023-10-30 18:37:50,775:INFO:SubProcess create_model() end ==================================
2023-10-30 18:37:50,775:INFO:Creating metrics dataframe
2023-10-30 18:37:50,781:INFO:Initializing Dummy Classifier
2023-10-30 18:37:50,781:INFO:Total runtime is 66.53981839815776 minutes
2023-10-30 18:37:50,783:INFO:SubProcess create_model() called ==================================
2023-10-30 18:37:50,783:INFO:Initializing create_model()
2023-10-30 18:37:50,783:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x2a279fcd0>, estimator=dummy, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x2b258bb20>, model_only=True, return_train_score=False, kwargs={})
2023-10-30 18:37:50,783:INFO:Checking exceptions
2023-10-30 18:37:50,783:INFO:Importing libraries
2023-10-30 18:37:50,783:INFO:Copying training dataset
2023-10-30 18:37:50,799:INFO:Defining folds
2023-10-30 18:37:50,799:INFO:Declaring metric variables
2023-10-30 18:37:50,801:INFO:Importing untrained model
2023-10-30 18:37:50,803:INFO:Dummy Classifier Imported successfully
2023-10-30 18:37:50,806:INFO:Starting cross validation
2023-10-30 18:37:50,873:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-10-30 18:41:47,451:WARNING:/Users/tanmaysharma/projects/Jak2Biotech/.venv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-10-30 18:41:47,788:WARNING:/Users/tanmaysharma/projects/Jak2Biotech/.venv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-10-30 18:41:47,826:WARNING:/Users/tanmaysharma/projects/Jak2Biotech/.venv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-10-30 18:41:48,116:WARNING:/Users/tanmaysharma/projects/Jak2Biotech/.venv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-10-30 18:41:48,402:WARNING:/Users/tanmaysharma/projects/Jak2Biotech/.venv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-10-30 18:41:49,169:WARNING:/Users/tanmaysharma/projects/Jak2Biotech/.venv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-10-30 18:41:49,223:WARNING:/Users/tanmaysharma/projects/Jak2Biotech/.venv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-10-30 18:41:49,223:WARNING:/Users/tanmaysharma/projects/Jak2Biotech/.venv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-10-30 18:41:49,268:WARNING:/Users/tanmaysharma/projects/Jak2Biotech/.venv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-10-30 18:42:10,909:WARNING:/Users/tanmaysharma/projects/Jak2Biotech/.venv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-10-30 18:42:11,024:INFO:Calculating mean and std
2023-10-30 18:42:11,026:INFO:Creating metrics dataframe
2023-10-30 18:42:11,046:INFO:Uploading results into container
2023-10-30 18:42:11,047:INFO:Uploading model into container now
2023-10-30 18:42:11,047:INFO:_master_model_container: 15
2023-10-30 18:42:11,047:INFO:_display_container: 2
2023-10-30 18:42:11,048:INFO:DummyClassifier(constant=None, random_state=1784, strategy='prior')
2023-10-30 18:42:11,048:INFO:create_model() successfully completed......................................
2023-10-30 18:42:11,151:INFO:SubProcess create_model() end ==================================
2023-10-30 18:42:11,151:INFO:Creating metrics dataframe
2023-10-30 18:42:11,161:INFO:Initializing create_model()
2023-10-30 18:42:11,161:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x2a279fcd0>, estimator=ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     n_estimators=100, n_jobs=-1, oob_score=False,
                     random_state=1784, verbose=0, warm_start=False), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-10-30 18:42:11,161:INFO:Checking exceptions
2023-10-30 18:42:11,163:INFO:Importing libraries
2023-10-30 18:42:11,163:INFO:Copying training dataset
2023-10-30 18:42:11,179:INFO:Defining folds
2023-10-30 18:42:11,179:INFO:Declaring metric variables
2023-10-30 18:42:11,179:INFO:Importing untrained model
2023-10-30 18:42:11,179:INFO:Declaring custom model
2023-10-30 18:42:11,180:INFO:Extra Trees Classifier Imported successfully
2023-10-30 18:42:11,246:INFO:Cross validation set to False
2023-10-30 18:42:11,246:INFO:Fitting Model
2023-10-30 18:51:02,706:INFO:[LightGBM] [Info] Number of positive: 3312, number of negative: 3414
2023-10-30 18:51:02,969:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.260545 seconds.
2023-10-30 18:51:02,969:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-30 18:51:02,977:INFO:[LightGBM] [Info] Total Bins 1409640
2023-10-30 18:51:03,029:INFO:[LightGBM] [Info] Number of data points in the train set: 6726, number of used features: 5528
2023-10-30 18:51:03,035:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.492417 -> initscore=-0.030332
2023-10-30 18:51:03,035:INFO:[LightGBM] [Info] Start training from score -0.030332
2023-10-30 18:51:37,247:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     n_estimators=100, n_jobs=-1, oob_score=False,
                     random_state=1784, verbose=0, warm_start=False)
2023-10-30 18:51:37,247:INFO:create_model() successfully completed......................................
2023-10-30 18:51:37,322:INFO:Initializing create_model()
2023-10-30 18:51:37,323:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x2a279fcd0>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=100, n_jobs=-1, oob_score=False,
                       random_state=1784, verbose=0, warm_start=False), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-10-30 18:51:37,323:INFO:Checking exceptions
2023-10-30 18:51:37,324:INFO:Importing libraries
2023-10-30 18:51:37,324:INFO:Copying training dataset
2023-10-30 18:51:37,333:INFO:Defining folds
2023-10-30 18:51:37,333:INFO:Declaring metric variables
2023-10-30 18:51:37,333:INFO:Importing untrained model
2023-10-30 18:51:37,333:INFO:Declaring custom model
2023-10-30 18:51:37,333:INFO:Random Forest Classifier Imported successfully
2023-10-30 18:51:37,363:INFO:Cross validation set to False
2023-10-30 18:51:37,363:INFO:Fitting Model
2023-10-30 19:00:26,005:INFO:[LightGBM] [Info] Number of positive: 3312, number of negative: 3414
2023-10-30 19:00:26,272:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.264802 seconds.
2023-10-30 19:00:26,272:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-30 19:00:26,279:INFO:[LightGBM] [Info] Total Bins 1409640
2023-10-30 19:00:26,332:INFO:[LightGBM] [Info] Number of data points in the train set: 6726, number of used features: 5528
2023-10-30 19:00:26,338:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.492417 -> initscore=-0.030332
2023-10-30 19:00:26,338:INFO:[LightGBM] [Info] Start training from score -0.030332
2023-10-30 19:01:00,833:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=100, n_jobs=-1, oob_score=False,
                       random_state=1784, verbose=0, warm_start=False)
2023-10-30 19:01:00,833:INFO:create_model() successfully completed......................................
2023-10-30 19:01:00,908:INFO:Initializing create_model()
2023-10-30 19:01:00,908:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x2a279fcd0>, estimator=QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-10-30 19:01:00,908:INFO:Checking exceptions
2023-10-30 19:01:00,909:INFO:Importing libraries
2023-10-30 19:01:00,909:INFO:Copying training dataset
2023-10-30 19:01:00,919:INFO:Defining folds
2023-10-30 19:01:00,919:INFO:Declaring metric variables
2023-10-30 19:01:00,919:INFO:Importing untrained model
2023-10-30 19:01:00,919:INFO:Declaring custom model
2023-10-30 19:01:00,920:INFO:Quadratic Discriminant Analysis Imported successfully
2023-10-30 19:01:00,949:INFO:Cross validation set to False
2023-10-30 19:01:00,949:INFO:Fitting Model
2023-10-30 19:09:52,981:INFO:[LightGBM] [Info] Number of positive: 3312, number of negative: 3414
2023-10-30 19:09:53,226:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.242936 seconds.
2023-10-30 19:09:53,226:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-30 19:09:53,233:INFO:[LightGBM] [Info] Total Bins 1409640
2023-10-30 19:09:53,284:INFO:[LightGBM] [Info] Number of data points in the train set: 6726, number of used features: 5528
2023-10-30 19:09:53,290:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.492417 -> initscore=-0.030332
2023-10-30 19:09:53,290:INFO:[LightGBM] [Info] Start training from score -0.030332
2023-10-30 19:10:27,254:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2023-10-30 19:10:27,254:INFO:create_model() successfully completed......................................
2023-10-30 19:10:27,344:INFO:Initializing create_model()
2023-10-30 19:10:27,344:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x2a279fcd0>, estimator=<catboost.core.CatBoostClassifier object at 0x2b3b547c0>, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-10-30 19:10:27,344:INFO:Checking exceptions
2023-10-30 19:10:27,345:INFO:Importing libraries
2023-10-30 19:10:27,345:INFO:Copying training dataset
2023-10-30 19:10:27,356:INFO:Defining folds
2023-10-30 19:10:27,356:INFO:Declaring metric variables
2023-10-30 19:10:27,356:INFO:Importing untrained model
2023-10-30 19:10:27,356:INFO:Declaring custom model
2023-10-30 19:10:27,356:INFO:CatBoost Classifier Imported successfully
2023-10-30 19:10:27,388:INFO:Cross validation set to False
2023-10-30 19:10:27,388:INFO:Fitting Model
2023-10-30 19:19:24,372:INFO:[LightGBM] [Info] Number of positive: 3312, number of negative: 3414
2023-10-30 19:19:24,631:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.257231 seconds.
2023-10-30 19:19:24,631:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-30 19:19:24,638:INFO:[LightGBM] [Info] Total Bins 1409640
2023-10-30 19:19:24,691:INFO:[LightGBM] [Info] Number of data points in the train set: 6726, number of used features: 5528
2023-10-30 19:19:24,697:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.492417 -> initscore=-0.030332
2023-10-30 19:19:24,697:INFO:[LightGBM] [Info] Start training from score -0.030332
2023-10-30 19:20:01,150:INFO:<catboost.core.CatBoostClassifier object at 0x2b25da2f0>
2023-10-30 19:20:01,151:INFO:create_model() successfully completed......................................
2023-10-30 19:20:01,230:INFO:Initializing create_model()
2023-10-30 19:20:01,231:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x2a279fcd0>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=1784, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-10-30 19:20:01,231:INFO:Checking exceptions
2023-10-30 19:20:01,232:INFO:Importing libraries
2023-10-30 19:20:01,232:INFO:Copying training dataset
2023-10-30 19:20:01,241:INFO:Defining folds
2023-10-30 19:20:01,241:INFO:Declaring metric variables
2023-10-30 19:20:01,241:INFO:Importing untrained model
2023-10-30 19:20:01,241:INFO:Declaring custom model
2023-10-30 19:20:01,242:INFO:Light Gradient Boosting Machine Imported successfully
2023-10-30 19:20:01,273:INFO:Cross validation set to False
2023-10-30 19:20:01,273:INFO:Fitting Model
2023-10-30 19:28:59,518:INFO:[LightGBM] [Info] Number of positive: 3312, number of negative: 3414
2023-10-30 19:28:59,780:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.259886 seconds.
2023-10-30 19:28:59,780:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-30 19:28:59,787:INFO:[LightGBM] [Info] Total Bins 1409640
2023-10-30 19:28:59,838:INFO:[LightGBM] [Info] Number of data points in the train set: 6726, number of used features: 5528
2023-10-30 19:28:59,844:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.492417 -> initscore=-0.030332
2023-10-30 19:28:59,844:INFO:[LightGBM] [Info] Start training from score -0.030332
2023-10-30 19:29:33,565:INFO:[LightGBM] [Info] Number of positive: 3312, number of negative: 3414
2023-10-30 19:29:33,567:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001699 seconds.
2023-10-30 19:29:33,567:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-30 19:29:33,567:INFO:[LightGBM] [Info] Total Bins 15045
2023-10-30 19:29:33,568:INFO:[LightGBM] [Info] Number of data points in the train set: 6726, number of used features: 59
2023-10-30 19:29:33,568:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.492417 -> initscore=-0.030332
2023-10-30 19:29:33,568:INFO:[LightGBM] [Info] Start training from score -0.030332
2023-10-30 19:29:33,944:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=1784, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2023-10-30 19:29:33,944:INFO:create_model() successfully completed......................................
2023-10-30 19:29:34,031:INFO:_master_model_container: 15
2023-10-30 19:29:34,031:INFO:_display_container: 2
2023-10-30 19:29:34,032:INFO:[ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     n_estimators=100, n_jobs=-1, oob_score=False,
                     random_state=1784, verbose=0, warm_start=False), RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=100, n_jobs=-1, oob_score=False,
                       random_state=1784, verbose=0, warm_start=False), QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001), <catboost.core.CatBoostClassifier object at 0x2b25da2f0>, LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=1784, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)]
2023-10-30 19:29:34,032:INFO:compare_models() successfully completed......................................
2023-10-30 19:31:10,645:INFO:Initializing interpret_model()
2023-10-30 19:31:10,645:INFO:interpret_model(estimator=ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     n_estimators=100, n_jobs=-1, oob_score=False,
                     random_state=1784, verbose=0, warm_start=False), use_train_data=False, X_new_sample=None, y_new_sample=None, feature=None, kwargs={}, observation=None, plot=summary, save=False, self=<pycaret.classification.oop.ClassificationExperiment object at 0x2a279fcd0>)
2023-10-30 19:31:10,645:INFO:Checking exceptions
2023-10-30 19:31:10,645:INFO:Soft dependency imported: shap: 0.43.0
2023-10-30 19:31:19,543:INFO:plot type: summary
2023-10-30 19:31:19,543:INFO:Creating TreeExplainer
2023-10-30 19:31:19,555:INFO:Compiling shap values
2023-10-30 19:36:37,620:INFO:Visual Rendered Successfully
2023-10-30 19:36:37,620:INFO:interpret_model() successfully completed......................................
2023-10-30 19:36:37,718:INFO:Initializing interpret_model()
2023-10-30 19:36:37,719:INFO:interpret_model(estimator=ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     n_estimators=100, n_jobs=-1, oob_score=False,
                     random_state=1784, verbose=0, warm_start=False), use_train_data=False, X_new_sample=None, y_new_sample=None, feature=None, kwargs={}, observation=None, plot=summary, save=False, self=<pycaret.classification.oop.ClassificationExperiment object at 0x2a279fcd0>)
2023-10-30 19:36:37,719:INFO:Checking exceptions
2023-10-30 19:36:37,719:INFO:Soft dependency imported: shap: 0.43.0
2023-10-30 19:36:37,901:INFO:plot type: summary
2023-10-30 19:36:37,901:INFO:Creating TreeExplainer
2023-10-30 19:36:37,913:INFO:Compiling shap values
2023-10-30 19:41:55,145:INFO:Visual Rendered Successfully
2023-10-30 19:41:55,146:INFO:interpret_model() successfully completed......................................
2023-10-30 19:41:55,233:INFO:Initializing interpret_model()
2023-10-30 19:41:55,233:INFO:interpret_model(estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=100, n_jobs=-1, oob_score=False,
                       random_state=1784, verbose=0, warm_start=False), use_train_data=False, X_new_sample=None, y_new_sample=None, feature=None, kwargs={}, observation=None, plot=summary, save=False, self=<pycaret.classification.oop.ClassificationExperiment object at 0x2a279fcd0>)
2023-10-30 19:41:55,233:INFO:Checking exceptions
2023-10-30 19:41:55,233:INFO:Soft dependency imported: shap: 0.43.0
2023-10-30 19:41:55,415:INFO:plot type: summary
2023-10-30 19:41:55,415:INFO:Creating TreeExplainer
2023-10-30 19:41:55,420:INFO:Compiling shap values
2023-10-30 19:43:03,486:INFO:Visual Rendered Successfully
2023-10-30 19:43:03,486:INFO:interpret_model() successfully completed......................................
2023-10-30 19:43:03,574:INFO:Initializing interpret_model()
2023-10-30 19:43:03,574:INFO:interpret_model(estimator=QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001), use_train_data=False, X_new_sample=None, y_new_sample=None, feature=None, kwargs={}, observation=None, plot=summary, save=False, self=<pycaret.classification.oop.ClassificationExperiment object at 0x2a279fcd0>)
2023-10-30 19:43:03,574:INFO:Checking exceptions
2023-10-30 19:43:03,574:INFO:Soft dependency imported: shap: 0.43.0
2023-10-30 19:44:37,992:INFO:Initializing create_model()
2023-10-30 19:44:37,993:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x2a279fcd0>, estimator=et, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-10-30 19:44:37,993:INFO:Checking exceptions
2023-10-30 19:44:38,002:INFO:Importing libraries
2023-10-30 19:44:38,002:INFO:Copying training dataset
2023-10-30 19:44:38,019:INFO:Defining folds
2023-10-30 19:44:38,019:INFO:Declaring metric variables
2023-10-30 19:44:38,021:INFO:Importing untrained model
2023-10-30 19:44:38,023:INFO:Extra Trees Classifier Imported successfully
2023-10-30 19:44:38,026:INFO:Starting cross validation
2023-10-30 19:44:38,058:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-10-30 19:46:08,146:INFO:[LightGBM] [Info] Number of positive: 2952, number of negative: 3101
2023-10-30 19:46:08,630:INFO:[LightGBM] [Info] Number of positive: 2954, number of negative: 3099
2023-10-30 19:46:08,665:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.514223 seconds.
2023-10-30 19:46:08,666:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-30 19:46:08,674:INFO:[LightGBM] [Info] Total Bins 1286730
2023-10-30 19:46:08,884:INFO:[LightGBM] [Info] Number of data points in the train set: 6053, number of used features: 5046
2023-10-30 19:46:08,897:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.487692 -> initscore=-0.049242
2023-10-30 19:46:08,898:INFO:[LightGBM] [Info] Start training from score -0.049242
2023-10-30 19:46:09,101:INFO:[LightGBM] [Info] Number of positive: 2942, number of negative: 3110
2023-10-30 19:46:09,221:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.585398 seconds.
2023-10-30 19:46:09,221:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-30 19:46:09,229:INFO:[LightGBM] [Info] Total Bins 1290810
2023-10-30 19:46:09,297:INFO:[LightGBM] [Info] Number of positive: 2956, number of negative: 3097
2023-10-30 19:46:09,428:INFO:[LightGBM] [Info] Number of data points in the train set: 6053, number of used features: 5062
2023-10-30 19:46:09,441:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.488022 -> initscore=-0.047919
2023-10-30 19:46:09,442:INFO:[LightGBM] [Info] Start training from score -0.047919
2023-10-30 19:46:09,828:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.726831 seconds.
2023-10-30 19:46:09,829:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-30 19:46:09,837:INFO:[LightGBM] [Info] Total Bins 1294635
2023-10-30 19:46:10,014:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.713300 seconds.
2023-10-30 19:46:10,014:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-30 19:46:10,025:INFO:[LightGBM] [Info] Total Bins 1289790
2023-10-30 19:46:10,070:INFO:[LightGBM] [Info] Number of data points in the train set: 6052, number of used features: 5077
2023-10-30 19:46:10,079:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.486120 -> initscore=-0.055533
2023-10-30 19:46:10,079:INFO:[LightGBM] [Info] Start training from score -0.055533
2023-10-30 19:46:10,195:INFO:[LightGBM] [Info] Number of data points in the train set: 6053, number of used features: 5058
2023-10-30 19:46:10,203:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.488353 -> initscore=-0.046597
2023-10-30 19:46:10,204:INFO:[LightGBM] [Info] Start training from score -0.046597
2023-10-30 19:46:11,777:INFO:[LightGBM] [Info] Number of positive: 2944, number of negative: 3109
2023-10-30 19:46:12,246:INFO:[LightGBM] [Info] Number of positive: 2929, number of negative: 3124
2023-10-30 19:46:12,356:INFO:[LightGBM] [Info] Number of positive: 2975, number of negative: 3077
2023-10-30 19:46:12,478:INFO:[LightGBM] [Info] Number of positive: 2921, number of negative: 3132
2023-10-30 19:46:12,603:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.816840 seconds.
2023-10-30 19:46:12,604:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-30 19:46:12,611:INFO:[LightGBM] [Info] Total Bins 1292085
2023-10-30 19:46:12,832:INFO:[LightGBM] [Info] Number of data points in the train set: 6053, number of used features: 5067
2023-10-30 19:46:12,845:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.486370 -> initscore=-0.054532
2023-10-30 19:46:12,846:INFO:[LightGBM] [Info] Start training from score -0.054532
2023-10-30 19:46:12,886:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.636525 seconds.
2023-10-30 19:46:12,886:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-30 19:46:12,896:INFO:[LightGBM] [Info] Total Bins 1291830
2023-10-30 19:46:13,064:INFO:[LightGBM] [Info] Number of data points in the train set: 6053, number of used features: 5066
2023-10-30 19:46:13,078:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.483892 -> initscore=-0.064453
2023-10-30 19:46:13,078:INFO:[LightGBM] [Info] Start training from score -0.064453
2023-10-30 19:46:13,298:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.932544 seconds.
2023-10-30 19:46:13,298:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-30 19:46:13,305:INFO:[LightGBM] [Info] Total Bins 1289280
2023-10-30 19:46:13,372:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.888683 seconds.
2023-10-30 19:46:13,373:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-30 19:46:13,389:INFO:[LightGBM] [Info] Total Bins 1294890
2023-10-30 19:46:13,617:INFO:[LightGBM] [Info] Number of data points in the train set: 6052, number of used features: 5056
2023-10-30 19:46:13,643:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.491573 -> initscore=-0.033711
2023-10-30 19:46:13,644:INFO:[LightGBM] [Info] Start training from score -0.033711
2023-10-30 19:46:13,678:INFO:[LightGBM] [Info] Number of data points in the train set: 6053, number of used features: 5078
2023-10-30 19:46:13,691:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.482571 -> initscore=-0.069746
2023-10-30 19:46:13,691:INFO:[LightGBM] [Info] Start training from score -0.069746
2023-10-30 19:46:15,361:INFO:[LightGBM] [Info] Number of positive: 2946, number of negative: 3107
2023-10-30 19:46:16,272:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.882877 seconds.
2023-10-30 19:46:16,276:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-30 19:46:16,282:INFO:[LightGBM] [Info] Total Bins 1294890
2023-10-30 19:46:16,517:INFO:[LightGBM] [Info] Number of data points in the train set: 6053, number of used features: 5078
2023-10-30 19:46:16,534:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.486701 -> initscore=-0.053209
2023-10-30 19:46:16,534:INFO:[LightGBM] [Info] Start training from score -0.053209
2023-10-30 19:48:11,717:INFO:[LightGBM] [Info] Number of positive: 2940, number of negative: 3113
2023-10-30 19:48:12,195:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.460986 seconds.
2023-10-30 19:48:12,199:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-30 19:48:12,208:INFO:[LightGBM] [Info] Total Bins 1291830
2023-10-30 19:48:12,441:INFO:[LightGBM] [Info] Number of data points in the train set: 6053, number of used features: 5066
2023-10-30 19:48:12,457:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.485710 -> initscore=-0.057177
2023-10-30 19:48:12,457:INFO:[LightGBM] [Info] Start training from score -0.057177
2023-10-30 19:49:09,681:INFO:Calculating mean and std
2023-10-30 19:49:09,696:INFO:Creating metrics dataframe
2023-10-30 19:49:09,738:INFO:Finalizing model
2023-10-30 19:58:09,953:INFO:[LightGBM] [Info] Number of positive: 3312, number of negative: 3414
2023-10-30 19:58:10,241:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.286354 seconds.
2023-10-30 19:58:10,242:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-30 19:58:10,249:INFO:[LightGBM] [Info] Total Bins 1409640
2023-10-30 19:58:10,300:INFO:[LightGBM] [Info] Number of data points in the train set: 6726, number of used features: 5528
2023-10-30 19:58:10,306:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.492417 -> initscore=-0.030332
2023-10-30 19:58:10,306:INFO:[LightGBM] [Info] Start training from score -0.030332
2023-10-30 19:58:45,327:INFO:Uploading results into container
2023-10-30 19:58:45,328:INFO:Uploading model into container now
2023-10-30 19:58:45,339:INFO:_master_model_container: 16
2023-10-30 19:58:45,339:INFO:_display_container: 3
2023-10-30 19:58:45,340:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     n_estimators=100, n_jobs=-1, oob_score=False,
                     random_state=1784, verbose=0, warm_start=False)
2023-10-30 19:58:45,340:INFO:create_model() successfully completed......................................
2023-10-30 19:58:45,574:INFO:Initializing create_model()
2023-10-30 19:58:45,574:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x2a279fcd0>, estimator=rfc, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-10-30 19:58:45,574:INFO:Checking exceptions
2023-10-30 21:07:57,042:INFO:Initializing create_model()
2023-10-30 21:07:57,043:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x2a279fcd0>, estimator=et, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-10-30 21:07:57,043:INFO:Checking exceptions
2023-10-30 21:07:57,051:INFO:Importing libraries
2023-10-30 21:07:57,052:INFO:Copying training dataset
2023-10-30 21:07:57,079:INFO:Defining folds
2023-10-30 21:07:57,080:INFO:Declaring metric variables
2023-10-30 21:07:57,082:INFO:Importing untrained model
2023-10-30 21:07:57,085:INFO:Extra Trees Classifier Imported successfully
2023-10-30 21:07:57,088:INFO:Starting cross validation
2023-10-30 21:07:57,455:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-10-30 21:09:18,338:INFO:[LightGBM] [Info] Number of positive: 2956, number of negative: 3097
2023-10-30 21:09:18,339:INFO:[LightGBM] [Info] Number of positive: 2942, number of negative: 3110
2023-10-30 21:09:18,764:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.423358 seconds.
2023-10-30 21:09:18,765:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-30 21:09:18,771:INFO:[LightGBM] [Info] Total Bins 1294635
2023-10-30 21:09:18,781:INFO:[LightGBM] [Info] Number of positive: 2954, number of negative: 3099
2023-10-30 21:09:18,933:INFO:[LightGBM] [Info] Number of data points in the train set: 6052, number of used features: 5077
2023-10-30 21:09:18,942:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.486120 -> initscore=-0.055533
2023-10-30 21:09:18,942:INFO:[LightGBM] [Info] Start training from score -0.055533
2023-10-30 21:09:18,990:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.649693 seconds.
2023-10-30 21:09:18,990:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-30 21:09:18,997:INFO:[LightGBM] [Info] Total Bins 1289790
2023-10-30 21:09:19,176:INFO:[LightGBM] [Info] Number of data points in the train set: 6053, number of used features: 5058
2023-10-30 21:09:19,184:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.488353 -> initscore=-0.046597
2023-10-30 21:09:19,184:INFO:[LightGBM] [Info] Start training from score -0.046597
2023-10-30 21:09:19,473:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.688726 seconds.
2023-10-30 21:09:19,473:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-30 21:09:19,482:INFO:[LightGBM] [Info] Total Bins 1290810
2023-10-30 21:09:19,710:INFO:[LightGBM] [Info] Number of data points in the train set: 6053, number of used features: 5062
2023-10-30 21:09:19,717:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.488022 -> initscore=-0.047919
2023-10-30 21:09:19,717:INFO:[LightGBM] [Info] Start training from score -0.047919
2023-10-30 21:09:21,538:INFO:[LightGBM] [Info] Number of positive: 2946, number of negative: 3107
2023-10-30 21:09:21,798:INFO:[LightGBM] [Info] Number of positive: 2921, number of negative: 3132
2023-10-30 21:09:22,274:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.725858 seconds.
2023-10-30 21:09:22,274:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-30 21:09:22,281:INFO:[LightGBM] [Info] Total Bins 1294890
2023-10-30 21:09:22,347:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.544998 seconds.
2023-10-30 21:09:22,348:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-30 21:09:22,357:INFO:[LightGBM] [Info] Total Bins 1294890
2023-10-30 21:09:22,522:INFO:[LightGBM] [Info] Number of data points in the train set: 6053, number of used features: 5078
2023-10-30 21:09:22,535:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.486701 -> initscore=-0.053209
2023-10-30 21:09:22,539:INFO:[LightGBM] [Info] Start training from score -0.053209
2023-10-30 21:09:22,595:INFO:[LightGBM] [Info] Number of data points in the train set: 6053, number of used features: 5078
2023-10-30 21:09:22,601:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.482571 -> initscore=-0.069746
2023-10-30 21:09:22,601:INFO:[LightGBM] [Info] Start training from score -0.069746
2023-10-30 21:09:24,237:INFO:[LightGBM] [Info] Number of positive: 2929, number of negative: 3124
2023-10-30 21:09:24,434:INFO:[LightGBM] [Info] Number of positive: 2975, number of negative: 3077
2023-10-30 21:09:24,806:INFO:[LightGBM] [Info] Number of positive: 2944, number of negative: 3109
2023-10-30 21:09:24,957:INFO:[LightGBM] [Info] Number of positive: 2952, number of negative: 3101
2023-10-30 21:09:25,035:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.795207 seconds.
2023-10-30 21:09:25,035:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-30 21:09:25,044:INFO:[LightGBM] [Info] Total Bins 1291830
2023-10-30 21:09:25,302:INFO:[LightGBM] [Info] Number of data points in the train set: 6053, number of used features: 5066
2023-10-30 21:09:25,311:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.483892 -> initscore=-0.064453
2023-10-30 21:09:25,311:INFO:[LightGBM] [Info] Start training from score -0.064453
2023-10-30 21:09:25,360:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.921835 seconds.
2023-10-30 21:09:25,360:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-30 21:09:25,368:INFO:[LightGBM] [Info] Total Bins 1289280
2023-10-30 21:09:25,593:INFO:[LightGBM] [Info] Number of data points in the train set: 6052, number of used features: 5056
2023-10-30 21:09:25,607:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.491573 -> initscore=-0.033711
2023-10-30 21:09:25,607:INFO:[LightGBM] [Info] Start training from score -0.033711
2023-10-30 21:09:25,637:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.827681 seconds.
2023-10-30 21:09:25,637:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-30 21:09:25,644:INFO:[LightGBM] [Info] Total Bins 1292085
2023-10-30 21:09:25,854:INFO:[LightGBM] [Info] Number of data points in the train set: 6053, number of used features: 5067
2023-10-30 21:09:25,855:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.886743 seconds.
2023-10-30 21:09:25,855:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-30 21:09:25,862:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.486370 -> initscore=-0.054532
2023-10-30 21:09:25,862:INFO:[LightGBM] [Info] Start training from score -0.054532
2023-10-30 21:09:25,875:INFO:[LightGBM] [Info] Total Bins 1286730
2023-10-30 21:09:26,046:INFO:[LightGBM] [Info] Number of data points in the train set: 6053, number of used features: 5046
2023-10-30 21:09:26,067:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.487692 -> initscore=-0.049242
2023-10-30 21:09:26,067:INFO:[LightGBM] [Info] Start training from score -0.049242
2023-10-30 21:11:16,252:INFO:[LightGBM] [Info] Number of positive: 2940, number of negative: 3113
2023-10-30 21:11:16,779:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.525974 seconds.
2023-10-30 21:11:16,786:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-30 21:11:16,792:INFO:[LightGBM] [Info] Total Bins 1291830
2023-10-30 21:11:17,005:INFO:[LightGBM] [Info] Number of data points in the train set: 6053, number of used features: 5066
2023-10-30 21:11:17,022:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.485710 -> initscore=-0.057177
2023-10-30 21:11:17,023:INFO:[LightGBM] [Info] Start training from score -0.057177
2023-10-30 21:12:06,590:INFO:Calculating mean and std
2023-10-30 21:12:06,600:INFO:Creating metrics dataframe
2023-10-30 21:12:06,618:INFO:Finalizing model
2023-10-30 21:21:01,399:INFO:[LightGBM] [Info] Number of positive: 3312, number of negative: 3414
2023-10-30 21:21:01,644:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.242671 seconds.
2023-10-30 21:21:01,644:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-30 21:21:01,651:INFO:[LightGBM] [Info] Total Bins 1409640
2023-10-30 21:21:01,702:INFO:[LightGBM] [Info] Number of data points in the train set: 6726, number of used features: 5528
2023-10-30 21:21:01,707:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.492417 -> initscore=-0.030332
2023-10-30 21:21:01,707:INFO:[LightGBM] [Info] Start training from score -0.030332
2023-10-30 21:21:35,805:INFO:Uploading results into container
2023-10-30 21:21:35,805:INFO:Uploading model into container now
2023-10-30 21:21:35,811:INFO:_master_model_container: 17
2023-10-30 21:21:35,811:INFO:_display_container: 4
2023-10-30 21:21:35,811:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     n_estimators=100, n_jobs=-1, oob_score=False,
                     random_state=1784, verbose=0, warm_start=False)
2023-10-30 21:21:35,811:INFO:create_model() successfully completed......................................
2023-10-30 21:21:35,944:INFO:Initializing create_model()
2023-10-30 21:21:35,944:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x2a279fcd0>, estimator=catboost, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-10-30 21:21:35,944:INFO:Checking exceptions
2023-10-30 21:21:35,950:INFO:Importing libraries
2023-10-30 21:21:35,950:INFO:Copying training dataset
2023-10-30 21:21:35,965:INFO:Defining folds
2023-10-30 21:21:35,966:INFO:Declaring metric variables
2023-10-30 21:21:35,967:INFO:Importing untrained model
2023-10-30 21:21:35,971:INFO:CatBoost Classifier Imported successfully
2023-10-30 21:21:35,973:INFO:Starting cross validation
2023-10-30 21:21:36,061:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-10-30 21:22:59,988:INFO:[LightGBM] [Info] Number of positive: 2975, number of negative: 3077
2023-10-30 21:23:00,585:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.595265 seconds.
2023-10-30 21:23:00,585:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-30 21:23:00,597:INFO:[LightGBM] [Info] Total Bins 1289280
2023-10-30 21:23:00,744:INFO:[LightGBM] [Info] Number of data points in the train set: 6052, number of used features: 5056
2023-10-30 21:23:00,751:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.491573 -> initscore=-0.033711
2023-10-30 21:23:00,751:INFO:[LightGBM] [Info] Start training from score -0.033711
2023-10-30 21:23:00,893:INFO:[LightGBM] [Info] Number of positive: 2954, number of negative: 3099
2023-10-30 21:23:00,975:INFO:[LightGBM] [Info] Number of positive: 2944, number of negative: 3109
2023-10-30 21:23:01,606:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.626425 seconds.
2023-10-30 21:23:01,607:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-30 21:23:01,611:INFO:[LightGBM] [Info] Total Bins 1292085
2023-10-30 21:23:01,702:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.802594 seconds.
2023-10-30 21:23:01,702:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-30 21:23:01,717:INFO:[LightGBM] [Info] Total Bins 1290810
2023-10-30 21:23:01,787:INFO:[LightGBM] [Info] Number of positive: 2952, number of negative: 3101
2023-10-30 21:23:01,888:INFO:[LightGBM] [Info] Number of data points in the train set: 6053, number of used features: 5067
2023-10-30 21:23:01,896:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.486370 -> initscore=-0.054532
2023-10-30 21:23:01,900:INFO:[LightGBM] [Info] Start training from score -0.054532
2023-10-30 21:23:01,959:INFO:[LightGBM] [Info] Number of data points in the train set: 6053, number of used features: 5062
2023-10-30 21:23:01,968:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.488022 -> initscore=-0.047919
2023-10-30 21:23:01,968:INFO:[LightGBM] [Info] Start training from score -0.047919
2023-10-30 21:23:01,978:INFO:[LightGBM] [Info] Number of positive: 2956, number of negative: 3097
2023-10-30 21:23:02,437:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.645859 seconds.
2023-10-30 21:23:02,437:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-30 21:23:02,445:INFO:[LightGBM] [Info] Total Bins 1286730
2023-10-30 21:23:02,665:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.675144 seconds.
2023-10-30 21:23:02,665:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-30 21:23:02,674:INFO:[LightGBM] [Info] Total Bins 1289790
2023-10-30 21:23:02,684:INFO:[LightGBM] [Info] Number of positive: 2942, number of negative: 3110
2023-10-30 21:23:02,716:INFO:[LightGBM] [Info] Number of data points in the train set: 6053, number of used features: 5046
2023-10-30 21:23:02,725:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.487692 -> initscore=-0.049242
2023-10-30 21:23:02,725:INFO:[LightGBM] [Info] Start training from score -0.049242
2023-10-30 21:23:02,731:INFO:[LightGBM] [Info] Number of positive: 2946, number of negative: 3107
2023-10-30 21:23:02,847:INFO:[LightGBM] [Info] Number of data points in the train set: 6053, number of used features: 5058
2023-10-30 21:23:02,855:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.488353 -> initscore=-0.046597
2023-10-30 21:23:02,855:INFO:[LightGBM] [Info] Start training from score -0.046597
2023-10-30 21:23:03,221:INFO:[LightGBM] [Info] Number of positive: 2921, number of negative: 3132
2023-10-30 21:23:03,225:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.490109 seconds.
2023-10-30 21:23:03,225:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-30 21:23:03,234:INFO:[LightGBM] [Info] Total Bins 1294890
2023-10-30 21:23:03,425:INFO:[LightGBM] [Info] Number of data points in the train set: 6053, number of used features: 5078
2023-10-30 21:23:03,433:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.486701 -> initscore=-0.053209
2023-10-30 21:23:03,433:INFO:[LightGBM] [Info] Start training from score -0.053209
2023-10-30 21:23:03,452:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.764250 seconds.
2023-10-30 21:23:03,452:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-30 21:23:03,459:INFO:[LightGBM] [Info] Total Bins 1294635
2023-10-30 21:23:03,627:INFO:[LightGBM] [Info] Number of data points in the train set: 6052, number of used features: 5077
2023-10-30 21:23:03,638:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.486120 -> initscore=-0.055533
2023-10-30 21:23:03,638:INFO:[LightGBM] [Info] Start training from score -0.055533
2023-10-30 21:23:03,955:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.726968 seconds.
2023-10-30 21:23:03,955:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-30 21:23:03,969:INFO:[LightGBM] [Info] Total Bins 1294890
2023-10-30 21:23:04,232:INFO:[LightGBM] [Info] Number of data points in the train set: 6053, number of used features: 5078
2023-10-30 21:23:04,239:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.482571 -> initscore=-0.069746
2023-10-30 21:23:04,239:INFO:[LightGBM] [Info] Start training from score -0.069746
2023-10-30 21:23:04,615:INFO:[LightGBM] [Info] Number of positive: 2929, number of negative: 3124
2023-10-30 21:23:05,239:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.616825 seconds.
2023-10-30 21:23:05,240:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-30 21:23:05,248:INFO:[LightGBM] [Info] Total Bins 1291830
2023-10-30 21:23:05,544:INFO:[LightGBM] [Info] Number of data points in the train set: 6053, number of used features: 5066
2023-10-30 21:23:05,559:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.483892 -> initscore=-0.064453
2023-10-30 21:23:05,559:INFO:[LightGBM] [Info] Start training from score -0.064453
2023-10-30 21:24:52,188:INFO:[LightGBM] [Info] Number of positive: 2940, number of negative: 3113
2023-10-30 21:24:52,868:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.661319 seconds.
2023-10-30 21:24:52,868:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-30 21:24:52,875:INFO:[LightGBM] [Info] Total Bins 1291830
2023-10-30 21:24:53,111:INFO:[LightGBM] [Info] Number of data points in the train set: 6053, number of used features: 5066
2023-10-30 21:24:53,125:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.485710 -> initscore=-0.057177
2023-10-30 21:24:53,125:INFO:[LightGBM] [Info] Start training from score -0.057177
2023-10-30 21:26:04,245:INFO:Calculating mean and std
2023-10-30 21:26:04,248:INFO:Creating metrics dataframe
2023-10-30 21:26:04,255:INFO:Finalizing model
2023-10-30 21:35:34,712:INFO:[LightGBM] [Info] Number of positive: 3312, number of negative: 3414
2023-10-30 21:35:35,004:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.288571 seconds.
2023-10-30 21:35:35,004:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-30 21:35:35,011:INFO:[LightGBM] [Info] Total Bins 1409640
2023-10-30 21:35:35,064:INFO:[LightGBM] [Info] Number of data points in the train set: 6726, number of used features: 5528
2023-10-30 21:35:35,070:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.492417 -> initscore=-0.030332
2023-10-30 21:35:35,070:INFO:[LightGBM] [Info] Start training from score -0.030332
2023-10-30 21:36:13,046:INFO:Uploading results into container
2023-10-30 21:36:13,047:INFO:Uploading model into container now
2023-10-30 21:36:13,053:INFO:_master_model_container: 18
2023-10-30 21:36:13,053:INFO:_display_container: 5
2023-10-30 21:36:13,053:INFO:<catboost.core.CatBoostClassifier object at 0x148c4ac50>
2023-10-30 21:36:13,053:INFO:create_model() successfully completed......................................
2023-10-30 21:37:32,906:INFO:Initializing create_model()
2023-10-30 21:37:32,906:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x2a279fcd0>, estimator=qda, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-10-30 21:37:32,906:INFO:Checking exceptions
2023-10-30 21:37:32,914:INFO:Importing libraries
2023-10-30 21:37:32,914:INFO:Copying training dataset
2023-10-30 21:37:32,943:INFO:Defining folds
2023-10-30 21:37:32,943:INFO:Declaring metric variables
2023-10-30 21:37:32,944:INFO:Importing untrained model
2023-10-30 21:37:32,946:INFO:Quadratic Discriminant Analysis Imported successfully
2023-10-30 21:37:32,949:INFO:Starting cross validation
2023-10-30 21:37:33,114:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-10-30 21:39:11,654:INFO:[LightGBM] [Info] Number of positive: 2954, number of negative: 3099
2023-10-30 21:39:12,206:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.551213 seconds.
2023-10-30 21:39:12,206:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-30 21:39:12,212:INFO:[LightGBM] [Info] Total Bins 1290810
2023-10-30 21:39:12,346:INFO:[LightGBM] [Info] Number of data points in the train set: 6053, number of used features: 5062
2023-10-30 21:39:12,354:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.488022 -> initscore=-0.047919
2023-10-30 21:39:12,354:INFO:[LightGBM] [Info] Start training from score -0.047919
2023-10-30 21:39:13,048:INFO:[LightGBM] [Info] Number of positive: 2975, number of negative: 3077
2023-10-30 21:39:13,520:INFO:[LightGBM] [Info] Number of positive: 2942, number of negative: 3110
2023-10-30 21:39:13,648:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.596599 seconds.
2023-10-30 21:39:13,648:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-30 21:39:13,655:INFO:[LightGBM] [Info] Total Bins 1289280
2023-10-30 21:39:13,848:INFO:[LightGBM] [Info] Number of data points in the train set: 6052, number of used features: 5056
2023-10-30 21:39:13,866:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.491573 -> initscore=-0.033711
2023-10-30 21:39:13,867:INFO:[LightGBM] [Info] Start training from score -0.033711
2023-10-30 21:39:14,243:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.716236 seconds.
2023-10-30 21:39:14,243:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-30 21:39:14,259:INFO:[LightGBM] [Info] Total Bins 1294635
2023-10-30 21:39:14,368:INFO:[LightGBM] [Info] Number of data points in the train set: 6052, number of used features: 5077
2023-10-30 21:39:14,376:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.486120 -> initscore=-0.055533
2023-10-30 21:39:14,376:INFO:[LightGBM] [Info] Start training from score -0.055533
2023-10-30 21:39:14,538:INFO:[LightGBM] [Info] Number of positive: 2929, number of negative: 3124
2023-10-30 21:39:14,708:INFO:[LightGBM] [Info] Number of positive: 2956, number of negative: 3097
2023-10-30 21:39:14,840:INFO:[LightGBM] [Info] Number of positive: 2921, number of negative: 3132
2023-10-30 21:39:15,035:INFO:[LightGBM] [Info] Number of positive: 2944, number of negative: 3109
2023-10-30 21:39:15,209:INFO:[LightGBM] [Info] Number of positive: 2952, number of negative: 3101
2023-10-30 21:39:15,334:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.622569 seconds.
2023-10-30 21:39:15,334:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-30 21:39:15,344:INFO:[LightGBM] [Info] Total Bins 1289790
2023-10-30 21:39:15,418:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.876264 seconds.
2023-10-30 21:39:15,418:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-30 21:39:15,444:INFO:[LightGBM] [Info] Total Bins 1291830
2023-10-30 21:39:15,549:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.704033 seconds.
2023-10-30 21:39:15,552:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-30 21:39:15,564:INFO:[LightGBM] [Info] Total Bins 1294890
2023-10-30 21:39:15,670:INFO:[LightGBM] [Info] Number of data points in the train set: 6053, number of used features: 5058
2023-10-30 21:39:15,686:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.488353 -> initscore=-0.046597
2023-10-30 21:39:15,687:INFO:[LightGBM] [Info] Start training from score -0.046597
2023-10-30 21:39:15,824:INFO:[LightGBM] [Info] Number of data points in the train set: 6053, number of used features: 5066
2023-10-30 21:39:15,851:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.483892 -> initscore=-0.064453
2023-10-30 21:39:15,852:INFO:[LightGBM] [Info] Start training from score -0.064453
2023-10-30 21:39:15,855:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.800230 seconds.
2023-10-30 21:39:15,855:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-30 21:39:15,862:INFO:[LightGBM] [Info] Number of data points in the train set: 6053, number of used features: 5078
2023-10-30 21:39:15,870:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.482571 -> initscore=-0.069746
2023-10-30 21:39:15,870:INFO:[LightGBM] [Info] Start training from score -0.069746
2023-10-30 21:39:15,874:INFO:[LightGBM] [Info] Total Bins 1292085
2023-10-30 21:39:16,034:INFO:[LightGBM] [Info] Number of data points in the train set: 6053, number of used features: 5067
2023-10-30 21:39:16,045:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.486370 -> initscore=-0.054532
2023-10-30 21:39:16,045:INFO:[LightGBM] [Info] Start training from score -0.054532
2023-10-30 21:39:16,363:INFO:[LightGBM] [Info] Number of positive: 2946, number of negative: 3107
2023-10-30 21:39:16,394:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 1.169165 seconds.
2023-10-30 21:39:16,394:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-30 21:39:16,409:INFO:[LightGBM] [Info] Total Bins 1286730
2023-10-30 21:39:16,651:INFO:[LightGBM] [Info] Number of data points in the train set: 6053, number of used features: 5046
2023-10-30 21:39:16,677:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.487692 -> initscore=-0.049242
2023-10-30 21:39:16,677:INFO:[LightGBM] [Info] Start training from score -0.049242
2023-10-30 21:39:17,243:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.862099 seconds.
2023-10-30 21:39:17,245:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-30 21:39:17,274:INFO:[LightGBM] [Info] Total Bins 1294890
2023-10-30 21:39:17,604:INFO:[LightGBM] [Info] Number of data points in the train set: 6053, number of used features: 5078
2023-10-30 21:39:17,628:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.486701 -> initscore=-0.053209
2023-10-30 21:39:17,629:INFO:[LightGBM] [Info] Start training from score -0.053209
2023-10-30 21:41:20,889:INFO:[LightGBM] [Info] Number of positive: 2940, number of negative: 3113
2023-10-30 21:41:21,629:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.727049 seconds.
2023-10-30 21:41:21,638:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-30 21:41:21,643:INFO:[LightGBM] [Info] Total Bins 1291830
2023-10-30 21:41:21,898:INFO:[LightGBM] [Info] Number of data points in the train set: 6053, number of used features: 5066
2023-10-30 21:41:21,914:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.485710 -> initscore=-0.057177
2023-10-30 21:41:21,915:INFO:[LightGBM] [Info] Start training from score -0.057177
2023-10-30 21:42:36,925:INFO:Calculating mean and std
2023-10-30 21:42:36,937:INFO:Creating metrics dataframe
2023-10-30 21:42:36,962:INFO:Finalizing model
2023-10-30 21:51:57,346:INFO:[LightGBM] [Info] Number of positive: 3312, number of negative: 3414
2023-10-30 21:51:57,614:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.264606 seconds.
2023-10-30 21:51:57,614:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-30 21:51:57,622:INFO:[LightGBM] [Info] Total Bins 1409640
2023-10-30 21:51:57,674:INFO:[LightGBM] [Info] Number of data points in the train set: 6726, number of used features: 5528
2023-10-30 21:51:57,682:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.492417 -> initscore=-0.030332
2023-10-30 21:51:57,682:INFO:[LightGBM] [Info] Start training from score -0.030332
2023-10-30 21:52:31,590:INFO:Uploading results into container
2023-10-30 21:52:31,591:INFO:Uploading model into container now
2023-10-30 21:52:31,602:INFO:_master_model_container: 19
2023-10-30 21:52:31,602:INFO:_display_container: 6
2023-10-30 21:52:31,603:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2023-10-30 21:52:31,603:INFO:create_model() successfully completed......................................
2023-10-30 21:53:39,637:INFO:Initializing create_model()
2023-10-30 21:53:39,637:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x2a279fcd0>, estimator=xgboost, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-10-30 21:53:39,637:INFO:Checking exceptions
2023-10-30 21:54:00,549:INFO:Initializing create_model()
2023-10-30 21:54:00,549:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x2a279fcd0>, estimator=rfc, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-10-30 21:54:00,549:INFO:Checking exceptions
2023-10-30 21:54:13,376:INFO:Initializing create_model()
2023-10-30 21:54:13,377:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x2a279fcd0>, estimator=rf, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-10-30 21:54:13,377:INFO:Checking exceptions
2023-10-30 21:54:13,383:INFO:Importing libraries
2023-10-30 21:54:13,383:INFO:Copying training dataset
2023-10-30 21:54:13,426:INFO:Defining folds
2023-10-30 21:54:13,426:INFO:Declaring metric variables
2023-10-30 21:54:13,428:INFO:Importing untrained model
2023-10-30 21:54:13,429:INFO:Random Forest Classifier Imported successfully
2023-10-30 21:54:13,432:INFO:Starting cross validation
2023-10-30 21:54:13,841:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-10-30 21:55:49,518:INFO:[LightGBM] [Info] Number of positive: 2921, number of negative: 3132
2023-10-30 21:55:50,076:INFO:[LightGBM] [Info] Number of positive: 2946, number of negative: 3107
2023-10-30 21:55:50,132:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.622018 seconds.
2023-10-30 21:55:50,133:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-30 21:55:50,140:INFO:[LightGBM] [Info] Total Bins 1294890
2023-10-30 21:55:50,326:INFO:[LightGBM] [Info] Number of data points in the train set: 6053, number of used features: 5078
2023-10-30 21:55:50,334:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.482571 -> initscore=-0.069746
2023-10-30 21:55:50,335:INFO:[LightGBM] [Info] Start training from score -0.069746
2023-10-30 21:55:50,532:INFO:[LightGBM] [Info] Number of positive: 2954, number of negative: 3099
2023-10-30 21:55:50,773:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.695303 seconds.
2023-10-30 21:55:50,773:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-30 21:55:50,782:INFO:[LightGBM] [Info] Total Bins 1294890
2023-10-30 21:55:50,918:INFO:[LightGBM] [Info] Number of data points in the train set: 6053, number of used features: 5078
2023-10-30 21:55:50,929:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.486701 -> initscore=-0.053209
2023-10-30 21:55:50,931:INFO:[LightGBM] [Info] Start training from score -0.053209
2023-10-30 21:55:51,304:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.770210 seconds.
2023-10-30 21:55:51,304:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-30 21:55:51,312:INFO:[LightGBM] [Info] Total Bins 1290810
2023-10-30 21:55:51,526:INFO:[LightGBM] [Info] Number of data points in the train set: 6053, number of used features: 5062
2023-10-30 21:55:51,535:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.488022 -> initscore=-0.047919
2023-10-30 21:55:51,535:INFO:[LightGBM] [Info] Start training from score -0.047919
2023-10-30 21:55:52,056:INFO:[LightGBM] [Info] Number of positive: 2956, number of negative: 3097
2023-10-30 21:55:52,194:INFO:[LightGBM] [Info] Number of positive: 2952, number of negative: 3101
2023-10-30 21:55:52,211:INFO:[LightGBM] [Info] Number of positive: 2929, number of negative: 3124
2023-10-30 21:55:52,421:INFO:[LightGBM] [Info] Number of positive: 2975, number of negative: 3077
2023-10-30 21:55:52,544:INFO:[LightGBM] [Info] Number of positive: 2944, number of negative: 3109
2023-10-30 21:55:52,609:INFO:[LightGBM] [Info] Number of positive: 2942, number of negative: 3110
2023-10-30 21:55:52,807:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.608071 seconds.
2023-10-30 21:55:52,808:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-30 21:55:52,816:INFO:[LightGBM] [Info] Total Bins 1286730
2023-10-30 21:55:52,986:INFO:[LightGBM] [Info] Number of data points in the train set: 6053, number of used features: 5046
2023-10-30 21:55:52,996:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.487692 -> initscore=-0.049242
2023-10-30 21:55:52,997:INFO:[LightGBM] [Info] Start training from score -0.049242
2023-10-30 21:55:53,023:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.808182 seconds.
2023-10-30 21:55:53,023:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-30 21:55:53,030:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.968750 seconds.
2023-10-30 21:55:53,031:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-30 21:55:53,031:INFO:[LightGBM] [Info] Total Bins 1291830
2023-10-30 21:55:53,036:INFO:[LightGBM] [Info] Total Bins 1289790
2023-10-30 21:55:53,251:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.821609 seconds.
2023-10-30 21:55:53,252:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-30 21:55:53,266:INFO:[LightGBM] [Info] Total Bins 1289280
2023-10-30 21:55:53,309:INFO:[LightGBM] [Info] Number of data points in the train set: 6053, number of used features: 5066
2023-10-30 21:55:53,324:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.483892 -> initscore=-0.064453
2023-10-30 21:55:53,325:INFO:[LightGBM] [Info] Start training from score -0.064453
2023-10-30 21:55:53,441:INFO:[LightGBM] [Info] Number of data points in the train set: 6053, number of used features: 5058
2023-10-30 21:55:53,479:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.488353 -> initscore=-0.046597
2023-10-30 21:55:53,481:INFO:[LightGBM] [Info] Start training from score -0.046597
2023-10-30 21:55:53,525:INFO:[LightGBM] [Info] Number of data points in the train set: 6052, number of used features: 5056
2023-10-30 21:55:53,535:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.491573 -> initscore=-0.033711
2023-10-30 21:55:53,535:INFO:[LightGBM] [Info] Start training from score -0.033711
2023-10-30 21:55:53,816:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 1.253017 seconds.
2023-10-30 21:55:53,816:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-30 21:55:53,831:INFO:[LightGBM] [Info] Total Bins 1292085
2023-10-30 21:55:53,923:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 1.310271 seconds.
2023-10-30 21:55:53,924:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-30 21:55:53,932:INFO:[LightGBM] [Info] Total Bins 1294635
2023-10-30 21:55:54,183:INFO:[LightGBM] [Info] Number of data points in the train set: 6053, number of used features: 5067
2023-10-30 21:55:54,207:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.486370 -> initscore=-0.054532
2023-10-30 21:55:54,208:INFO:[LightGBM] [Info] Start training from score -0.054532
2023-10-30 21:55:54,303:INFO:[LightGBM] [Info] Number of data points in the train set: 6052, number of used features: 5077
2023-10-30 21:55:54,316:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.486120 -> initscore=-0.055533
2023-10-30 21:55:54,316:INFO:[LightGBM] [Info] Start training from score -0.055533
2023-10-30 21:58:00,445:INFO:[LightGBM] [Info] Number of positive: 2940, number of negative: 3113
2023-10-30 21:58:01,314:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.854563 seconds.
2023-10-30 21:58:01,318:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-30 21:58:01,324:INFO:[LightGBM] [Info] Total Bins 1291830
2023-10-30 21:58:01,571:INFO:[LightGBM] [Info] Number of data points in the train set: 6053, number of used features: 5066
2023-10-30 21:58:01,590:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.485710 -> initscore=-0.057177
2023-10-30 21:58:01,590:INFO:[LightGBM] [Info] Start training from score -0.057177
2023-10-30 21:59:15,706:INFO:Calculating mean and std
2023-10-30 21:59:15,718:INFO:Creating metrics dataframe
2023-10-30 21:59:15,746:INFO:Finalizing model
2023-10-30 22:08:47,711:INFO:[LightGBM] [Info] Number of positive: 3312, number of negative: 3414
2023-10-30 22:08:48,061:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.347369 seconds.
2023-10-30 22:08:48,061:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-30 22:08:48,068:INFO:[LightGBM] [Info] Total Bins 1409640
2023-10-30 22:08:48,120:INFO:[LightGBM] [Info] Number of data points in the train set: 6726, number of used features: 5528
2023-10-30 22:08:48,125:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.492417 -> initscore=-0.030332
2023-10-30 22:08:48,125:INFO:[LightGBM] [Info] Start training from score -0.030332
2023-10-30 22:09:23,681:INFO:Uploading results into container
2023-10-30 22:09:23,681:INFO:Uploading model into container now
2023-10-30 22:09:23,689:INFO:_master_model_container: 20
2023-10-30 22:09:23,689:INFO:_display_container: 7
2023-10-30 22:09:23,689:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=100, n_jobs=-1, oob_score=False,
                       random_state=1784, verbose=0, warm_start=False)
2023-10-30 22:09:23,689:INFO:create_model() successfully completed......................................
2023-10-30 22:09:24,040:INFO:Initializing create_model()
2023-10-30 22:09:24,040:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x2a279fcd0>, estimator=lgbm, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-10-30 22:09:24,040:INFO:Checking exceptions
2023-10-30 22:15:37,574:INFO:Initializing create_model()
2023-10-30 22:15:37,574:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x2a279fcd0>, estimator=lightgbm, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-10-30 22:15:37,574:INFO:Checking exceptions
2023-10-30 22:15:37,582:INFO:Importing libraries
2023-10-30 22:15:37,582:INFO:Copying training dataset
2023-10-30 22:15:37,632:INFO:Defining folds
2023-10-30 22:15:37,632:INFO:Declaring metric variables
2023-10-30 22:15:37,634:INFO:Importing untrained model
2023-10-30 22:15:37,636:INFO:Light Gradient Boosting Machine Imported successfully
2023-10-30 22:15:37,640:INFO:Starting cross validation
2023-10-30 22:15:38,053:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-10-30 22:17:04,441:INFO:[LightGBM] [Info] Number of positive: 2954, number of negative: 3099
2023-10-30 22:17:04,443:INFO:[LightGBM] [Info] Number of positive: 2942, number of negative: 3110
2023-10-30 22:17:04,662:INFO:[LightGBM] [Info] Number of positive: 2944, number of negative: 3109
2023-10-30 22:17:04,794:INFO:[LightGBM] [Info] Number of positive: 2956, number of negative: 3097
2023-10-30 22:17:04,800:INFO:[LightGBM] [Info] Number of positive: 2952, number of negative: 3101
2023-10-30 22:17:05,028:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.576359 seconds.
2023-10-30 22:17:05,029:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-30 22:17:05,038:INFO:[LightGBM] [Info] Total Bins 1294635
2023-10-30 22:17:05,157:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.707783 seconds.
2023-10-30 22:17:05,157:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-30 22:17:05,171:INFO:[LightGBM] [Info] Total Bins 1290810
2023-10-30 22:17:05,301:INFO:[LightGBM] [Info] Number of data points in the train set: 6052, number of used features: 5077
2023-10-30 22:17:05,312:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.486120 -> initscore=-0.055533
2023-10-30 22:17:05,313:INFO:[LightGBM] [Info] Start training from score -0.055533
2023-10-30 22:17:05,383:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.710327 seconds.
2023-10-30 22:17:05,383:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-30 22:17:05,394:INFO:[LightGBM] [Info] Number of data points in the train set: 6053, number of used features: 5062
2023-10-30 22:17:05,402:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.592467 seconds.
2023-10-30 22:17:05,403:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-30 22:17:05,405:INFO:[LightGBM] [Info] Total Bins 1292085
2023-10-30 22:17:05,409:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.488022 -> initscore=-0.047919
2023-10-30 22:17:05,410:INFO:[LightGBM] [Info] Start training from score -0.047919
2023-10-30 22:17:05,410:INFO:[LightGBM] [Info] Total Bins 1289790
2023-10-30 22:17:05,430:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.610674 seconds.
2023-10-30 22:17:05,431:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-30 22:17:05,448:INFO:[LightGBM] [Info] Total Bins 1286730
2023-10-30 22:17:05,674:INFO:[LightGBM] [Info] Number of data points in the train set: 6053, number of used features: 5067
2023-10-30 22:17:05,675:INFO:[LightGBM] [Info] Number of data points in the train set: 6053, number of used features: 5058
2023-10-30 22:17:05,686:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.486370 -> initscore=-0.054532
2023-10-30 22:17:05,686:INFO:[LightGBM] [Info] Start training from score -0.054532
2023-10-30 22:17:05,689:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.488353 -> initscore=-0.046597
2023-10-30 22:17:05,689:INFO:[LightGBM] [Info] Start training from score -0.046597
2023-10-30 22:17:05,749:INFO:[LightGBM] [Info] Number of data points in the train set: 6053, number of used features: 5046
2023-10-30 22:17:05,759:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.487692 -> initscore=-0.049242
2023-10-30 22:17:05,759:INFO:[LightGBM] [Info] Start training from score -0.049242
2023-10-30 22:17:09,723:INFO:[LightGBM] [Info] Number of positive: 2975, number of negative: 3077
2023-10-30 22:17:09,752:INFO:[LightGBM] [Info] Number of positive: 2929, number of negative: 3124
2023-10-30 22:17:10,534:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.768106 seconds.
2023-10-30 22:17:10,534:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-30 22:17:10,544:INFO:[LightGBM] [Info] Total Bins 1291830
2023-10-30 22:17:10,564:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.825201 seconds.
2023-10-30 22:17:10,564:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-30 22:17:10,575:INFO:[LightGBM] [Info] Total Bins 1289280
2023-10-30 22:17:10,809:INFO:[LightGBM] [Info] Number of data points in the train set: 6053, number of used features: 5066
2023-10-30 22:17:10,817:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.483892 -> initscore=-0.064453
2023-10-30 22:17:10,817:INFO:[LightGBM] [Info] Start training from score -0.064453
2023-10-30 22:17:10,847:INFO:[LightGBM] [Info] Number of positive: 2946, number of negative: 3107
2023-10-30 22:17:10,894:INFO:[LightGBM] [Info] Number of data points in the train set: 6052, number of used features: 5056
2023-10-30 22:17:10,901:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.491573 -> initscore=-0.033711
2023-10-30 22:17:10,901:INFO:[LightGBM] [Info] Start training from score -0.033711
2023-10-30 22:17:11,273:INFO:[LightGBM] [Info] Number of positive: 2921, number of negative: 3132
2023-10-30 22:17:11,450:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.581794 seconds.
2023-10-30 22:17:11,451:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-30 22:17:11,461:INFO:[LightGBM] [Info] Total Bins 1294890
2023-10-30 22:17:11,696:INFO:[LightGBM] [Info] Number of data points in the train set: 6053, number of used features: 5078
2023-10-30 22:17:11,708:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.486701 -> initscore=-0.053209
2023-10-30 22:17:11,708:INFO:[LightGBM] [Info] Start training from score -0.053209
2023-10-30 22:17:11,773:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.480345 seconds.
2023-10-30 22:17:11,775:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-30 22:17:11,786:INFO:[LightGBM] [Info] Total Bins 1294890
2023-10-30 22:17:12,005:INFO:[LightGBM] [Info] Number of data points in the train set: 6053, number of used features: 5078
2023-10-30 22:17:12,028:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.482571 -> initscore=-0.069746
2023-10-30 22:17:12,028:INFO:[LightGBM] [Info] Start training from score -0.069746
2023-10-30 22:19:05,794:INFO:[LightGBM] [Info] Number of positive: 2940, number of negative: 3113
2023-10-30 22:19:06,476:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.673455 seconds.
2023-10-30 22:19:06,492:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-30 22:19:06,494:INFO:[LightGBM] [Info] Total Bins 1291830
2023-10-30 22:19:06,698:INFO:[LightGBM] [Info] Number of data points in the train set: 6053, number of used features: 5066
2023-10-30 22:19:06,726:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.485710 -> initscore=-0.057177
2023-10-30 22:19:06,727:INFO:[LightGBM] [Info] Start training from score -0.057177
2023-10-30 22:19:42,644:INFO:[LightGBM] [Info] Number of positive: 2956, number of negative: 3097
2023-10-30 22:19:42,648:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002497 seconds.
2023-10-30 22:19:42,648:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-30 22:19:42,648:INFO:[LightGBM] [Info] Total Bins 15045
2023-10-30 22:19:42,648:INFO:[LightGBM] [Info] Number of data points in the train set: 6053, number of used features: 59
2023-10-30 22:19:42,649:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.488353 -> initscore=-0.046597
2023-10-30 22:19:42,650:INFO:[LightGBM] [Info] Start training from score -0.046597
2023-10-30 22:19:43,048:INFO:[LightGBM] [Info] Number of positive: 2954, number of negative: 3099
2023-10-30 22:19:43,058:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008541 seconds.
2023-10-30 22:19:43,058:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-30 22:19:43,058:INFO:[LightGBM] [Info] Total Bins 15045
2023-10-30 22:19:43,059:INFO:[LightGBM] [Info] Number of data points in the train set: 6053, number of used features: 59
2023-10-30 22:19:43,059:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.488022 -> initscore=-0.047919
2023-10-30 22:19:43,059:INFO:[LightGBM] [Info] Start training from score -0.047919
2023-10-30 22:19:43,580:INFO:[LightGBM] [Info] Number of positive: 2944, number of negative: 3109
2023-10-30 22:19:43,582:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001701 seconds.
2023-10-30 22:19:43,582:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-30 22:19:43,582:INFO:[LightGBM] [Info] Total Bins 15045
2023-10-30 22:19:43,587:INFO:[LightGBM] [Info] Number of data points in the train set: 6053, number of used features: 59
2023-10-30 22:19:43,587:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.486370 -> initscore=-0.054532
2023-10-30 22:19:43,587:INFO:[LightGBM] [Info] Start training from score -0.054532
2023-10-30 22:19:43,677:INFO:[LightGBM] [Info] Number of positive: 2952, number of negative: 3101
2023-10-30 22:19:43,679:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002015 seconds.
2023-10-30 22:19:43,679:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-30 22:19:43,681:INFO:[LightGBM] [Info] Total Bins 15045
2023-10-30 22:19:43,681:INFO:[LightGBM] [Info] Number of data points in the train set: 6053, number of used features: 59
2023-10-30 22:19:43,681:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.487692 -> initscore=-0.049242
2023-10-30 22:19:43,681:INFO:[LightGBM] [Info] Start training from score -0.049242
2023-10-30 22:19:43,717:INFO:[LightGBM] [Info] Number of positive: 2942, number of negative: 3110
2023-10-30 22:19:43,720:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001689 seconds.
2023-10-30 22:19:43,726:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-30 22:19:43,727:INFO:[LightGBM] [Info] Total Bins 15045
2023-10-30 22:19:43,727:INFO:[LightGBM] [Info] Number of data points in the train set: 6052, number of used features: 59
2023-10-30 22:19:43,727:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.486120 -> initscore=-0.055533
2023-10-30 22:19:43,727:INFO:[LightGBM] [Info] Start training from score -0.055533
2023-10-30 22:19:46,038:INFO:[LightGBM] [Info] Number of positive: 2975, number of negative: 3077
2023-10-30 22:19:46,040:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001683 seconds.
2023-10-30 22:19:46,040:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-30 22:19:46,040:INFO:[LightGBM] [Info] Total Bins 15045
2023-10-30 22:19:46,041:INFO:[LightGBM] [Info] Number of data points in the train set: 6052, number of used features: 59
2023-10-30 22:19:46,041:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.491573 -> initscore=-0.033711
2023-10-30 22:19:46,041:INFO:[LightGBM] [Info] Start training from score -0.033711
2023-10-30 22:19:46,300:INFO:[LightGBM] [Info] Number of positive: 2946, number of negative: 3107
2023-10-30 22:19:46,301:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001609 seconds.
2023-10-30 22:19:46,301:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-30 22:19:46,301:INFO:[LightGBM] [Info] Total Bins 15045
2023-10-30 22:19:46,302:INFO:[LightGBM] [Info] Number of data points in the train set: 6053, number of used features: 59
2023-10-30 22:19:46,302:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.486701 -> initscore=-0.053209
2023-10-30 22:19:46,302:INFO:[LightGBM] [Info] Start training from score -0.053209
2023-10-30 22:19:46,606:INFO:[LightGBM] [Info] Number of positive: 2921, number of negative: 3132
2023-10-30 22:19:46,608:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001568 seconds.
2023-10-30 22:19:46,608:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-30 22:19:46,608:INFO:[LightGBM] [Info] Total Bins 15045
2023-10-30 22:19:46,608:INFO:[LightGBM] [Info] Number of data points in the train set: 6053, number of used features: 59
2023-10-30 22:19:46,608:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.482571 -> initscore=-0.069746
2023-10-30 22:19:46,609:INFO:[LightGBM] [Info] Start training from score -0.069746
2023-10-30 22:19:46,718:INFO:[LightGBM] [Info] Number of positive: 2929, number of negative: 3124
2023-10-30 22:19:46,720:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001737 seconds.
2023-10-30 22:19:46,720:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-30 22:19:46,720:INFO:[LightGBM] [Info] Total Bins 15045
2023-10-30 22:19:46,721:INFO:[LightGBM] [Info] Number of data points in the train set: 6053, number of used features: 59
2023-10-30 22:19:46,721:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.483892 -> initscore=-0.064453
2023-10-30 22:19:46,721:INFO:[LightGBM] [Info] Start training from score -0.064453
2023-10-30 22:20:07,568:INFO:[LightGBM] [Info] Number of positive: 2940, number of negative: 3113
2023-10-30 22:20:07,570:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001540 seconds.
2023-10-30 22:20:07,570:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-30 22:20:07,570:INFO:[LightGBM] [Info] Total Bins 15045
2023-10-30 22:20:07,570:INFO:[LightGBM] [Info] Number of data points in the train set: 6053, number of used features: 59
2023-10-30 22:20:07,570:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.485710 -> initscore=-0.057177
2023-10-30 22:20:07,570:INFO:[LightGBM] [Info] Start training from score -0.057177
2023-10-30 22:20:08,749:INFO:Calculating mean and std
2023-10-30 22:20:08,758:INFO:Creating metrics dataframe
2023-10-30 22:20:08,783:INFO:Finalizing model
2023-10-30 22:29:12,333:INFO:[LightGBM] [Info] Number of positive: 3312, number of negative: 3414
2023-10-30 22:29:12,623:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.285801 seconds.
2023-10-30 22:29:12,623:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-30 22:29:12,631:INFO:[LightGBM] [Info] Total Bins 1409640
2023-10-30 22:29:12,687:INFO:[LightGBM] [Info] Number of data points in the train set: 6726, number of used features: 5528
2023-10-30 22:29:12,693:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.492417 -> initscore=-0.030332
2023-10-30 22:29:12,693:INFO:[LightGBM] [Info] Start training from score -0.030332
2023-10-30 22:29:46,743:INFO:[LightGBM] [Info] Number of positive: 3312, number of negative: 3414
2023-10-30 22:29:46,745:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001703 seconds.
2023-10-30 22:29:46,745:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-30 22:29:46,745:INFO:[LightGBM] [Info] Total Bins 15045
2023-10-30 22:29:46,746:INFO:[LightGBM] [Info] Number of data points in the train set: 6726, number of used features: 59
2023-10-30 22:29:46,746:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.492417 -> initscore=-0.030332
2023-10-30 22:29:46,746:INFO:[LightGBM] [Info] Start training from score -0.030332
2023-10-30 22:29:47,124:INFO:Uploading results into container
2023-10-30 22:29:47,124:INFO:Uploading model into container now
2023-10-30 22:29:47,130:INFO:_master_model_container: 21
2023-10-30 22:29:47,130:INFO:_display_container: 8
2023-10-30 22:29:47,131:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=1784, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2023-10-30 22:29:47,131:INFO:create_model() successfully completed......................................
2023-10-30 22:31:57,299:INFO:Initializing create_model()
2023-10-30 22:31:57,300:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x2a279fcd0>, estimator=xgboost, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-10-30 22:31:57,300:INFO:Checking exceptions
2023-10-30 22:32:00,258:INFO:Initializing create_model()
2023-10-30 22:32:00,258:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x2a279fcd0>, estimator=xgb, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-10-30 22:32:00,258:INFO:Checking exceptions
2023-10-30 22:32:09,653:INFO:Initializing create_model()
2023-10-30 22:32:09,654:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x2a279fcd0>, estimator=xgboost, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-10-30 22:32:09,654:INFO:Checking exceptions
2023-10-30 22:33:38,042:INFO:Initializing tune_model()
2023-10-30 22:33:38,042:INFO:tune_model(estimator=<catboost.core.CatBoostClassifier object at 0x148c4ac50>, fold=None, round=4, n_iter=10, custom_grid=None, optimize=AUC, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={}, self=<pycaret.classification.oop.ClassificationExperiment object at 0x2a279fcd0>)
2023-10-30 22:33:38,042:INFO:Checking exceptions
2023-10-30 22:33:38,060:INFO:Copying training dataset
2023-10-30 22:33:38,068:INFO:Checking base model
2023-10-30 22:33:38,068:INFO:Base model : CatBoost Classifier
2023-10-30 22:33:38,071:INFO:Declaring metric variables
2023-10-30 22:33:38,073:INFO:Defining Hyperparameters
2023-10-30 22:33:38,215:INFO:Tuning with n_jobs=-1
2023-10-30 22:33:38,215:INFO:Initializing RandomizedSearchCV
2023-10-30 22:41:58,215:WARNING:A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.

2023-10-30 23:13:35,972:WARNING:
10 fits failed out of a total of 100.
The score on these train-test partitions for these parameters will be set to nan.
If these failures are not expected, you can try to debug them by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
10 fits failed with the following error:
Traceback (most recent call last):
  File "/Users/tanmaysharma/projects/Jak2Biotech/.venv/lib/python3.10/site-packages/sklearn/model_selection/_validation.py", line 686, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "/Users/tanmaysharma/projects/Jak2Biotech/.venv/lib/python3.10/site-packages/pycaret/internal/pipeline.py", line 267, in fit
    fitted_estimator = self._memory_fit(
  File "/Users/tanmaysharma/projects/Jak2Biotech/.venv/lib/python3.10/site-packages/joblib/memory.py", line 353, in __call__
    return self.func(*args, **kwargs)
  File "/Users/tanmaysharma/projects/Jak2Biotech/.venv/lib/python3.10/site-packages/pycaret/internal/pipeline.py", line 66, in _fit_one
    transformer.fit(*args, **fit_params)
  File "/Users/tanmaysharma/projects/Jak2Biotech/.venv/lib/python3.10/site-packages/catboost/core.py", line 5128, in fit
    self._fit(X, y, cat_features, text_features, embedding_features, None, sample_weight, None, None, None, None, baseline, use_best_model,
  File "/Users/tanmaysharma/projects/Jak2Biotech/.venv/lib/python3.10/site-packages/catboost/core.py", line 2339, in _fit
    train_params = self._prepare_train_params(
  File "/Users/tanmaysharma/projects/Jak2Biotech/.venv/lib/python3.10/site-packages/catboost/core.py", line 2266, in _prepare_train_params
    _check_train_params(params)
  File "_catboost.pyx", line 6080, in _catboost._check_train_params
  File "_catboost.pyx", line 6099, in _catboost._check_train_params
_catboost.CatBoostError: catboost/private/libs/options/boosting_options.cpp:79: Learning rate should be non-zero


2023-10-30 23:13:35,989:WARNING:One or more of the test scores are non-finite: [0.89939752 0.77477976 0.82695506 0.81329758 0.74720467        nan
 0.91576694 0.84336278 0.65642895 0.8932355 ]

2023-10-30 23:13:36,108:INFO:best_params: {'actual_estimator__random_strength': 0.3, 'actual_estimator__n_estimators': 240, 'actual_estimator__l2_leaf_reg': 100, 'actual_estimator__eta': 0.3, 'actual_estimator__depth': 9}
2023-10-30 23:13:36,115:INFO:Hyperparameter search completed
2023-10-30 23:13:36,116:INFO:SubProcess create_model() called ==================================
2023-10-30 23:13:36,119:INFO:Initializing create_model()
2023-10-30 23:13:36,119:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x2a279fcd0>, estimator=<catboost.core.CatBoostClassifier object at 0x148bbaf20>, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x1489615a0>, model_only=True, return_train_score=False, kwargs={'random_strength': 0.3, 'n_estimators': 240, 'l2_leaf_reg': 100, 'eta': 0.3, 'depth': 9})
2023-10-30 23:13:36,120:INFO:Checking exceptions
2023-10-30 23:13:36,121:INFO:Importing libraries
2023-10-30 23:13:36,121:INFO:Copying training dataset
2023-10-30 23:13:36,178:INFO:Defining folds
2023-10-30 23:13:36,179:INFO:Declaring metric variables
2023-10-30 23:13:36,186:INFO:Importing untrained model
2023-10-30 23:13:36,186:INFO:Declaring custom model
2023-10-30 23:13:36,193:INFO:CatBoost Classifier Imported successfully
2023-10-30 23:13:36,197:INFO:Starting cross validation
2023-10-30 23:13:36,636:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-10-30 23:17:00,641:INFO:[LightGBM] [Info] Number of positive: 2940, number of negative: 3113
2023-10-30 23:17:01,264:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.624665 seconds.
2023-10-30 23:17:01,265:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-30 23:17:01,273:INFO:[LightGBM] [Info] Total Bins 1291830
2023-10-30 23:17:01,499:INFO:[LightGBM] [Info] Number of data points in the train set: 6053, number of used features: 5066
2023-10-30 23:17:01,518:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.485710 -> initscore=-0.057177
2023-10-30 23:17:01,518:INFO:[LightGBM] [Info] Start training from score -0.057177
2023-10-30 23:18:13,181:INFO:Calculating mean and std
2023-10-30 23:18:13,184:INFO:Creating metrics dataframe
2023-10-30 23:18:13,197:INFO:Finalizing model
2023-10-30 23:27:14,740:INFO:[LightGBM] [Info] Number of positive: 3312, number of negative: 3414
2023-10-30 23:27:15,005:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.262492 seconds.
2023-10-30 23:27:15,005:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-30 23:27:15,012:INFO:[LightGBM] [Info] Total Bins 1409640
2023-10-30 23:27:15,063:INFO:[LightGBM] [Info] Number of data points in the train set: 6726, number of used features: 5528
2023-10-30 23:27:15,069:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.492417 -> initscore=-0.030332
2023-10-30 23:27:15,069:INFO:[LightGBM] [Info] Start training from score -0.030332
2023-10-30 23:27:51,763:INFO:Uploading results into container
2023-10-30 23:27:51,763:INFO:Uploading model into container now
2023-10-30 23:27:51,764:INFO:_master_model_container: 22
2023-10-30 23:27:51,764:INFO:_display_container: 9
2023-10-30 23:27:51,764:INFO:<catboost.core.CatBoostClassifier object at 0x148b720e0>
2023-10-30 23:27:51,764:INFO:create_model() successfully completed......................................
2023-10-30 23:27:52,090:INFO:SubProcess create_model() end ==================================
2023-10-30 23:27:52,090:INFO:choose_better activated
2023-10-30 23:27:52,092:INFO:SubProcess create_model() called ==================================
2023-10-30 23:27:52,092:INFO:Initializing create_model()
2023-10-30 23:27:52,092:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x2a279fcd0>, estimator=<catboost.core.CatBoostClassifier object at 0x148c4ac50>, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-10-30 23:27:52,092:INFO:Checking exceptions
2023-10-30 23:27:52,095:INFO:Importing libraries
2023-10-30 23:27:52,095:INFO:Copying training dataset
2023-10-30 23:27:52,108:INFO:Defining folds
2023-10-30 23:27:52,108:INFO:Declaring metric variables
2023-10-30 23:27:52,108:INFO:Importing untrained model
2023-10-30 23:27:52,108:INFO:Declaring custom model
2023-10-30 23:27:52,109:INFO:CatBoost Classifier Imported successfully
2023-10-30 23:27:52,109:INFO:Starting cross validation
2023-10-30 23:27:52,171:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-10-30 23:29:19,652:INFO:[LightGBM] [Info] Number of positive: 2952, number of negative: 3101
2023-10-30 23:29:20,140:INFO:[LightGBM] [Info] Number of positive: 2929, number of negative: 3124
2023-10-30 23:29:20,302:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.648257 seconds.
2023-10-30 23:29:20,302:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-30 23:29:20,312:INFO:[LightGBM] [Info] Total Bins 1286730
2023-10-30 23:29:20,463:INFO:[LightGBM] [Info] Number of data points in the train set: 6053, number of used features: 5046
2023-10-30 23:29:20,472:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.487692 -> initscore=-0.049242
2023-10-30 23:29:20,472:INFO:[LightGBM] [Info] Start training from score -0.049242
2023-10-30 23:29:20,671:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.530723 seconds.
2023-10-30 23:29:20,671:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-30 23:29:20,687:INFO:[LightGBM] [Info] Total Bins 1291830
2023-10-30 23:29:20,724:INFO:[LightGBM] [Info] Number of positive: 2975, number of negative: 3077
2023-10-30 23:29:20,867:INFO:[LightGBM] [Info] Number of data points in the train set: 6053, number of used features: 5066
2023-10-30 23:29:20,880:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.483892 -> initscore=-0.064453
2023-10-30 23:29:20,880:INFO:[LightGBM] [Info] Start training from score -0.064453
2023-10-30 23:29:20,897:INFO:[LightGBM] [Info] Number of positive: 2956, number of negative: 3097
2023-10-30 23:29:20,924:INFO:[LightGBM] [Info] Number of positive: 2954, number of negative: 3099
2023-10-30 23:29:21,368:INFO:[LightGBM] [Info] Number of positive: 2942, number of negative: 3110
2023-10-30 23:29:21,477:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.747193 seconds.
2023-10-30 23:29:21,477:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-30 23:29:21,485:INFO:[LightGBM] [Info] Total Bins 1289280
2023-10-30 23:29:21,492:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.564459 seconds.
2023-10-30 23:29:21,493:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-30 23:29:21,493:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.592793 seconds.
2023-10-30 23:29:21,493:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-30 23:29:21,499:INFO:[LightGBM] [Info] Total Bins 1290810
2023-10-30 23:29:21,502:INFO:[LightGBM] [Info] Total Bins 1289790
2023-10-30 23:29:21,627:INFO:[LightGBM] [Info] Number of positive: 2921, number of negative: 3132
2023-10-30 23:29:21,904:INFO:[LightGBM] [Info] Number of data points in the train set: 6052, number of used features: 5056
2023-10-30 23:29:21,926:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.491573 -> initscore=-0.033711
2023-10-30 23:29:21,926:INFO:[LightGBM] [Info] Start training from score -0.033711
2023-10-30 23:29:21,937:INFO:[LightGBM] [Info] Number of data points in the train set: 6053, number of used features: 5058
2023-10-30 23:29:21,943:INFO:[LightGBM] [Info] Number of data points in the train set: 6053, number of used features: 5062
2023-10-30 23:29:21,954:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.488353 -> initscore=-0.046597
2023-10-30 23:29:21,954:INFO:[LightGBM] [Info] Start training from score -0.046597
2023-10-30 23:29:21,954:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.488022 -> initscore=-0.047919
2023-10-30 23:29:21,954:INFO:[LightGBM] [Info] Start training from score -0.047919
2023-10-30 23:29:22,316:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.678141 seconds.
2023-10-30 23:29:22,317:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-30 23:29:22,317:INFO:[LightGBM] [Info] Total Bins 1294890
2023-10-30 23:29:22,376:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 1.001340 seconds.
2023-10-30 23:29:22,376:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-30 23:29:22,386:INFO:[LightGBM] [Info] Total Bins 1294635
2023-10-30 23:29:22,629:INFO:[LightGBM] [Info] Number of data points in the train set: 6053, number of used features: 5078
2023-10-30 23:29:22,651:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.482571 -> initscore=-0.069746
2023-10-30 23:29:22,651:INFO:[LightGBM] [Info] Start training from score -0.069746
2023-10-30 23:29:22,690:INFO:[LightGBM] [Info] Number of data points in the train set: 6052, number of used features: 5077
2023-10-30 23:29:22,709:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.486120 -> initscore=-0.055533
2023-10-30 23:29:22,711:INFO:[LightGBM] [Info] Start training from score -0.055533
2023-10-30 23:29:22,848:INFO:[LightGBM] [Info] Number of positive: 2944, number of negative: 3109
2023-10-30 23:29:23,430:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.571352 seconds.
2023-10-30 23:29:23,430:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-30 23:29:23,446:INFO:[LightGBM] [Info] Total Bins 1292085
2023-10-30 23:29:23,674:INFO:[LightGBM] [Info] Number of data points in the train set: 6053, number of used features: 5067
2023-10-30 23:29:23,695:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.486370 -> initscore=-0.054532
2023-10-30 23:29:23,695:INFO:[LightGBM] [Info] Start training from score -0.054532
2023-10-30 23:29:24,068:INFO:[LightGBM] [Info] Number of positive: 2946, number of negative: 3107
2023-10-30 23:29:24,938:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.847197 seconds.
2023-10-30 23:29:24,942:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-30 23:29:24,947:INFO:[LightGBM] [Info] Total Bins 1294890
2023-10-30 23:29:25,168:INFO:[LightGBM] [Info] Number of data points in the train set: 6053, number of used features: 5078
2023-10-30 23:29:25,178:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.486701 -> initscore=-0.053209
2023-10-30 23:29:25,178:INFO:[LightGBM] [Info] Start training from score -0.053209
2023-10-30 23:31:16,093:INFO:[LightGBM] [Info] Number of positive: 2940, number of negative: 3113
2023-10-30 23:31:16,770:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.668638 seconds.
2023-10-30 23:31:16,771:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-30 23:31:16,780:INFO:[LightGBM] [Info] Total Bins 1291830
2023-10-30 23:31:17,029:INFO:[LightGBM] [Info] Number of data points in the train set: 6053, number of used features: 5066
2023-10-30 23:31:17,046:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.485710 -> initscore=-0.057177
2023-10-30 23:31:17,046:INFO:[LightGBM] [Info] Start training from score -0.057177
2023-10-30 23:32:33,692:INFO:Calculating mean and std
2023-10-30 23:32:33,693:INFO:Creating metrics dataframe
2023-10-30 23:32:33,698:INFO:Finalizing model
2023-10-30 23:41:36,605:INFO:[LightGBM] [Info] Number of positive: 3312, number of negative: 3414
2023-10-30 23:41:36,860:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.251547 seconds.
2023-10-30 23:41:36,860:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-30 23:41:36,867:INFO:[LightGBM] [Info] Total Bins 1409640
2023-10-30 23:41:36,918:INFO:[LightGBM] [Info] Number of data points in the train set: 6726, number of used features: 5528
2023-10-30 23:41:36,924:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.492417 -> initscore=-0.030332
2023-10-30 23:41:36,924:INFO:[LightGBM] [Info] Start training from score -0.030332
2023-10-30 23:42:13,685:INFO:Uploading results into container
2023-10-30 23:42:13,686:INFO:Uploading model into container now
2023-10-30 23:42:13,686:INFO:_master_model_container: 23
2023-10-30 23:42:13,686:INFO:_display_container: 10
2023-10-30 23:42:13,686:INFO:<catboost.core.CatBoostClassifier object at 0x2b2526e60>
2023-10-30 23:42:13,686:INFO:create_model() successfully completed......................................
2023-10-30 23:42:13,795:INFO:SubProcess create_model() end ==================================
2023-10-30 23:42:13,795:INFO:<catboost.core.CatBoostClassifier object at 0x2b2526e60> result for AUC is 0.9169
2023-10-30 23:42:13,796:INFO:<catboost.core.CatBoostClassifier object at 0x148b720e0> result for AUC is 0.9158
2023-10-30 23:42:13,796:INFO:<catboost.core.CatBoostClassifier object at 0x2b2526e60> is best model
2023-10-30 23:42:13,796:INFO:choose_better completed
2023-10-30 23:42:13,796:INFO:Original model was better than the tuned model, hence it will be returned. NOTE: The display metrics are for the tuned model (not the original one).
2023-10-30 23:42:13,803:INFO:_master_model_container: 23
2023-10-30 23:42:13,803:INFO:_display_container: 9
2023-10-30 23:42:13,803:INFO:<catboost.core.CatBoostClassifier object at 0x2b2526e60>
2023-10-30 23:42:13,803:INFO:tune_model() successfully completed......................................
2023-10-30 23:42:13,890:INFO:Initializing tune_model()
2023-10-30 23:42:13,890:INFO:tune_model(estimator=ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     n_estimators=100, n_jobs=-1, oob_score=False,
                     random_state=1784, verbose=0, warm_start=False), fold=None, round=4, n_iter=10, custom_grid=None, optimize=AUC, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={}, self=<pycaret.classification.oop.ClassificationExperiment object at 0x2a279fcd0>)
2023-10-30 23:42:13,890:INFO:Checking exceptions
2023-10-30 23:42:13,904:INFO:Copying training dataset
2023-10-30 23:42:13,911:INFO:Checking base model
2023-10-30 23:42:13,911:INFO:Base model : Extra Trees Classifier
2023-10-30 23:42:13,913:INFO:Declaring metric variables
2023-10-30 23:42:13,914:INFO:Defining Hyperparameters
2023-10-30 23:42:14,067:INFO:Tuning with n_jobs=-1
2023-10-30 23:42:14,067:INFO:Initializing RandomizedSearchCV
2023-10-30 23:54:17,617:WARNING:A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.

2023-10-31 00:18:32,245:INFO:best_params: {'actual_estimator__n_estimators': 110, 'actual_estimator__min_samples_split': 2, 'actual_estimator__min_samples_leaf': 5, 'actual_estimator__min_impurity_decrease': 0.001, 'actual_estimator__max_features': 'log2', 'actual_estimator__max_depth': 10, 'actual_estimator__criterion': 'gini', 'actual_estimator__class_weight': 'balanced_subsample', 'actual_estimator__bootstrap': False}
2023-10-31 00:18:32,259:INFO:Hyperparameter search completed
2023-10-31 00:18:32,260:INFO:SubProcess create_model() called ==================================
2023-10-31 00:18:32,266:INFO:Initializing create_model()
2023-10-31 00:18:32,266:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x2a279fcd0>, estimator=ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     n_estimators=100, n_jobs=-1, oob_score=False,
                     random_state=1784, verbose=0, warm_start=False), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x148861960>, model_only=True, return_train_score=False, kwargs={'n_estimators': 110, 'min_samples_split': 2, 'min_samples_leaf': 5, 'min_impurity_decrease': 0.001, 'max_features': 'log2', 'max_depth': 10, 'criterion': 'gini', 'class_weight': 'balanced_subsample', 'bootstrap': False})
2023-10-31 00:18:32,266:INFO:Checking exceptions
2023-10-31 00:18:32,267:INFO:Importing libraries
2023-10-31 00:18:32,268:INFO:Copying training dataset
2023-10-31 00:18:32,322:INFO:Defining folds
2023-10-31 00:18:32,323:INFO:Declaring metric variables
2023-10-31 00:18:32,343:INFO:Importing untrained model
2023-10-31 00:18:32,343:INFO:Declaring custom model
2023-10-31 00:18:32,349:INFO:Extra Trees Classifier Imported successfully
2023-10-31 00:18:32,354:INFO:Starting cross validation
2023-10-31 00:18:32,801:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-10-31 00:22:03,248:INFO:[LightGBM] [Info] Number of positive: 2940, number of negative: 3113
2023-10-31 00:22:03,738:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.492784 seconds.
2023-10-31 00:22:03,740:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 00:22:03,750:INFO:[LightGBM] [Info] Total Bins 1291830
2023-10-31 00:22:03,971:INFO:[LightGBM] [Info] Number of data points in the train set: 6053, number of used features: 5066
2023-10-31 00:22:03,987:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.485710 -> initscore=-0.057177
2023-10-31 00:22:03,987:INFO:[LightGBM] [Info] Start training from score -0.057177
2023-10-31 00:23:01,735:INFO:Calculating mean and std
2023-10-31 00:23:01,739:INFO:Creating metrics dataframe
2023-10-31 00:23:01,750:INFO:Finalizing model
2023-10-31 00:31:52,575:INFO:[LightGBM] [Info] Number of positive: 3312, number of negative: 3414
2023-10-31 00:31:52,849:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.272273 seconds.
2023-10-31 00:31:52,849:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 00:31:52,857:INFO:[LightGBM] [Info] Total Bins 1409640
2023-10-31 00:31:52,915:INFO:[LightGBM] [Info] Number of data points in the train set: 6726, number of used features: 5528
2023-10-31 00:31:52,923:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.492417 -> initscore=-0.030332
2023-10-31 00:31:52,923:INFO:[LightGBM] [Info] Start training from score -0.030332
2023-10-31 00:32:26,855:INFO:Uploading results into container
2023-10-31 00:32:26,856:INFO:Uploading model into container now
2023-10-31 00:32:26,857:INFO:_master_model_container: 24
2023-10-31 00:32:26,857:INFO:_display_container: 10
2023-10-31 00:32:26,857:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0,
                     class_weight='balanced_subsample', criterion='gini',
                     max_depth=10, max_features='log2', max_leaf_nodes=None,
                     max_samples=None, min_impurity_decrease=0.001,
                     min_samples_leaf=5, min_samples_split=2,
                     min_weight_fraction_leaf=0.0, n_estimators=110, n_jobs=-1,
                     oob_score=False, random_state=1784, verbose=0,
                     warm_start=False)
2023-10-31 00:32:26,857:INFO:create_model() successfully completed......................................
2023-10-31 00:32:27,326:INFO:SubProcess create_model() end ==================================
2023-10-31 00:32:27,326:INFO:choose_better activated
2023-10-31 00:32:27,328:INFO:SubProcess create_model() called ==================================
2023-10-31 00:32:27,328:INFO:Initializing create_model()
2023-10-31 00:32:27,328:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x2a279fcd0>, estimator=ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     n_estimators=100, n_jobs=-1, oob_score=False,
                     random_state=1784, verbose=0, warm_start=False), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-10-31 00:32:27,328:INFO:Checking exceptions
2023-10-31 00:32:27,329:INFO:Importing libraries
2023-10-31 00:32:27,329:INFO:Copying training dataset
2023-10-31 00:32:27,342:INFO:Defining folds
2023-10-31 00:32:27,342:INFO:Declaring metric variables
2023-10-31 00:32:27,342:INFO:Importing untrained model
2023-10-31 00:32:27,343:INFO:Declaring custom model
2023-10-31 00:32:27,343:INFO:Extra Trees Classifier Imported successfully
2023-10-31 00:32:27,343:INFO:Starting cross validation
2023-10-31 00:32:27,410:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-10-31 00:33:55,132:INFO:[LightGBM] [Info] Number of positive: 2954, number of negative: 3099
2023-10-31 00:33:55,211:INFO:[LightGBM] [Info] Number of positive: 2975, number of negative: 3077
2023-10-31 00:33:55,828:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.694927 seconds.
2023-10-31 00:33:55,829:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 00:33:55,844:INFO:[LightGBM] [Info] Total Bins 1290810
2023-10-31 00:33:55,858:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.645752 seconds.
2023-10-31 00:33:55,858:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 00:33:55,871:INFO:[LightGBM] [Info] Total Bins 1289280
2023-10-31 00:33:56,185:INFO:[LightGBM] [Info] Number of data points in the train set: 6053, number of used features: 5062
2023-10-31 00:33:56,195:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.488022 -> initscore=-0.047919
2023-10-31 00:33:56,195:INFO:[LightGBM] [Info] Start training from score -0.047919
2023-10-31 00:33:56,235:INFO:[LightGBM] [Info] Number of data points in the train set: 6052, number of used features: 5056
2023-10-31 00:33:56,250:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.491573 -> initscore=-0.033711
2023-10-31 00:33:56,252:INFO:[LightGBM] [Info] Start training from score -0.033711
2023-10-31 00:33:56,403:INFO:[LightGBM] [Info] Number of positive: 2929, number of negative: 3124
2023-10-31 00:33:57,091:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.683723 seconds.
2023-10-31 00:33:57,091:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 00:33:57,096:INFO:[LightGBM] [Info] Total Bins 1291830
2023-10-31 00:33:57,296:INFO:[LightGBM] [Info] Number of data points in the train set: 6053, number of used features: 5066
2023-10-31 00:33:57,302:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.483892 -> initscore=-0.064453
2023-10-31 00:33:57,302:INFO:[LightGBM] [Info] Start training from score -0.064453
2023-10-31 00:33:57,422:INFO:[LightGBM] [Info] Number of positive: 2956, number of negative: 3097
2023-10-31 00:33:57,490:INFO:[LightGBM] [Info] Number of positive: 2944, number of negative: 3109
2023-10-31 00:33:57,535:INFO:[LightGBM] [Info] Number of positive: 2921, number of negative: 3132
2023-10-31 00:33:57,633:INFO:[LightGBM] [Info] Number of positive: 2952, number of negative: 3101
2023-10-31 00:33:58,113:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.620026 seconds.
2023-10-31 00:33:58,114:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 00:33:58,121:INFO:[LightGBM] [Info] Total Bins 1292085
2023-10-31 00:33:58,293:INFO:[LightGBM] [Info] Number of data points in the train set: 6053, number of used features: 5067
2023-10-31 00:33:58,308:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.486370 -> initscore=-0.054532
2023-10-31 00:33:58,308:INFO:[LightGBM] [Info] Start training from score -0.054532
2023-10-31 00:33:58,313:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.677702 seconds.
2023-10-31 00:33:58,313:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 00:33:58,324:INFO:[LightGBM] [Info] Total Bins 1286730
2023-10-31 00:33:58,373:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.944806 seconds.
2023-10-31 00:33:58,373:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 00:33:58,388:INFO:[LightGBM] [Info] Total Bins 1289790
2023-10-31 00:33:58,445:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.905850 seconds.
2023-10-31 00:33:58,446:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 00:33:58,461:INFO:[LightGBM] [Info] Total Bins 1294890
2023-10-31 00:33:58,606:INFO:[LightGBM] [Info] Number of data points in the train set: 6053, number of used features: 5046
2023-10-31 00:33:58,631:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.487692 -> initscore=-0.049242
2023-10-31 00:33:58,632:INFO:[LightGBM] [Info] Start training from score -0.049242
2023-10-31 00:33:58,674:INFO:[LightGBM] [Info] Number of data points in the train set: 6053, number of used features: 5058
2023-10-31 00:33:58,705:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.488353 -> initscore=-0.046597
2023-10-31 00:33:58,705:INFO:[LightGBM] [Info] Start training from score -0.046597
2023-10-31 00:33:58,740:INFO:[LightGBM] [Info] Number of data points in the train set: 6053, number of used features: 5078
2023-10-31 00:33:58,760:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.482571 -> initscore=-0.069746
2023-10-31 00:33:58,760:INFO:[LightGBM] [Info] Start training from score -0.069746
2023-10-31 00:33:59,762:INFO:[LightGBM] [Info] Number of positive: 2942, number of negative: 3110
2023-10-31 00:33:59,998:INFO:[LightGBM] [Info] Number of positive: 2946, number of negative: 3107
2023-10-31 00:34:00,359:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.585658 seconds.
2023-10-31 00:34:00,359:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 00:34:00,360:INFO:[LightGBM] [Info] Total Bins 1294635
2023-10-31 00:34:00,501:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.487043 seconds.
2023-10-31 00:34:00,501:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 00:34:00,526:INFO:[LightGBM] [Info] Total Bins 1294890
2023-10-31 00:34:00,645:INFO:[LightGBM] [Info] Number of data points in the train set: 6052, number of used features: 5077
2023-10-31 00:34:00,675:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.486120 -> initscore=-0.055533
2023-10-31 00:34:00,675:INFO:[LightGBM] [Info] Start training from score -0.055533
2023-10-31 00:34:00,737:INFO:[LightGBM] [Info] Number of data points in the train set: 6053, number of used features: 5078
2023-10-31 00:34:00,752:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.486701 -> initscore=-0.053209
2023-10-31 00:34:00,752:INFO:[LightGBM] [Info] Start training from score -0.053209
2023-10-31 00:35:56,660:INFO:[LightGBM] [Info] Number of positive: 2940, number of negative: 3113
2023-10-31 00:35:57,271:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.597477 seconds.
2023-10-31 00:35:57,273:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 00:35:57,279:INFO:[LightGBM] [Info] Total Bins 1291830
2023-10-31 00:35:57,516:INFO:[LightGBM] [Info] Number of data points in the train set: 6053, number of used features: 5066
2023-10-31 00:35:57,530:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.485710 -> initscore=-0.057177
2023-10-31 00:35:57,531:INFO:[LightGBM] [Info] Start training from score -0.057177
2023-10-31 00:37:02,043:INFO:Calculating mean and std
2023-10-31 00:37:02,044:INFO:Creating metrics dataframe
2023-10-31 00:37:02,050:INFO:Finalizing model
2023-10-31 00:46:02,345:INFO:[LightGBM] [Info] Number of positive: 3312, number of negative: 3414
2023-10-31 00:46:02,604:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.257126 seconds.
2023-10-31 00:46:02,605:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 00:46:02,612:INFO:[LightGBM] [Info] Total Bins 1409640
2023-10-31 00:46:02,663:INFO:[LightGBM] [Info] Number of data points in the train set: 6726, number of used features: 5528
2023-10-31 00:46:02,669:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.492417 -> initscore=-0.030332
2023-10-31 00:46:02,669:INFO:[LightGBM] [Info] Start training from score -0.030332
2023-10-31 00:46:36,533:INFO:Uploading results into container
2023-10-31 00:46:36,534:INFO:Uploading model into container now
2023-10-31 00:46:36,534:INFO:_master_model_container: 25
2023-10-31 00:46:36,534:INFO:_display_container: 11
2023-10-31 00:46:36,534:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     n_estimators=100, n_jobs=-1, oob_score=False,
                     random_state=1784, verbose=0, warm_start=False)
2023-10-31 00:46:36,534:INFO:create_model() successfully completed......................................
2023-10-31 00:46:36,646:INFO:SubProcess create_model() end ==================================
2023-10-31 00:46:36,646:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     n_estimators=100, n_jobs=-1, oob_score=False,
                     random_state=1784, verbose=0, warm_start=False) result for AUC is 0.901
2023-10-31 00:46:36,646:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0,
                     class_weight='balanced_subsample', criterion='gini',
                     max_depth=10, max_features='log2', max_leaf_nodes=None,
                     max_samples=None, min_impurity_decrease=0.001,
                     min_samples_leaf=5, min_samples_split=2,
                     min_weight_fraction_leaf=0.0, n_estimators=110, n_jobs=-1,
                     oob_score=False, random_state=1784, verbose=0,
                     warm_start=False) result for AUC is 0.8515
2023-10-31 00:46:36,646:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     n_estimators=100, n_jobs=-1, oob_score=False,
                     random_state=1784, verbose=0, warm_start=False) is best model
2023-10-31 00:46:36,646:INFO:choose_better completed
2023-10-31 00:46:36,647:INFO:Original model was better than the tuned model, hence it will be returned. NOTE: The display metrics are for the tuned model (not the original one).
2023-10-31 00:46:36,654:INFO:_master_model_container: 25
2023-10-31 00:46:36,654:INFO:_display_container: 10
2023-10-31 00:46:36,654:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     n_estimators=100, n_jobs=-1, oob_score=False,
                     random_state=1784, verbose=0, warm_start=False)
2023-10-31 00:46:36,654:INFO:tune_model() successfully completed......................................
2023-10-31 00:46:36,738:INFO:Initializing tune_model()
2023-10-31 00:46:36,738:INFO:tune_model(estimator=QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001), fold=None, round=4, n_iter=10, custom_grid=None, optimize=AUC, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={}, self=<pycaret.classification.oop.ClassificationExperiment object at 0x2a279fcd0>)
2023-10-31 00:46:36,738:INFO:Checking exceptions
2023-10-31 00:46:36,751:INFO:Copying training dataset
2023-10-31 00:46:36,758:INFO:Checking base model
2023-10-31 00:46:36,758:INFO:Base model : Quadratic Discriminant Analysis
2023-10-31 00:46:36,760:INFO:Declaring metric variables
2023-10-31 00:46:36,761:INFO:Defining Hyperparameters
2023-10-31 00:46:36,943:INFO:Tuning with n_jobs=-1
2023-10-31 00:46:36,943:INFO:Initializing RandomizedSearchCV
2023-10-31 00:54:12,510:WARNING:A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.

2023-10-31 01:24:49,988:INFO:best_params: {'actual_estimator__reg_param': 0.03}
2023-10-31 01:24:50,003:INFO:Hyperparameter search completed
2023-10-31 01:24:50,003:INFO:SubProcess create_model() called ==================================
2023-10-31 01:24:50,010:INFO:Initializing create_model()
2023-10-31 01:24:50,010:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x2a279fcd0>, estimator=QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x165773eb0>, model_only=True, return_train_score=False, kwargs={'reg_param': 0.03})
2023-10-31 01:24:50,010:INFO:Checking exceptions
2023-10-31 01:24:50,011:INFO:Importing libraries
2023-10-31 01:24:50,012:INFO:Copying training dataset
2023-10-31 01:24:50,060:INFO:Defining folds
2023-10-31 01:24:50,060:INFO:Declaring metric variables
2023-10-31 01:24:50,079:INFO:Importing untrained model
2023-10-31 01:24:50,079:INFO:Declaring custom model
2023-10-31 01:24:50,084:INFO:Quadratic Discriminant Analysis Imported successfully
2023-10-31 01:24:50,087:INFO:Starting cross validation
2023-10-31 01:24:50,540:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-10-31 01:29:28,138:INFO:Calculating mean and std
2023-10-31 01:29:28,143:INFO:Creating metrics dataframe
2023-10-31 01:29:28,157:INFO:Finalizing model
2023-10-31 01:38:45,565:INFO:[LightGBM] [Info] Number of positive: 3312, number of negative: 3414
2023-10-31 01:38:45,830:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.262636 seconds.
2023-10-31 01:38:45,830:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 01:38:45,837:INFO:[LightGBM] [Info] Total Bins 1409640
2023-10-31 01:38:45,891:INFO:[LightGBM] [Info] Number of data points in the train set: 6726, number of used features: 5528
2023-10-31 01:38:45,897:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.492417 -> initscore=-0.030332
2023-10-31 01:38:45,897:INFO:[LightGBM] [Info] Start training from score -0.030332
2023-10-31 01:39:20,028:INFO:Uploading results into container
2023-10-31 01:39:20,028:INFO:Uploading model into container now
2023-10-31 01:39:20,031:INFO:_master_model_container: 26
2023-10-31 01:39:20,031:INFO:_display_container: 11
2023-10-31 01:39:20,031:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.03,
                              store_covariance=False, tol=0.0001)
2023-10-31 01:39:20,031:INFO:create_model() successfully completed......................................
2023-10-31 01:39:20,360:INFO:SubProcess create_model() end ==================================
2023-10-31 01:39:20,360:INFO:choose_better activated
2023-10-31 01:39:20,362:INFO:SubProcess create_model() called ==================================
2023-10-31 01:39:20,362:INFO:Initializing create_model()
2023-10-31 01:39:20,362:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x2a279fcd0>, estimator=QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-10-31 01:39:20,362:INFO:Checking exceptions
2023-10-31 01:39:20,366:INFO:Importing libraries
2023-10-31 01:39:20,366:INFO:Copying training dataset
2023-10-31 01:39:20,380:INFO:Defining folds
2023-10-31 01:39:20,380:INFO:Declaring metric variables
2023-10-31 01:39:20,380:INFO:Importing untrained model
2023-10-31 01:39:20,380:INFO:Declaring custom model
2023-10-31 01:39:20,381:INFO:Quadratic Discriminant Analysis Imported successfully
2023-10-31 01:39:20,381:INFO:Starting cross validation
2023-10-31 01:39:20,457:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-10-31 01:40:40,976:INFO:[LightGBM] [Info] Number of positive: 2975, number of negative: 3077
2023-10-31 01:40:41,588:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.610838 seconds.
2023-10-31 01:40:41,588:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 01:40:41,595:INFO:[LightGBM] [Info] Total Bins 1289280
2023-10-31 01:40:41,803:INFO:[LightGBM] [Info] Number of data points in the train set: 6052, number of used features: 5056
2023-10-31 01:40:41,811:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.491573 -> initscore=-0.033711
2023-10-31 01:40:41,812:INFO:[LightGBM] [Info] Start training from score -0.033711
2023-10-31 01:40:44,758:INFO:[LightGBM] [Info] Number of positive: 2942, number of negative: 3110
2023-10-31 01:40:44,891:INFO:[LightGBM] [Info] Number of positive: 2952, number of negative: 3101
2023-10-31 01:40:45,141:INFO:[LightGBM] [Info] Number of positive: 2956, number of negative: 3097
2023-10-31 01:40:45,452:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.691848 seconds.
2023-10-31 01:40:45,452:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 01:40:45,459:INFO:[LightGBM] [Info] Total Bins 1294635
2023-10-31 01:40:45,590:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.687500 seconds.
2023-10-31 01:40:45,590:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 01:40:45,599:INFO:[LightGBM] [Info] Total Bins 1286730
2023-10-31 01:40:45,676:INFO:[LightGBM] [Info] Number of data points in the train set: 6052, number of used features: 5077
2023-10-31 01:40:45,685:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.486120 -> initscore=-0.055533
2023-10-31 01:40:45,685:INFO:[LightGBM] [Info] Start training from score -0.055533
2023-10-31 01:40:45,779:INFO:[LightGBM] [Info] Number of data points in the train set: 6053, number of used features: 5046
2023-10-31 01:40:45,785:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.487692 -> initscore=-0.049242
2023-10-31 01:40:45,785:INFO:[LightGBM] [Info] Start training from score -0.049242
2023-10-31 01:40:45,902:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.744659 seconds.
2023-10-31 01:40:45,902:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 01:40:45,910:INFO:[LightGBM] [Info] Total Bins 1289790
2023-10-31 01:40:46,081:INFO:[LightGBM] [Info] Number of data points in the train set: 6053, number of used features: 5058
2023-10-31 01:40:46,090:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.488353 -> initscore=-0.046597
2023-10-31 01:40:46,090:INFO:[LightGBM] [Info] Start training from score -0.046597
2023-10-31 01:40:46,617:INFO:[LightGBM] [Info] Number of positive: 2946, number of negative: 3107
2023-10-31 01:40:47,328:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.708365 seconds.
2023-10-31 01:40:47,328:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 01:40:47,336:INFO:[LightGBM] [Info] Total Bins 1294890
2023-10-31 01:40:47,579:INFO:[LightGBM] [Info] Number of data points in the train set: 6053, number of used features: 5078
2023-10-31 01:40:47,593:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.486701 -> initscore=-0.053209
2023-10-31 01:40:47,597:INFO:[LightGBM] [Info] Start training from score -0.053209
2023-10-31 01:40:47,800:INFO:[LightGBM] [Info] Number of positive: 2921, number of negative: 3132
2023-10-31 01:40:48,487:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.678295 seconds.
2023-10-31 01:40:48,488:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 01:40:48,494:INFO:[LightGBM] [Info] Total Bins 1294890
2023-10-31 01:40:48,589:INFO:[LightGBM] [Info] Number of positive: 2954, number of negative: 3099
2023-10-31 01:40:48,741:INFO:[LightGBM] [Info] Number of data points in the train set: 6053, number of used features: 5078
2023-10-31 01:40:48,755:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.482571 -> initscore=-0.069746
2023-10-31 01:40:48,755:INFO:[LightGBM] [Info] Start training from score -0.069746
2023-10-31 01:40:48,970:INFO:[LightGBM] [Info] Number of positive: 2944, number of negative: 3109
2023-10-31 01:40:49,172:INFO:[LightGBM] [Info] Number of positive: 2929, number of negative: 3124
2023-10-31 01:40:49,461:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.857279 seconds.
2023-10-31 01:40:49,463:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 01:40:49,469:INFO:[LightGBM] [Info] Total Bins 1290810
2023-10-31 01:40:49,572:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.582090 seconds.
2023-10-31 01:40:49,572:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 01:40:49,585:INFO:[LightGBM] [Info] Total Bins 1292085
2023-10-31 01:40:49,789:INFO:[LightGBM] [Info] Number of data points in the train set: 6053, number of used features: 5062
2023-10-31 01:40:49,817:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.488022 -> initscore=-0.047919
2023-10-31 01:40:49,817:INFO:[LightGBM] [Info] Start training from score -0.047919
2023-10-31 01:40:49,850:INFO:[LightGBM] [Info] Number of data points in the train set: 6053, number of used features: 5067
2023-10-31 01:40:49,857:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.486370 -> initscore=-0.054532
2023-10-31 01:40:49,858:INFO:[LightGBM] [Info] Start training from score -0.054532
2023-10-31 01:40:49,972:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.783756 seconds.
2023-10-31 01:40:49,972:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 01:40:49,983:INFO:[LightGBM] [Info] Total Bins 1291830
2023-10-31 01:40:50,210:INFO:[LightGBM] [Info] Number of data points in the train set: 6053, number of used features: 5066
2023-10-31 01:40:50,232:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.483892 -> initscore=-0.064453
2023-10-31 01:40:50,232:INFO:[LightGBM] [Info] Start training from score -0.064453
2023-10-31 01:42:46,484:INFO:[LightGBM] [Info] Number of positive: 2940, number of negative: 3113
2023-10-31 01:42:47,232:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.734776 seconds.
2023-10-31 01:42:47,232:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 01:42:47,242:INFO:[LightGBM] [Info] Total Bins 1291830
2023-10-31 01:42:47,480:INFO:[LightGBM] [Info] Number of data points in the train set: 6053, number of used features: 5066
2023-10-31 01:42:47,502:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.485710 -> initscore=-0.057177
2023-10-31 01:42:47,503:INFO:[LightGBM] [Info] Start training from score -0.057177
2023-10-31 01:43:39,836:INFO:Calculating mean and std
2023-10-31 01:43:39,838:INFO:Creating metrics dataframe
2023-10-31 01:43:39,843:INFO:Finalizing model
2023-10-31 01:53:01,372:INFO:[LightGBM] [Info] Number of positive: 3312, number of negative: 3414
2023-10-31 01:53:01,618:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.242877 seconds.
2023-10-31 01:53:01,618:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 01:53:01,625:INFO:[LightGBM] [Info] Total Bins 1409640
2023-10-31 01:53:01,677:INFO:[LightGBM] [Info] Number of data points in the train set: 6726, number of used features: 5528
2023-10-31 01:53:01,683:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.492417 -> initscore=-0.030332
2023-10-31 01:53:01,683:INFO:[LightGBM] [Info] Start training from score -0.030332
2023-10-31 01:59:53,466:INFO:Uploading results into container
2023-10-31 01:59:53,466:INFO:Uploading model into container now
2023-10-31 01:59:53,466:INFO:_master_model_container: 27
2023-10-31 01:59:53,466:INFO:_display_container: 12
2023-10-31 01:59:53,467:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2023-10-31 01:59:53,467:INFO:create_model() successfully completed......................................
2023-10-31 01:59:53,584:INFO:SubProcess create_model() end ==================================
2023-10-31 01:59:53,584:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001) result for AUC is 0.8442
2023-10-31 01:59:53,584:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.03,
                              store_covariance=False, tol=0.0001) result for AUC is 0.844
2023-10-31 01:59:53,584:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001) is best model
2023-10-31 01:59:53,584:INFO:choose_better completed
2023-10-31 01:59:53,585:INFO:Original model was better than the tuned model, hence it will be returned. NOTE: The display metrics are for the tuned model (not the original one).
2023-10-31 01:59:53,615:INFO:_master_model_container: 27
2023-10-31 01:59:53,616:INFO:_display_container: 11
2023-10-31 01:59:53,616:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2023-10-31 01:59:53,616:INFO:tune_model() successfully completed......................................
2023-10-31 01:59:53,702:INFO:Initializing tune_model()
2023-10-31 01:59:53,702:INFO:tune_model(estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=100, n_jobs=-1, oob_score=False,
                       random_state=1784, verbose=0, warm_start=False), fold=None, round=4, n_iter=10, custom_grid=None, optimize=AUC, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={}, self=<pycaret.classification.oop.ClassificationExperiment object at 0x2a279fcd0>)
2023-10-31 01:59:53,702:INFO:Checking exceptions
2023-10-31 01:59:53,731:INFO:Copying training dataset
2023-10-31 01:59:53,739:INFO:Checking base model
2023-10-31 01:59:53,740:INFO:Base model : Random Forest Classifier
2023-10-31 01:59:53,741:INFO:Declaring metric variables
2023-10-31 01:59:53,743:INFO:Defining Hyperparameters
2023-10-31 01:59:53,939:INFO:Tuning with n_jobs=-1
2023-10-31 01:59:53,940:INFO:Initializing RandomizedSearchCV
2023-10-31 02:29:42,693:WARNING:A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.

2023-10-31 02:45:09,325:INFO:best_params: {'actual_estimator__n_estimators': 110, 'actual_estimator__min_samples_split': 2, 'actual_estimator__min_samples_leaf': 5, 'actual_estimator__min_impurity_decrease': 0.001, 'actual_estimator__max_features': 'log2', 'actual_estimator__max_depth': 10, 'actual_estimator__criterion': 'gini', 'actual_estimator__class_weight': 'balanced_subsample', 'actual_estimator__bootstrap': False}
2023-10-31 02:45:09,340:INFO:Hyperparameter search completed
2023-10-31 02:45:09,341:INFO:SubProcess create_model() called ==================================
2023-10-31 02:45:09,346:INFO:Initializing create_model()
2023-10-31 02:45:09,346:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x2a279fcd0>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=100, n_jobs=-1, oob_score=False,
                       random_state=1784, verbose=0, warm_start=False), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x2b25c8640>, model_only=True, return_train_score=False, kwargs={'n_estimators': 110, 'min_samples_split': 2, 'min_samples_leaf': 5, 'min_impurity_decrease': 0.001, 'max_features': 'log2', 'max_depth': 10, 'criterion': 'gini', 'class_weight': 'balanced_subsample', 'bootstrap': False})
2023-10-31 02:45:09,346:INFO:Checking exceptions
2023-10-31 02:45:09,347:INFO:Importing libraries
2023-10-31 02:45:09,348:INFO:Copying training dataset
2023-10-31 02:45:09,400:INFO:Defining folds
2023-10-31 02:45:09,401:INFO:Declaring metric variables
2023-10-31 02:45:09,418:INFO:Importing untrained model
2023-10-31 02:45:09,418:INFO:Declaring custom model
2023-10-31 02:45:09,424:INFO:Random Forest Classifier Imported successfully
2023-10-31 02:45:09,427:INFO:Starting cross validation
2023-10-31 02:45:09,978:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-10-31 02:48:31,067:INFO:[LightGBM] [Info] Number of positive: 2940, number of negative: 3113
2023-10-31 02:48:31,655:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.584813 seconds.
2023-10-31 02:48:31,657:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 02:48:31,666:INFO:[LightGBM] [Info] Total Bins 1291830
2023-10-31 02:48:31,894:INFO:[LightGBM] [Info] Number of data points in the train set: 6053, number of used features: 5066
2023-10-31 02:48:31,909:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.485710 -> initscore=-0.057177
2023-10-31 02:48:31,909:INFO:[LightGBM] [Info] Start training from score -0.057177
2023-10-31 02:49:32,995:INFO:Calculating mean and std
2023-10-31 02:49:33,000:INFO:Creating metrics dataframe
2023-10-31 02:49:33,022:INFO:Finalizing model
2023-10-31 02:58:42,302:INFO:[LightGBM] [Info] Number of positive: 3312, number of negative: 3414
2023-10-31 02:58:42,622:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.317268 seconds.
2023-10-31 02:58:42,622:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 02:58:42,629:INFO:[LightGBM] [Info] Total Bins 1409640
2023-10-31 02:58:42,687:INFO:[LightGBM] [Info] Number of data points in the train set: 6726, number of used features: 5528
2023-10-31 02:58:42,694:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.492417 -> initscore=-0.030332
2023-10-31 02:58:42,694:INFO:[LightGBM] [Info] Start training from score -0.030332
2023-10-31 02:59:18,240:INFO:Uploading results into container
2023-10-31 02:59:18,241:INFO:Uploading model into container now
2023-10-31 02:59:18,242:INFO:_master_model_container: 28
2023-10-31 02:59:18,242:INFO:_display_container: 12
2023-10-31 02:59:18,242:INFO:RandomForestClassifier(bootstrap=False, ccp_alpha=0.0,
                       class_weight='balanced_subsample', criterion='gini',
                       max_depth=10, max_features='log2', max_leaf_nodes=None,
                       max_samples=None, min_impurity_decrease=0.001,
                       min_samples_leaf=5, min_samples_split=2,
                       min_weight_fraction_leaf=0.0, n_estimators=110,
                       n_jobs=-1, oob_score=False, random_state=1784, verbose=0,
                       warm_start=False)
2023-10-31 02:59:18,242:INFO:create_model() successfully completed......................................
2023-10-31 02:59:18,590:INFO:SubProcess create_model() end ==================================
2023-10-31 02:59:18,590:INFO:choose_better activated
2023-10-31 02:59:18,592:INFO:SubProcess create_model() called ==================================
2023-10-31 02:59:18,592:INFO:Initializing create_model()
2023-10-31 02:59:18,592:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x2a279fcd0>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=100, n_jobs=-1, oob_score=False,
                       random_state=1784, verbose=0, warm_start=False), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-10-31 02:59:18,592:INFO:Checking exceptions
2023-10-31 02:59:18,595:INFO:Importing libraries
2023-10-31 02:59:18,595:INFO:Copying training dataset
2023-10-31 02:59:18,609:INFO:Defining folds
2023-10-31 02:59:18,610:INFO:Declaring metric variables
2023-10-31 02:59:18,610:INFO:Importing untrained model
2023-10-31 02:59:18,610:INFO:Declaring custom model
2023-10-31 02:59:18,610:INFO:Random Forest Classifier Imported successfully
2023-10-31 02:59:18,610:INFO:Starting cross validation
2023-10-31 02:59:18,688:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-10-31 03:00:44,712:INFO:[LightGBM] [Info] Number of positive: 2975, number of negative: 3077
2023-10-31 03:00:44,850:INFO:[LightGBM] [Info] Number of positive: 2929, number of negative: 3124
2023-10-31 03:00:45,069:INFO:[LightGBM] [Info] Number of positive: 2956, number of negative: 3097
2023-10-31 03:00:45,186:INFO:[LightGBM] [Info] Number of positive: 2942, number of negative: 3110
2023-10-31 03:00:45,271:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.556115 seconds.
2023-10-31 03:00:45,272:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 03:00:45,278:INFO:[LightGBM] [Info] Total Bins 1289280
2023-10-31 03:00:45,445:INFO:[LightGBM] [Info] Number of data points in the train set: 6052, number of used features: 5056
2023-10-31 03:00:45,454:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.491573 -> initscore=-0.033711
2023-10-31 03:00:45,454:INFO:[LightGBM] [Info] Start training from score -0.033711
2023-10-31 03:00:45,552:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.480606 seconds.
2023-10-31 03:00:45,552:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 03:00:45,561:INFO:[LightGBM] [Info] Total Bins 1289790
2023-10-31 03:00:45,651:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.795783 seconds.
2023-10-31 03:00:45,652:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 03:00:45,659:INFO:[LightGBM] [Info] Total Bins 1291830
2023-10-31 03:00:45,785:INFO:[LightGBM] [Info] Number of data points in the train set: 6053, number of used features: 5058
2023-10-31 03:00:45,800:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.488353 -> initscore=-0.046597
2023-10-31 03:00:45,800:INFO:[LightGBM] [Info] Start training from score -0.046597
2023-10-31 03:00:45,869:INFO:[LightGBM] [Info] Number of data points in the train set: 6053, number of used features: 5066
2023-10-31 03:00:45,891:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.483892 -> initscore=-0.064453
2023-10-31 03:00:45,892:INFO:[LightGBM] [Info] Start training from score -0.064453
2023-10-31 03:00:45,950:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.741544 seconds.
2023-10-31 03:00:45,951:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 03:00:45,963:INFO:[LightGBM] [Info] Total Bins 1294635
2023-10-31 03:00:46,165:INFO:[LightGBM] [Info] Number of data points in the train set: 6052, number of used features: 5077
2023-10-31 03:00:46,171:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.486120 -> initscore=-0.055533
2023-10-31 03:00:46,172:INFO:[LightGBM] [Info] Start training from score -0.055533
2023-10-31 03:00:46,619:INFO:[LightGBM] [Info] Number of positive: 2921, number of negative: 3132
2023-10-31 03:00:46,716:INFO:[LightGBM] [Info] Number of positive: 2946, number of negative: 3107
2023-10-31 03:00:46,756:INFO:[LightGBM] [Info] Number of positive: 2944, number of negative: 3109
2023-10-31 03:00:47,177:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.553233 seconds.
2023-10-31 03:00:47,178:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 03:00:47,183:INFO:[LightGBM] [Info] Total Bins 1294890
2023-10-31 03:00:47,324:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.604635 seconds.
2023-10-31 03:00:47,324:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 03:00:47,356:INFO:[LightGBM] [Info] Total Bins 1294890
2023-10-31 03:00:47,402:INFO:[LightGBM] [Info] Number of data points in the train set: 6053, number of used features: 5078
2023-10-31 03:00:47,441:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.482571 -> initscore=-0.069746
2023-10-31 03:00:47,441:INFO:[LightGBM] [Info] Start training from score -0.069746
2023-10-31 03:00:47,566:INFO:[LightGBM] [Info] Number of data points in the train set: 6053, number of used features: 5078
2023-10-31 03:00:47,575:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.486701 -> initscore=-0.053209
2023-10-31 03:00:47,576:INFO:[LightGBM] [Info] Start training from score -0.053209
2023-10-31 03:00:47,634:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.873483 seconds.
2023-10-31 03:00:47,634:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 03:00:47,651:INFO:[LightGBM] [Info] Total Bins 1292085
2023-10-31 03:00:47,844:INFO:[LightGBM] [Info] Number of data points in the train set: 6053, number of used features: 5067
2023-10-31 03:00:47,853:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.486370 -> initscore=-0.054532
2023-10-31 03:00:47,853:INFO:[LightGBM] [Info] Start training from score -0.054532
2023-10-31 03:00:50,811:INFO:[LightGBM] [Info] Number of positive: 2952, number of negative: 3101
2023-10-31 03:00:51,503:INFO:[LightGBM] [Info] Number of positive: 2954, number of negative: 3099
2023-10-31 03:00:51,556:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.740997 seconds.
2023-10-31 03:00:51,556:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 03:00:51,568:INFO:[LightGBM] [Info] Total Bins 1286730
2023-10-31 03:00:51,839:INFO:[LightGBM] [Info] Number of data points in the train set: 6053, number of used features: 5046
2023-10-31 03:00:51,859:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.487692 -> initscore=-0.049242
2023-10-31 03:00:51,859:INFO:[LightGBM] [Info] Start training from score -0.049242
2023-10-31 03:00:52,480:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.936747 seconds.
2023-10-31 03:00:52,482:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 03:00:52,493:INFO:[LightGBM] [Info] Total Bins 1290810
2023-10-31 03:00:52,753:INFO:[LightGBM] [Info] Number of data points in the train set: 6053, number of used features: 5062
2023-10-31 03:00:52,774:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.488022 -> initscore=-0.047919
2023-10-31 03:00:52,775:INFO:[LightGBM] [Info] Start training from score -0.047919
2023-10-31 03:02:40,695:INFO:[LightGBM] [Info] Number of positive: 2940, number of negative: 3113
2023-10-31 03:02:41,261:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.555553 seconds.
2023-10-31 03:02:41,262:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 03:02:41,270:INFO:[LightGBM] [Info] Total Bins 1291830
2023-10-31 03:02:41,494:INFO:[LightGBM] [Info] Number of data points in the train set: 6053, number of used features: 5066
2023-10-31 03:02:41,507:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.485710 -> initscore=-0.057177
2023-10-31 03:02:41,507:INFO:[LightGBM] [Info] Start training from score -0.057177
2023-10-31 03:03:56,021:INFO:Calculating mean and std
2023-10-31 03:03:56,024:INFO:Creating metrics dataframe
2023-10-31 03:03:56,054:INFO:Finalizing model
2023-10-31 03:12:45,434:INFO:[LightGBM] [Info] Number of positive: 3312, number of negative: 3414
2023-10-31 03:12:45,696:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.259955 seconds.
2023-10-31 03:12:45,696:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 03:12:45,704:INFO:[LightGBM] [Info] Total Bins 1409640
2023-10-31 03:12:45,755:INFO:[LightGBM] [Info] Number of data points in the train set: 6726, number of used features: 5528
2023-10-31 03:12:45,761:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.492417 -> initscore=-0.030332
2023-10-31 03:12:45,761:INFO:[LightGBM] [Info] Start training from score -0.030332
2023-10-31 03:13:20,778:INFO:Uploading results into container
2023-10-31 03:13:20,779:INFO:Uploading model into container now
2023-10-31 03:13:20,779:INFO:_master_model_container: 29
2023-10-31 03:13:20,779:INFO:_display_container: 13
2023-10-31 03:13:20,780:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=100, n_jobs=-1, oob_score=False,
                       random_state=1784, verbose=0, warm_start=False)
2023-10-31 03:13:20,780:INFO:create_model() successfully completed......................................
2023-10-31 03:13:20,887:INFO:SubProcess create_model() end ==================================
2023-10-31 03:13:20,887:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=100, n_jobs=-1, oob_score=False,
                       random_state=1784, verbose=0, warm_start=False) result for AUC is 0.8985
2023-10-31 03:13:20,888:INFO:RandomForestClassifier(bootstrap=False, ccp_alpha=0.0,
                       class_weight='balanced_subsample', criterion='gini',
                       max_depth=10, max_features='log2', max_leaf_nodes=None,
                       max_samples=None, min_impurity_decrease=0.001,
                       min_samples_leaf=5, min_samples_split=2,
                       min_weight_fraction_leaf=0.0, n_estimators=110,
                       n_jobs=-1, oob_score=False, random_state=1784, verbose=0,
                       warm_start=False) result for AUC is 0.8739
2023-10-31 03:13:20,888:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=100, n_jobs=-1, oob_score=False,
                       random_state=1784, verbose=0, warm_start=False) is best model
2023-10-31 03:13:20,888:INFO:choose_better completed
2023-10-31 03:13:20,888:INFO:Original model was better than the tuned model, hence it will be returned. NOTE: The display metrics are for the tuned model (not the original one).
2023-10-31 03:13:20,896:INFO:_master_model_container: 29
2023-10-31 03:13:20,896:INFO:_display_container: 12
2023-10-31 03:13:20,897:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=100, n_jobs=-1, oob_score=False,
                       random_state=1784, verbose=0, warm_start=False)
2023-10-31 03:13:20,897:INFO:tune_model() successfully completed......................................
2023-10-31 03:13:20,980:INFO:Initializing tune_model()
2023-10-31 03:13:20,980:INFO:tune_model(estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=1784, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=None, round=4, n_iter=10, custom_grid=None, optimize=AUC, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={}, self=<pycaret.classification.oop.ClassificationExperiment object at 0x2a279fcd0>)
2023-10-31 03:13:20,980:INFO:Checking exceptions
2023-10-31 03:13:20,993:INFO:Copying training dataset
2023-10-31 03:13:21,000:INFO:Checking base model
2023-10-31 03:13:21,000:INFO:Base model : Light Gradient Boosting Machine
2023-10-31 03:13:21,002:INFO:Declaring metric variables
2023-10-31 03:13:21,004:INFO:Defining Hyperparameters
2023-10-31 03:13:21,144:INFO:Tuning with n_jobs=-1
2023-10-31 03:13:21,144:INFO:Initializing RandomizedSearchCV
2023-10-31 03:32:23,694:WARNING:A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.

2023-10-31 03:50:50,309:INFO:best_params: {'actual_estimator__reg_lambda': 0.7, 'actual_estimator__reg_alpha': 1e-07, 'actual_estimator__num_leaves': 80, 'actual_estimator__n_estimators': 240, 'actual_estimator__min_split_gain': 0.1, 'actual_estimator__min_child_samples': 61, 'actual_estimator__learning_rate': 0.05, 'actual_estimator__feature_fraction': 0.5, 'actual_estimator__bagging_freq': 7, 'actual_estimator__bagging_fraction': 0.6}
2023-10-31 03:50:50,324:INFO:Hyperparameter search completed
2023-10-31 03:50:50,324:INFO:SubProcess create_model() called ==================================
2023-10-31 03:50:50,329:INFO:Initializing create_model()
2023-10-31 03:50:50,329:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x2a279fcd0>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=1784, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x16561b250>, model_only=True, return_train_score=False, kwargs={'reg_lambda': 0.7, 'reg_alpha': 1e-07, 'num_leaves': 80, 'n_estimators': 240, 'min_split_gain': 0.1, 'min_child_samples': 61, 'learning_rate': 0.05, 'feature_fraction': 0.5, 'bagging_freq': 7, 'bagging_fraction': 0.6})
2023-10-31 03:50:50,329:INFO:Checking exceptions
2023-10-31 03:50:50,330:INFO:Importing libraries
2023-10-31 03:50:50,330:INFO:Copying training dataset
2023-10-31 03:50:50,388:INFO:Defining folds
2023-10-31 03:50:50,389:INFO:Declaring metric variables
2023-10-31 03:50:50,408:INFO:Importing untrained model
2023-10-31 03:50:50,408:INFO:Declaring custom model
2023-10-31 03:50:50,416:INFO:Light Gradient Boosting Machine Imported successfully
2023-10-31 03:50:50,420:INFO:Starting cross validation
2023-10-31 03:50:50,899:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-10-31 03:54:14,233:INFO:[LightGBM] [Info] Number of positive: 2940, number of negative: 3113
2023-10-31 03:54:15,172:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.938905 seconds.
2023-10-31 03:54:15,181:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 03:54:15,181:INFO:[LightGBM] [Info] Total Bins 1291830
2023-10-31 03:54:15,412:INFO:[LightGBM] [Info] Number of data points in the train set: 6053, number of used features: 5066
2023-10-31 03:54:15,431:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.485710 -> initscore=-0.057177
2023-10-31 03:54:15,432:INFO:[LightGBM] [Info] Start training from score -0.057177
2023-10-31 03:55:20,457:INFO:[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7
2023-10-31 03:55:20,457:INFO:[LightGBM] [Warning] feature_fraction is set=0.5, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5
2023-10-31 03:55:20,457:INFO:[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6
2023-10-31 03:55:20,488:INFO:[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7
2023-10-31 03:55:20,488:INFO:[LightGBM] [Warning] feature_fraction is set=0.5, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5
2023-10-31 03:55:20,488:INFO:[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6
2023-10-31 03:55:20,488:INFO:[LightGBM] [Info] Number of positive: 2940, number of negative: 3113
2023-10-31 03:55:20,490:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001287 seconds.
2023-10-31 03:55:20,490:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 03:55:20,490:INFO:[LightGBM] [Info] Total Bins 15045
2023-10-31 03:55:20,491:INFO:[LightGBM] [Info] Number of data points in the train set: 6053, number of used features: 59
2023-10-31 03:55:20,491:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.485710 -> initscore=-0.057177
2023-10-31 03:55:20,491:INFO:[LightGBM] [Info] Start training from score -0.057177
2023-10-31 03:55:20,493:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:20,496:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:20,498:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:20,500:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:20,502:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:20,505:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:20,507:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:20,509:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:20,511:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:20,514:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:20,516:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:20,518:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:20,520:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:20,522:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:20,525:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:20,527:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:20,529:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:20,532:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:20,535:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:20,536:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:20,538:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:20,541:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:20,543:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:20,545:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:20,547:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:20,549:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:20,552:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:20,554:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:20,556:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:20,559:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:20,561:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:20,563:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:20,565:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:20,568:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:20,570:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:20,572:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:20,575:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:20,577:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:20,579:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:20,582:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:20,584:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:20,586:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:20,589:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:20,591:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:20,594:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:20,596:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:20,598:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:20,601:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:20,603:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:20,605:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:20,607:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:20,610:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:20,612:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:20,614:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:20,617:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:20,619:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:20,622:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:20,624:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:20,626:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:20,629:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:20,631:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:20,633:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:20,635:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:20,638:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:20,640:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:20,643:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:20,645:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:20,648:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:20,650:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:20,652:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:20,655:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:20,657:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:20,659:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:20,662:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:20,664:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:20,666:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:20,668:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:20,671:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:20,673:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:20,676:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:20,678:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:20,680:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:20,683:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:20,685:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:20,687:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:20,690:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:20,692:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:20,695:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:20,697:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:20,699:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:20,701:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:20,704:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:20,706:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:20,708:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:20,711:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:20,713:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:20,715:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:20,718:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:20,720:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:20,722:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:20,724:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:20,727:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:20,729:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:20,732:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:20,734:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:20,737:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:20,739:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:20,741:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:20,744:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:20,746:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:20,748:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:20,751:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:20,753:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:20,756:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:20,758:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:20,760:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:20,762:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:20,765:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:20,767:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:20,770:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:20,772:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:20,774:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:20,776:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:20,779:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:20,781:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:20,784:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:20,786:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:20,788:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:20,791:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:20,793:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:20,796:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:20,798:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:20,800:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:20,803:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:20,805:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:20,807:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:20,810:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:20,812:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:20,814:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:20,817:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:20,819:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:20,821:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:20,824:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:20,826:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:20,829:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:20,831:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:20,833:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:20,836:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:20,838:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:20,840:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:20,842:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:20,845:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:20,847:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:20,849:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:20,851:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:20,854:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:20,856:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:20,859:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:20,861:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:20,863:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:20,866:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:20,868:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:20,871:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:20,873:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:20,875:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:20,877:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:20,880:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:20,882:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:20,884:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:20,887:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:20,889:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:20,891:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:20,893:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:20,896:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:20,898:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:20,900:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:20,903:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:20,905:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:20,907:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:20,910:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:20,912:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:20,914:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:20,917:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:20,919:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:20,922:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:20,924:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:20,926:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:20,929:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:20,931:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:20,933:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:20,936:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:20,938:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:20,941:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:20,943:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:20,945:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:20,948:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:20,950:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:20,952:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:20,954:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:20,957:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:20,959:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:20,961:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:20,964:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:20,966:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:20,968:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:20,971:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:20,973:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:20,976:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:20,978:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:20,980:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:20,983:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:20,985:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:20,987:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:20,990:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:20,992:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:20,994:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:20,997:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:20,999:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:21,001:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:21,004:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:21,006:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:21,008:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:21,011:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:21,013:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:21,015:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:21,018:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:21,020:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:21,022:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:21,025:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:21,027:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:21,029:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:21,031:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:21,034:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:21,036:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:21,038:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:21,041:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:21,043:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:21,045:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:21,047:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:21,050:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 03:55:21,775:INFO:[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7
2023-10-31 03:55:21,775:INFO:[LightGBM] [Warning] feature_fraction is set=0.5, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5
2023-10-31 03:55:21,775:INFO:[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6
2023-10-31 03:55:21,794:INFO:[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7
2023-10-31 03:55:21,794:INFO:[LightGBM] [Warning] feature_fraction is set=0.5, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5
2023-10-31 03:55:21,794:INFO:[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6
2023-10-31 03:55:21,936:INFO:Calculating mean and std
2023-10-31 03:55:21,939:INFO:Creating metrics dataframe
2023-10-31 03:55:21,951:INFO:Finalizing model
2023-10-31 04:04:25,148:INFO:[LightGBM] [Info] Number of positive: 3312, number of negative: 3414
2023-10-31 04:04:25,465:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.315002 seconds.
2023-10-31 04:04:25,465:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 04:04:25,474:INFO:[LightGBM] [Info] Total Bins 1409640
2023-10-31 04:04:25,531:INFO:[LightGBM] [Info] Number of data points in the train set: 6726, number of used features: 5528
2023-10-31 04:04:25,537:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.492417 -> initscore=-0.030332
2023-10-31 04:04:25,537:INFO:[LightGBM] [Info] Start training from score -0.030332
2023-10-31 04:05:00,722:INFO:[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7
2023-10-31 04:05:00,722:INFO:[LightGBM] [Warning] feature_fraction is set=0.5, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5
2023-10-31 04:05:00,723:INFO:[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6
2023-10-31 04:05:00,758:INFO:[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7
2023-10-31 04:05:00,758:INFO:[LightGBM] [Warning] feature_fraction is set=0.5, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5
2023-10-31 04:05:00,758:INFO:[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6
2023-10-31 04:05:00,758:INFO:[LightGBM] [Info] Number of positive: 3312, number of negative: 3414
2023-10-31 04:05:00,760:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001418 seconds.
2023-10-31 04:05:00,760:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 04:05:00,760:INFO:[LightGBM] [Info] Total Bins 15045
2023-10-31 04:05:00,761:INFO:[LightGBM] [Info] Number of data points in the train set: 6726, number of used features: 59
2023-10-31 04:05:00,761:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.492417 -> initscore=-0.030332
2023-10-31 04:05:00,761:INFO:[LightGBM] [Info] Start training from score -0.030332
2023-10-31 04:05:00,764:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:00,766:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:00,769:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:00,771:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:00,774:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:00,776:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:00,779:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:00,781:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:00,784:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:00,786:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:00,789:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:00,791:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:00,794:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:00,796:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:00,799:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:00,802:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:00,804:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:00,807:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:00,809:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:00,812:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:00,815:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:00,817:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:00,820:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:00,822:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:00,825:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:00,827:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:00,830:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:00,833:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:00,835:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:00,838:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:00,841:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:00,843:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:00,846:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:00,849:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:00,851:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:00,854:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:00,856:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:00,859:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:00,862:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:00,864:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:00,867:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:00,869:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:00,872:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:00,875:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:00,878:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:00,881:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:00,883:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:00,886:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:00,889:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:00,891:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:00,894:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:00,897:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:00,899:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:00,902:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:00,905:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:00,908:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:00,910:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:00,913:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:00,916:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:00,919:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:00,921:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:00,924:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:00,926:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:00,929:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:00,932:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:00,935:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:00,937:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:00,940:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:00,942:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:00,945:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:00,948:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:00,950:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:00,953:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:00,956:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:00,958:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:00,961:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:00,964:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:00,966:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:00,969:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:00,972:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:00,974:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:00,977:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:00,980:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:00,982:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:00,985:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:00,988:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:00,991:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:00,994:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:00,996:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:00,999:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:01,002:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:01,005:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:01,007:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:01,010:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:01,013:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:01,015:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:01,018:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:01,020:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:01,023:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:01,026:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:01,029:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:01,031:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:01,034:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:01,037:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:01,039:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:01,042:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:01,045:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:01,047:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:01,050:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:01,053:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:01,055:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:01,058:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:01,061:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:01,064:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:01,066:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:01,069:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:01,072:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:01,074:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:01,077:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:01,080:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:01,083:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:01,085:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:01,088:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:01,091:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:01,093:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:01,096:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:01,099:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:01,101:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:01,104:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:01,107:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:01,110:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:01,112:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:01,115:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:01,118:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:01,120:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:01,123:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:01,126:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:01,128:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:01,131:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:01,133:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:01,136:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:01,139:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:01,142:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:01,144:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:01,147:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:01,149:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:01,152:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:01,155:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:01,157:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:01,160:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:01,163:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:01,165:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:01,168:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:01,171:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:01,173:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:01,176:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:01,179:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:01,181:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:01,184:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:01,186:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:01,189:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:01,192:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:01,195:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:01,197:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:01,200:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:01,203:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:01,206:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:01,208:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:01,211:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:01,214:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:01,216:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:01,219:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:01,222:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:01,224:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:01,227:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:01,230:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:01,232:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:01,235:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:01,238:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:01,241:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:01,244:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:01,246:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:01,249:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:01,252:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:01,254:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:01,257:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:01,259:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:01,262:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:01,265:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:01,267:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:01,270:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:01,273:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:01,275:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:01,278:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:01,281:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:01,283:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:01,286:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:01,288:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:01,291:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:01,294:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:01,296:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:01,299:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:01,302:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:01,305:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:01,307:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:01,310:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:01,313:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:01,315:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:01,318:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:01,321:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:01,324:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:01,326:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:01,329:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:01,332:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:01,334:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:01,337:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:01,340:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:01,342:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:01,345:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:01,348:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:01,350:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:01,353:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:01,355:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:01,358:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:01,361:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:01,363:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:01,366:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:01,368:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:01,371:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:01,374:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:01,376:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:01,379:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:01,381:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:01,384:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:01,386:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:01,389:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:01,391:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:01,394:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:01,397:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:01,399:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-10-31 04:05:01,460:INFO:Uploading results into container
2023-10-31 04:05:01,460:INFO:Uploading model into container now
2023-10-31 04:05:01,461:INFO:_master_model_container: 30
2023-10-31 04:05:01,461:INFO:_display_container: 13
2023-10-31 04:05:01,462:INFO:LGBMClassifier(bagging_fraction=0.6, bagging_freq=7, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.5,
               importance_type='split', learning_rate=0.05, max_depth=-1,
               min_child_samples=61, min_child_weight=0.001, min_split_gain=0.1,
               n_estimators=240, n_jobs=-1, num_leaves=80, objective=None,
               random_state=1784, reg_alpha=1e-07, reg_lambda=0.7,
               subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
2023-10-31 04:05:01,462:INFO:create_model() successfully completed......................................
2023-10-31 04:05:01,838:INFO:SubProcess create_model() end ==================================
2023-10-31 04:05:01,838:INFO:choose_better activated
2023-10-31 04:05:01,840:INFO:SubProcess create_model() called ==================================
2023-10-31 04:05:01,840:INFO:Initializing create_model()
2023-10-31 04:05:01,840:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x2a279fcd0>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=1784, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-10-31 04:05:01,840:INFO:Checking exceptions
2023-10-31 04:05:01,843:INFO:Importing libraries
2023-10-31 04:05:01,843:INFO:Copying training dataset
2023-10-31 04:05:01,855:INFO:Defining folds
2023-10-31 04:05:01,855:INFO:Declaring metric variables
2023-10-31 04:05:01,855:INFO:Importing untrained model
2023-10-31 04:05:01,855:INFO:Declaring custom model
2023-10-31 04:05:01,856:INFO:Light Gradient Boosting Machine Imported successfully
2023-10-31 04:05:01,856:INFO:Starting cross validation
2023-10-31 04:05:01,958:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-10-31 04:06:29,424:INFO:[LightGBM] [Info] Number of positive: 2942, number of negative: 3110
2023-10-31 04:06:29,598:INFO:[LightGBM] [Info] Number of positive: 2954, number of negative: 3099
2023-10-31 04:06:29,762:INFO:[LightGBM] [Info] Number of positive: 2975, number of negative: 3077
2023-10-31 04:06:29,794:INFO:[LightGBM] [Info] Number of positive: 2956, number of negative: 3097
2023-10-31 04:06:29,814:INFO:[LightGBM] [Info] Number of positive: 2952, number of negative: 3101
2023-10-31 04:06:29,849:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.427537 seconds.
2023-10-31 04:06:29,849:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 04:06:29,859:INFO:[LightGBM] [Info] Total Bins 1294635
2023-10-31 04:06:29,919:INFO:[LightGBM] [Info] Number of positive: 2929, number of negative: 3124
2023-10-31 04:06:30,004:INFO:[LightGBM] [Info] Number of data points in the train set: 6052, number of used features: 5077
2023-10-31 04:06:30,011:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.486120 -> initscore=-0.055533
2023-10-31 04:06:30,011:INFO:[LightGBM] [Info] Start training from score -0.055533
2023-10-31 04:06:30,245:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.637319 seconds.
2023-10-31 04:06:30,245:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 04:06:30,245:INFO:[LightGBM] [Info] Total Bins 1290810
2023-10-31 04:06:30,307:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.542906 seconds.
2023-10-31 04:06:30,308:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 04:06:30,316:INFO:[LightGBM] [Info] Total Bins 1289280
2023-10-31 04:06:30,384:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.587335 seconds.
2023-10-31 04:06:30,385:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 04:06:30,394:INFO:[LightGBM] [Info] Total Bins 1289790
2023-10-31 04:06:30,546:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.716951 seconds.
2023-10-31 04:06:30,547:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 04:06:30,579:INFO:[LightGBM] [Info] Total Bins 1286730
2023-10-31 04:06:30,641:INFO:[LightGBM] [Info] Number of data points in the train set: 6053, number of used features: 5062
2023-10-31 04:06:30,658:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.488022 -> initscore=-0.047919
2023-10-31 04:06:30,659:INFO:[LightGBM] [Info] Start training from score -0.047919
2023-10-31 04:06:30,762:INFO:[LightGBM] [Info] Number of data points in the train set: 6052, number of used features: 5056
2023-10-31 04:06:30,801:INFO:[LightGBM] [Info] Number of data points in the train set: 6053, number of used features: 5058
2023-10-31 04:06:30,817:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.491573 -> initscore=-0.033711
2023-10-31 04:06:30,818:INFO:[LightGBM] [Info] Start training from score -0.033711
2023-10-31 04:06:30,825:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.488353 -> initscore=-0.046597
2023-10-31 04:06:30,825:INFO:[LightGBM] [Info] Start training from score -0.046597
2023-10-31 04:06:30,904:INFO:[LightGBM] [Info] Number of data points in the train set: 6053, number of used features: 5046
2023-10-31 04:06:30,915:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.487692 -> initscore=-0.049242
2023-10-31 04:06:30,916:INFO:[LightGBM] [Info] Start training from score -0.049242
2023-10-31 04:06:30,966:INFO:[LightGBM] [Info] Number of positive: 2944, number of negative: 3109
2023-10-31 04:06:31,035:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 1.091299 seconds.
2023-10-31 04:06:31,035:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 04:06:31,035:INFO:[LightGBM] [Info] Number of positive: 2921, number of negative: 3132
2023-10-31 04:06:31,047:INFO:[LightGBM] [Info] Total Bins 1291830
2023-10-31 04:06:31,322:INFO:[LightGBM] [Info] Number of data points in the train set: 6053, number of used features: 5066
2023-10-31 04:06:31,336:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.483892 -> initscore=-0.064453
2023-10-31 04:06:31,336:INFO:[LightGBM] [Info] Start training from score -0.064453
2023-10-31 04:06:31,637:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.667726 seconds.
2023-10-31 04:06:31,638:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 04:06:31,650:INFO:[LightGBM] [Info] Total Bins 1292085
2023-10-31 04:06:31,678:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.633440 seconds.
2023-10-31 04:06:31,678:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 04:06:31,686:INFO:[LightGBM] [Info] Total Bins 1294890
2023-10-31 04:06:32,003:INFO:[LightGBM] [Info] Number of data points in the train set: 6053, number of used features: 5067
2023-10-31 04:06:32,041:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.486370 -> initscore=-0.054532
2023-10-31 04:06:32,041:INFO:[LightGBM] [Info] Start training from score -0.054532
2023-10-31 04:06:32,089:INFO:[LightGBM] [Info] Number of data points in the train set: 6053, number of used features: 5078
2023-10-31 04:06:32,106:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.482571 -> initscore=-0.069746
2023-10-31 04:06:32,107:INFO:[LightGBM] [Info] Start training from score -0.069746
2023-10-31 04:06:33,718:INFO:[LightGBM] [Info] Number of positive: 2946, number of negative: 3107
2023-10-31 04:06:34,500:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.775595 seconds.
2023-10-31 04:06:34,514:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 04:06:34,529:INFO:[LightGBM] [Info] Total Bins 1294890
2023-10-31 04:06:34,770:INFO:[LightGBM] [Info] Number of data points in the train set: 6053, number of used features: 5078
2023-10-31 04:06:34,792:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.486701 -> initscore=-0.053209
2023-10-31 04:06:34,792:INFO:[LightGBM] [Info] Start training from score -0.053209
2023-10-31 04:08:23,487:INFO:[LightGBM] [Info] Number of positive: 2940, number of negative: 3113
2023-10-31 04:08:24,212:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.721899 seconds.
2023-10-31 04:08:24,212:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 04:08:24,219:INFO:[LightGBM] [Info] Total Bins 1291830
2023-10-31 04:08:24,490:INFO:[LightGBM] [Info] Number of data points in the train set: 6053, number of used features: 5066
2023-10-31 04:08:24,519:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.485710 -> initscore=-0.057177
2023-10-31 04:08:24,520:INFO:[LightGBM] [Info] Start training from score -0.057177
2023-10-31 04:09:16,024:INFO:[LightGBM] [Info] Number of positive: 2944, number of negative: 3109
2023-10-31 04:09:16,026:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001769 seconds.
2023-10-31 04:09:16,026:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 04:09:16,026:INFO:[LightGBM] [Info] Total Bins 15045
2023-10-31 04:09:16,026:INFO:[LightGBM] [Info] Number of data points in the train set: 6053, number of used features: 59
2023-10-31 04:09:16,026:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.486370 -> initscore=-0.054532
2023-10-31 04:09:16,026:INFO:[LightGBM] [Info] Start training from score -0.054532
2023-10-31 04:09:16,288:INFO:[LightGBM] [Info] Number of positive: 2975, number of negative: 3077
2023-10-31 04:09:16,290:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001821 seconds.
2023-10-31 04:09:16,291:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 04:09:16,291:INFO:[LightGBM] [Info] Total Bins 15045
2023-10-31 04:09:16,291:INFO:[LightGBM] [Info] Number of data points in the train set: 6052, number of used features: 59
2023-10-31 04:09:16,292:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.491573 -> initscore=-0.033711
2023-10-31 04:09:16,292:INFO:[LightGBM] [Info] Start training from score -0.033711
2023-10-31 04:09:16,468:INFO:[LightGBM] [Info] Number of positive: 2942, number of negative: 3110
2023-10-31 04:09:16,470:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001754 seconds.
2023-10-31 04:09:16,470:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 04:09:16,470:INFO:[LightGBM] [Info] Total Bins 15045
2023-10-31 04:09:16,470:INFO:[LightGBM] [Info] Number of data points in the train set: 6052, number of used features: 59
2023-10-31 04:09:16,471:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.486120 -> initscore=-0.055533
2023-10-31 04:09:16,471:INFO:[LightGBM] [Info] Start training from score -0.055533
2023-10-31 04:09:16,778:INFO:[LightGBM] [Info] Number of positive: 2956, number of negative: 3097
2023-10-31 04:09:16,780:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001799 seconds.
2023-10-31 04:09:16,784:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 04:09:16,784:INFO:[LightGBM] [Info] Total Bins 15045
2023-10-31 04:09:16,785:INFO:[LightGBM] [Info] Number of data points in the train set: 6053, number of used features: 59
2023-10-31 04:09:16,785:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.488353 -> initscore=-0.046597
2023-10-31 04:09:16,785:INFO:[LightGBM] [Info] Start training from score -0.046597
2023-10-31 04:09:16,804:INFO:[LightGBM] [Info] Number of positive: 2921, number of negative: 3132
2023-10-31 04:09:16,805:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001744 seconds.
2023-10-31 04:09:16,805:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 04:09:16,805:INFO:[LightGBM] [Info] Total Bins 15045
2023-10-31 04:09:16,806:INFO:[LightGBM] [Info] Number of data points in the train set: 6053, number of used features: 59
2023-10-31 04:09:16,806:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.482571 -> initscore=-0.069746
2023-10-31 04:09:16,806:INFO:[LightGBM] [Info] Start training from score -0.069746
2023-10-31 04:09:17,055:INFO:[LightGBM] [Info] Number of positive: 2952, number of negative: 3101
2023-10-31 04:09:17,057:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001714 seconds.
2023-10-31 04:09:17,057:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 04:09:17,057:INFO:[LightGBM] [Info] Total Bins 15045
2023-10-31 04:09:17,058:INFO:[LightGBM] [Info] Number of data points in the train set: 6053, number of used features: 59
2023-10-31 04:09:17,059:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.487692 -> initscore=-0.049242
2023-10-31 04:09:17,059:INFO:[LightGBM] [Info] Start training from score -0.049242
2023-10-31 04:09:17,189:INFO:[LightGBM] [Info] Number of positive: 2946, number of negative: 3107
2023-10-31 04:09:17,192:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002122 seconds.
2023-10-31 04:09:17,192:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 04:09:17,193:INFO:[LightGBM] [Info] Total Bins 15045
2023-10-31 04:09:17,193:INFO:[LightGBM] [Info] Number of data points in the train set: 6053, number of used features: 59
2023-10-31 04:09:17,194:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.486701 -> initscore=-0.053209
2023-10-31 04:09:17,194:INFO:[LightGBM] [Info] Start training from score -0.053209
2023-10-31 04:09:17,410:INFO:[LightGBM] [Info] Number of positive: 2929, number of negative: 3124
2023-10-31 04:09:17,411:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001674 seconds.
2023-10-31 04:09:17,411:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 04:09:17,411:INFO:[LightGBM] [Info] Total Bins 15045
2023-10-31 04:09:17,412:INFO:[LightGBM] [Info] Number of data points in the train set: 6053, number of used features: 59
2023-10-31 04:09:17,412:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.483892 -> initscore=-0.064453
2023-10-31 04:09:17,412:INFO:[LightGBM] [Info] Start training from score -0.064453
2023-10-31 04:09:17,475:INFO:[LightGBM] [Info] Number of positive: 2954, number of negative: 3099
2023-10-31 04:09:17,475:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001809 seconds.
2023-10-31 04:09:17,475:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 04:09:17,475:INFO:[LightGBM] [Info] Total Bins 15045
2023-10-31 04:09:17,477:INFO:[LightGBM] [Info] Number of data points in the train set: 6053, number of used features: 59
2023-10-31 04:09:17,477:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.488022 -> initscore=-0.047919
2023-10-31 04:09:17,477:INFO:[LightGBM] [Info] Start training from score -0.047919
2023-10-31 04:09:37,855:INFO:[LightGBM] [Info] Number of positive: 2940, number of negative: 3113
2023-10-31 04:09:37,856:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001548 seconds.
2023-10-31 04:09:37,856:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 04:09:37,857:INFO:[LightGBM] [Info] Total Bins 15045
2023-10-31 04:09:37,857:INFO:[LightGBM] [Info] Number of data points in the train set: 6053, number of used features: 59
2023-10-31 04:09:37,857:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.485710 -> initscore=-0.057177
2023-10-31 04:09:37,857:INFO:[LightGBM] [Info] Start training from score -0.057177
2023-10-31 04:09:39,022:INFO:Calculating mean and std
2023-10-31 04:09:39,023:INFO:Creating metrics dataframe
2023-10-31 04:09:39,028:INFO:Finalizing model
2023-10-31 04:18:41,394:INFO:[LightGBM] [Info] Number of positive: 3312, number of negative: 3414
2023-10-31 04:18:41,646:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.250460 seconds.
2023-10-31 04:18:41,647:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 04:18:41,654:INFO:[LightGBM] [Info] Total Bins 1409640
2023-10-31 04:18:41,705:INFO:[LightGBM] [Info] Number of data points in the train set: 6726, number of used features: 5528
2023-10-31 04:18:41,711:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.492417 -> initscore=-0.030332
2023-10-31 04:18:41,711:INFO:[LightGBM] [Info] Start training from score -0.030332
2023-10-31 04:19:15,486:INFO:[LightGBM] [Info] Number of positive: 3312, number of negative: 3414
2023-10-31 04:19:15,488:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001688 seconds.
2023-10-31 04:19:15,488:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 04:19:15,488:INFO:[LightGBM] [Info] Total Bins 15045
2023-10-31 04:19:15,489:INFO:[LightGBM] [Info] Number of data points in the train set: 6726, number of used features: 59
2023-10-31 04:19:15,489:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.492417 -> initscore=-0.030332
2023-10-31 04:19:15,489:INFO:[LightGBM] [Info] Start training from score -0.030332
2023-10-31 04:19:15,864:INFO:Uploading results into container
2023-10-31 04:19:15,864:INFO:Uploading model into container now
2023-10-31 04:19:15,864:INFO:_master_model_container: 31
2023-10-31 04:19:15,865:INFO:_display_container: 14
2023-10-31 04:19:15,865:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=1784, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2023-10-31 04:19:15,865:INFO:create_model() successfully completed......................................
2023-10-31 04:19:16,274:INFO:SubProcess create_model() end ==================================
2023-10-31 04:19:16,274:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=1784, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) result for AUC is 0.9119
2023-10-31 04:19:16,274:INFO:LGBMClassifier(bagging_fraction=0.6, bagging_freq=7, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.5,
               importance_type='split', learning_rate=0.05, max_depth=-1,
               min_child_samples=61, min_child_weight=0.001, min_split_gain=0.1,
               n_estimators=240, n_jobs=-1, num_leaves=80, objective=None,
               random_state=1784, reg_alpha=1e-07, reg_lambda=0.7,
               subsample=1.0, subsample_for_bin=200000, subsample_freq=0) result for AUC is 0.9143
2023-10-31 04:19:16,275:INFO:LGBMClassifier(bagging_fraction=0.6, bagging_freq=7, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.5,
               importance_type='split', learning_rate=0.05, max_depth=-1,
               min_child_samples=61, min_child_weight=0.001, min_split_gain=0.1,
               n_estimators=240, n_jobs=-1, num_leaves=80, objective=None,
               random_state=1784, reg_alpha=1e-07, reg_lambda=0.7,
               subsample=1.0, subsample_for_bin=200000, subsample_freq=0) is best model
2023-10-31 04:19:16,275:INFO:choose_better completed
2023-10-31 04:19:16,284:INFO:_master_model_container: 31
2023-10-31 04:19:16,284:INFO:_display_container: 13
2023-10-31 04:19:16,284:INFO:LGBMClassifier(bagging_fraction=0.6, bagging_freq=7, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.5,
               importance_type='split', learning_rate=0.05, max_depth=-1,
               min_child_samples=61, min_child_weight=0.001, min_split_gain=0.1,
               n_estimators=240, n_jobs=-1, num_leaves=80, objective=None,
               random_state=1784, reg_alpha=1e-07, reg_lambda=0.7,
               subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
2023-10-31 04:19:16,284:INFO:tune_model() successfully completed......................................
2023-10-31 10:51:02,120:INFO:Initializing stack_models()
2023-10-31 10:51:02,121:INFO:stack_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x2a279fcd0>, estimator_list=[ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     n_estimators=100, n_jobs=-1, oob_score=False,
                     random_state=1784, verbose=0, warm_start=False), RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=100, n_jobs=-1, oob_score=False,
                       random_state=1784, verbose=0, warm_start=False), QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001), <catboost.core.CatBoostClassifier object at 0x2b25da2f0>, LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=1784, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)], meta_model=ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     n_estimators=100, n_jobs=-1, oob_score=False,
                     random_state=1784, verbose=0, warm_start=False), meta_model_fold=5, fold=None, round=4, method=auto, restack=True, choose_better=False, optimize=AUC, fit_kwargs=None, groups=None, probability_threshold=None, verbose=True, return_train_score=False)
2023-10-31 10:51:02,121:INFO:Checking exceptions
2023-10-31 10:51:02,154:INFO:Defining meta model
2023-10-31 10:51:02,163:INFO:Getting model names
2023-10-31 10:51:02,163:INFO:[('Extra Trees Classifier', ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     n_estimators=100, n_jobs=-1, oob_score=False,
                     random_state=1784, verbose=0, warm_start=False)), ('Random Forest Classifier', RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=100, n_jobs=-1, oob_score=False,
                       random_state=1784, verbose=0, warm_start=False)), ('Quadratic Discriminant Analysis', QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)), ('CatBoost Classifier', <catboost.core.CatBoostClassifier object at 0x2b25da2f0>), ('Light Gradient Boosting Machine', LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=1784, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0))]
2023-10-31 10:51:02,165:INFO:SubProcess create_model() called ==================================
2023-10-31 10:51:02,168:INFO:Initializing create_model()
2023-10-31 10:51:02,168:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x2a279fcd0>, estimator=StackingClassifier(cv=5,
                   estimators=[('Extra Trees Classifier',
                                ExtraTreesClassifier(bootstrap=False,
                                                     ccp_alpha=0.0,
                                                     class_weight=None,
                                                     criterion='gini',
                                                     max_depth=None,
                                                     max_features='sqrt',
                                                     max_leaf_nodes=None,
                                                     max_samples=None,
                                                     min_impurity_decrease=0.0,
                                                     min_samples_leaf=1,
                                                     min_samples_split=2,
                                                     min_weight_fraction_leaf=0.0,
                                                     n_estimators=100,
                                                     n_jobs=-1, oob_score=F...
                                                        ccp_alpha=0.0,
                                                        class_weight=None,
                                                        criterion='gini',
                                                        max_depth=None,
                                                        max_features='sqrt',
                                                        max_leaf_nodes=None,
                                                        max_samples=None,
                                                        min_impurity_decrease=0.0,
                                                        min_samples_leaf=1,
                                                        min_samples_split=2,
                                                        min_weight_fraction_leaf=0.0,
                                                        n_estimators=100,
                                                        n_jobs=-1,
                                                        oob_score=False,
                                                        random_state=1784,
                                                        verbose=0,
                                                        warm_start=False),
                   n_jobs=-1, passthrough=True, stack_method='auto', verbose=0), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x2a39a8e80>, model_only=True, return_train_score=False, kwargs={})
2023-10-31 10:51:02,169:INFO:Checking exceptions
2023-10-31 10:51:02,169:INFO:Importing libraries
2023-10-31 10:51:02,169:INFO:Copying training dataset
2023-10-31 10:51:02,181:INFO:Defining folds
2023-10-31 10:51:02,181:INFO:Declaring metric variables
2023-10-31 10:51:02,183:INFO:Importing untrained model
2023-10-31 10:51:02,183:INFO:Declaring custom model
2023-10-31 10:51:02,189:INFO:Stacking Classifier Imported successfully
2023-10-31 10:51:02,193:INFO:Starting cross validation
2023-10-31 10:51:02,280:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-10-31 10:52:28,273:INFO:[LightGBM] [Info] Number of positive: 2954, number of negative: 3099
2023-10-31 10:52:28,805:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.527261 seconds.
2023-10-31 10:52:28,806:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 10:52:28,814:INFO:[LightGBM] [Info] Total Bins 1290810
2023-10-31 10:52:28,954:INFO:[LightGBM] [Info] Number of data points in the train set: 6053, number of used features: 5062
2023-10-31 10:52:28,963:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.488022 -> initscore=-0.047919
2023-10-31 10:52:28,963:INFO:[LightGBM] [Info] Start training from score -0.047919
2023-10-31 10:52:30,897:INFO:[LightGBM] [Info] Number of positive: 2942, number of negative: 3110
2023-10-31 10:52:31,489:INFO:[LightGBM] [Info] Number of positive: 2975, number of negative: 3077
2023-10-31 10:52:31,541:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.640361 seconds.
2023-10-31 10:52:31,541:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 10:52:31,548:INFO:[LightGBM] [Info] Total Bins 1294635
2023-10-31 10:52:31,776:INFO:[LightGBM] [Info] Number of data points in the train set: 6052, number of used features: 5077
2023-10-31 10:52:31,790:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.486120 -> initscore=-0.055533
2023-10-31 10:52:31,793:INFO:[LightGBM] [Info] Start training from score -0.055533
2023-10-31 10:52:32,085:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.592195 seconds.
2023-10-31 10:52:32,085:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 10:52:32,092:INFO:[LightGBM] [Info] Total Bins 1289280
2023-10-31 10:52:32,252:INFO:[LightGBM] [Info] Number of data points in the train set: 6052, number of used features: 5056
2023-10-31 10:52:32,263:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.491573 -> initscore=-0.033711
2023-10-31 10:52:32,263:INFO:[LightGBM] [Info] Start training from score -0.033711
2023-10-31 10:52:32,269:INFO:[LightGBM] [Info] Number of positive: 2952, number of negative: 3101
2023-10-31 10:52:32,524:INFO:[LightGBM] [Info] Number of positive: 2929, number of negative: 3124
2023-10-31 10:52:32,592:INFO:[LightGBM] [Info] Number of positive: 2956, number of negative: 3097
2023-10-31 10:52:32,955:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.680493 seconds.
2023-10-31 10:52:32,955:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 10:52:32,961:INFO:[LightGBM] [Info] Total Bins 1286730
2023-10-31 10:52:33,091:INFO:[LightGBM] [Info] Number of positive: 2944, number of negative: 3109
2023-10-31 10:52:33,132:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.604812 seconds.
2023-10-31 10:52:33,132:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 10:52:33,142:INFO:[LightGBM] [Info] Total Bins 1291830
2023-10-31 10:52:33,147:INFO:[LightGBM] [Info] Number of data points in the train set: 6053, number of used features: 5046
2023-10-31 10:52:33,157:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.487692 -> initscore=-0.049242
2023-10-31 10:52:33,157:INFO:[LightGBM] [Info] Start training from score -0.049242
2023-10-31 10:52:33,193:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.594477 seconds.
2023-10-31 10:52:33,193:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 10:52:33,204:INFO:[LightGBM] [Info] Total Bins 1289790
2023-10-31 10:52:33,346:INFO:[LightGBM] [Info] Number of data points in the train set: 6053, number of used features: 5066
2023-10-31 10:52:33,360:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.483892 -> initscore=-0.064453
2023-10-31 10:52:33,361:INFO:[LightGBM] [Info] Start training from score -0.064453
2023-10-31 10:52:33,423:INFO:[LightGBM] [Info] Number of data points in the train set: 6053, number of used features: 5058
2023-10-31 10:52:33,435:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.488353 -> initscore=-0.046597
2023-10-31 10:52:33,435:INFO:[LightGBM] [Info] Start training from score -0.046597
2023-10-31 10:52:34,024:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.922566 seconds.
2023-10-31 10:52:34,024:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 10:52:34,033:INFO:[LightGBM] [Info] Total Bins 1292085
2023-10-31 10:52:34,324:INFO:[LightGBM] [Info] Number of data points in the train set: 6053, number of used features: 5067
2023-10-31 10:52:34,344:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.486370 -> initscore=-0.054532
2023-10-31 10:52:34,345:INFO:[LightGBM] [Info] Start training from score -0.054532
2023-10-31 10:52:34,762:INFO:[LightGBM] [Info] Number of positive: 2921, number of negative: 3132
2023-10-31 10:52:34,763:INFO:[LightGBM] [Info] Number of positive: 2946, number of negative: 3107
2023-10-31 10:52:35,280:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.502882 seconds.
2023-10-31 10:52:35,281:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 10:52:35,288:INFO:[LightGBM] [Info] Total Bins 1294890
2023-10-31 10:52:35,518:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.724252 seconds.
2023-10-31 10:52:35,519:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 10:52:35,546:INFO:[LightGBM] [Info] Number of data points in the train set: 6053, number of used features: 5078
2023-10-31 10:52:35,546:INFO:[LightGBM] [Info] Total Bins 1294890
2023-10-31 10:52:35,570:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.482571 -> initscore=-0.069746
2023-10-31 10:52:35,570:INFO:[LightGBM] [Info] Start training from score -0.069746
2023-10-31 10:52:35,710:INFO:[LightGBM] [Info] Number of data points in the train set: 6053, number of used features: 5078
2023-10-31 10:52:35,729:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.486701 -> initscore=-0.053209
2023-10-31 10:52:35,730:INFO:[LightGBM] [Info] Start training from score -0.053209
2023-10-31 10:54:34,709:INFO:[LightGBM] [Info] Number of positive: 2940, number of negative: 3113
2023-10-31 10:54:35,444:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.713071 seconds.
2023-10-31 10:54:35,445:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 10:54:35,450:INFO:[LightGBM] [Info] Total Bins 1291830
2023-10-31 10:54:35,677:INFO:[LightGBM] [Info] Number of data points in the train set: 6053, number of used features: 5066
2023-10-31 10:54:35,691:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.485710 -> initscore=-0.057177
2023-10-31 10:54:35,693:INFO:[LightGBM] [Info] Start training from score -0.057177
2023-10-31 10:55:12,845:INFO:[LightGBM] [Info] Number of positive: 2954, number of negative: 3099
2023-10-31 10:55:12,848:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005200 seconds.
2023-10-31 10:55:12,848:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 10:55:12,849:INFO:[LightGBM] [Info] Total Bins 15045
2023-10-31 10:55:12,849:INFO:[LightGBM] [Info] Number of data points in the train set: 6053, number of used features: 59
2023-10-31 10:55:12,850:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.488022 -> initscore=-0.047919
2023-10-31 10:55:12,850:INFO:[LightGBM] [Info] Start training from score -0.047919
2023-10-31 10:55:23,440:INFO:[LightGBM] [Info] Number of positive: 2363, number of negative: 2479
2023-10-31 10:55:23,446:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001377 seconds.
2023-10-31 10:55:23,447:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 10:55:23,448:INFO:[LightGBM] [Info] Total Bins 15045
2023-10-31 10:55:23,448:INFO:[LightGBM] [Info] Number of data points in the train set: 4842, number of used features: 59
2023-10-31 10:55:23,448:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.488021 -> initscore=-0.047923
2023-10-31 10:55:23,448:INFO:[LightGBM] [Info] Start training from score -0.047923
2023-10-31 10:55:24,157:INFO:[LightGBM] [Info] Number of positive: 2975, number of negative: 3077
2023-10-31 10:55:24,159:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001663 seconds.
2023-10-31 10:55:24,160:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 10:55:24,160:INFO:[LightGBM] [Info] Total Bins 15045
2023-10-31 10:55:24,160:INFO:[LightGBM] [Info] Number of data points in the train set: 6052, number of used features: 59
2023-10-31 10:55:24,160:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.491573 -> initscore=-0.033711
2023-10-31 10:55:24,160:INFO:[LightGBM] [Info] Start training from score -0.033711
2023-10-31 10:55:24,493:INFO:[LightGBM] [Info] Number of positive: 2363, number of negative: 2479
2023-10-31 10:55:24,494:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001351 seconds.
2023-10-31 10:55:24,494:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 10:55:24,494:INFO:[LightGBM] [Info] Total Bins 15045
2023-10-31 10:55:24,496:INFO:[LightGBM] [Info] Number of data points in the train set: 4842, number of used features: 59
2023-10-31 10:55:24,496:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.488021 -> initscore=-0.047923
2023-10-31 10:55:24,496:INFO:[LightGBM] [Info] Start training from score -0.047923
2023-10-31 10:55:25,123:INFO:[LightGBM] [Info] Number of positive: 2952, number of negative: 3101
2023-10-31 10:55:25,123:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001682 seconds.
2023-10-31 10:55:25,124:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 10:55:25,125:INFO:[LightGBM] [Info] Total Bins 15045
2023-10-31 10:55:25,125:INFO:[LightGBM] [Info] Number of data points in the train set: 6053, number of used features: 59
2023-10-31 10:55:25,125:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.487692 -> initscore=-0.049242
2023-10-31 10:55:25,125:INFO:[LightGBM] [Info] Start training from score -0.049242
2023-10-31 10:55:25,165:INFO:[LightGBM] [Info] Number of positive: 2929, number of negative: 3124
2023-10-31 10:55:25,166:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001652 seconds.
2023-10-31 10:55:25,166:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 10:55:25,167:INFO:[LightGBM] [Info] Total Bins 15045
2023-10-31 10:55:25,168:INFO:[LightGBM] [Info] Number of data points in the train set: 6053, number of used features: 59
2023-10-31 10:55:25,168:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.483892 -> initscore=-0.064453
2023-10-31 10:55:25,168:INFO:[LightGBM] [Info] Start training from score -0.064453
2023-10-31 10:55:25,276:INFO:[LightGBM] [Info] Number of positive: 2942, number of negative: 3110
2023-10-31 10:55:25,278:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001678 seconds.
2023-10-31 10:55:25,278:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 10:55:25,278:INFO:[LightGBM] [Info] Total Bins 15045
2023-10-31 10:55:25,280:INFO:[LightGBM] [Info] Number of data points in the train set: 6052, number of used features: 59
2023-10-31 10:55:25,280:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.486120 -> initscore=-0.055533
2023-10-31 10:55:25,280:INFO:[LightGBM] [Info] Start training from score -0.055533
2023-10-31 10:55:25,362:INFO:[LightGBM] [Info] Number of positive: 2363, number of negative: 2479
2023-10-31 10:55:25,363:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001325 seconds.
2023-10-31 10:55:25,363:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 10:55:25,363:INFO:[LightGBM] [Info] Total Bins 15045
2023-10-31 10:55:25,363:INFO:[LightGBM] [Info] Number of data points in the train set: 4842, number of used features: 59
2023-10-31 10:55:25,363:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.488021 -> initscore=-0.047923
2023-10-31 10:55:25,363:INFO:[LightGBM] [Info] Start training from score -0.047923
2023-10-31 10:55:25,466:INFO:[LightGBM] [Info] Number of positive: 2956, number of negative: 3097
2023-10-31 10:55:25,469:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001820 seconds.
2023-10-31 10:55:25,469:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 10:55:25,469:INFO:[LightGBM] [Info] Total Bins 15045
2023-10-31 10:55:25,469:INFO:[LightGBM] [Info] Number of data points in the train set: 6053, number of used features: 59
2023-10-31 10:55:25,469:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.488353 -> initscore=-0.046597
2023-10-31 10:55:25,469:INFO:[LightGBM] [Info] Start training from score -0.046597
2023-10-31 10:55:26,185:INFO:[LightGBM] [Info] Number of positive: 2921, number of negative: 3132
2023-10-31 10:55:26,187:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002488 seconds.
2023-10-31 10:55:26,187:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 10:55:26,187:INFO:[LightGBM] [Info] Total Bins 15045
2023-10-31 10:55:26,188:INFO:[LightGBM] [Info] Number of data points in the train set: 6053, number of used features: 59
2023-10-31 10:55:26,188:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.482571 -> initscore=-0.069746
2023-10-31 10:55:26,188:INFO:[LightGBM] [Info] Start training from score -0.069746
2023-10-31 10:55:26,282:INFO:[LightGBM] [Info] Number of positive: 2363, number of negative: 2480
2023-10-31 10:55:26,290:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008328 seconds.
2023-10-31 10:55:26,290:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 10:55:26,290:INFO:[LightGBM] [Info] Total Bins 15045
2023-10-31 10:55:26,291:INFO:[LightGBM] [Info] Number of data points in the train set: 4843, number of used features: 59
2023-10-31 10:55:26,291:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.487921 -> initscore=-0.048327
2023-10-31 10:55:26,291:INFO:[LightGBM] [Info] Start training from score -0.048327
2023-10-31 10:55:26,612:INFO:[LightGBM] [Info] Number of positive: 2944, number of negative: 3109
2023-10-31 10:55:26,614:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001672 seconds.
2023-10-31 10:55:26,614:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 10:55:26,614:INFO:[LightGBM] [Info] Total Bins 15045
2023-10-31 10:55:26,615:INFO:[LightGBM] [Info] Number of data points in the train set: 6053, number of used features: 59
2023-10-31 10:55:26,616:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.486370 -> initscore=-0.054532
2023-10-31 10:55:26,616:INFO:[LightGBM] [Info] Start training from score -0.054532
2023-10-31 10:55:27,183:INFO:[LightGBM] [Info] Number of positive: 2364, number of negative: 2479
2023-10-31 10:55:27,185:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001289 seconds.
2023-10-31 10:55:27,185:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 10:55:27,185:INFO:[LightGBM] [Info] Total Bins 15045
2023-10-31 10:55:27,185:INFO:[LightGBM] [Info] Number of data points in the train set: 4843, number of used features: 59
2023-10-31 10:55:27,186:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.488127 -> initscore=-0.047500
2023-10-31 10:55:27,186:INFO:[LightGBM] [Info] Start training from score -0.047500
2023-10-31 10:55:27,701:INFO:[LightGBM] [Info] Number of positive: 2946, number of negative: 3107
2023-10-31 10:55:27,703:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001656 seconds.
2023-10-31 10:55:27,703:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 10:55:27,703:INFO:[LightGBM] [Info] Total Bins 15045
2023-10-31 10:55:27,704:INFO:[LightGBM] [Info] Number of data points in the train set: 6053, number of used features: 59
2023-10-31 10:55:27,705:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.486701 -> initscore=-0.053209
2023-10-31 10:55:27,705:INFO:[LightGBM] [Info] Start training from score -0.053209
2023-10-31 10:55:43,350:INFO:[LightGBM] [Info] Number of positive: 2380, number of negative: 2461
2023-10-31 10:55:43,352:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001679 seconds.
2023-10-31 10:55:43,352:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 10:55:43,352:INFO:[LightGBM] [Info] Total Bins 15045
2023-10-31 10:55:43,353:INFO:[LightGBM] [Info] Number of data points in the train set: 4841, number of used features: 59
2023-10-31 10:55:43,354:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.491634 -> initscore=-0.033467
2023-10-31 10:55:43,354:INFO:[LightGBM] [Info] Start training from score -0.033467
2023-10-31 10:55:43,839:INFO:[LightGBM] [Info] Number of positive: 2364, number of negative: 2478
2023-10-31 10:55:43,842:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003368 seconds.
2023-10-31 10:55:43,842:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 10:55:43,842:INFO:[LightGBM] [Info] Total Bins 15045
2023-10-31 10:55:43,845:INFO:[LightGBM] [Info] Number of data points in the train set: 4842, number of used features: 59
2023-10-31 10:55:43,845:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.488228 -> initscore=-0.047097
2023-10-31 10:55:43,845:INFO:[LightGBM] [Info] Start training from score -0.047097
2023-10-31 10:55:43,938:INFO:[LightGBM] [Info] Number of positive: 2361, number of negative: 2481
2023-10-31 10:55:43,943:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004160 seconds.
2023-10-31 10:55:43,943:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 10:55:43,943:INFO:[LightGBM] [Info] Total Bins 15045
2023-10-31 10:55:43,944:INFO:[LightGBM] [Info] Number of data points in the train set: 4842, number of used features: 59
2023-10-31 10:55:43,944:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.487608 -> initscore=-0.049576
2023-10-31 10:55:43,944:INFO:[LightGBM] [Info] Start training from score -0.049576
2023-10-31 10:55:43,984:INFO:[LightGBM] [Info] Number of positive: 2380, number of negative: 2461
2023-10-31 10:55:43,985:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001358 seconds.
2023-10-31 10:55:43,985:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 10:55:43,986:INFO:[LightGBM] [Info] Total Bins 15045
2023-10-31 10:55:43,986:INFO:[LightGBM] [Info] Number of data points in the train set: 4841, number of used features: 59
2023-10-31 10:55:43,986:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.491634 -> initscore=-0.033467
2023-10-31 10:55:43,986:INFO:[LightGBM] [Info] Start training from score -0.033467
2023-10-31 10:55:44,122:INFO:[LightGBM] [Info] Number of positive: 2343, number of negative: 2499
2023-10-31 10:55:44,125:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003132 seconds.
2023-10-31 10:55:44,125:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 10:55:44,126:INFO:[LightGBM] [Info] Total Bins 15045
2023-10-31 10:55:44,126:INFO:[LightGBM] [Info] Number of data points in the train set: 4842, number of used features: 59
2023-10-31 10:55:44,135:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.483891 -> initscore=-0.064458
2023-10-31 10:55:44,135:INFO:[LightGBM] [Info] Start training from score -0.064458
2023-10-31 10:55:44,220:INFO:[LightGBM] [Info] Number of positive: 2353, number of negative: 2488
2023-10-31 10:55:44,221:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001345 seconds.
2023-10-31 10:55:44,221:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 10:55:44,221:INFO:[LightGBM] [Info] Total Bins 15045
2023-10-31 10:55:44,222:INFO:[LightGBM] [Info] Number of data points in the train set: 4841, number of used features: 59
2023-10-31 10:55:44,223:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.486057 -> initscore=-0.055788
2023-10-31 10:55:44,223:INFO:[LightGBM] [Info] Start training from score -0.055788
2023-10-31 10:55:44,624:INFO:[LightGBM] [Info] Number of positive: 2380, number of negative: 2462
2023-10-31 10:55:44,627:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002659 seconds.
2023-10-31 10:55:44,627:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 10:55:44,627:INFO:[LightGBM] [Info] Total Bins 15045
2023-10-31 10:55:44,628:INFO:[LightGBM] [Info] Number of data points in the train set: 4842, number of used features: 59
2023-10-31 10:55:44,628:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.491532 -> initscore=-0.033874
2023-10-31 10:55:44,628:INFO:[LightGBM] [Info] Start training from score -0.033874
2023-10-31 10:55:45,335:INFO:[LightGBM] [Info] Number of positive: 2380, number of negative: 2462
2023-10-31 10:55:45,336:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001368 seconds.
2023-10-31 10:55:45,336:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 10:55:45,336:INFO:[LightGBM] [Info] Total Bins 15045
2023-10-31 10:55:45,337:INFO:[LightGBM] [Info] Number of data points in the train set: 4842, number of used features: 59
2023-10-31 10:55:45,337:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.491532 -> initscore=-0.033874
2023-10-31 10:55:45,337:INFO:[LightGBM] [Info] Start training from score -0.033874
2023-10-31 10:55:45,468:INFO:[LightGBM] [Info] Number of positive: 2365, number of negative: 2477
2023-10-31 10:55:45,469:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001335 seconds.
2023-10-31 10:55:45,470:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 10:55:45,471:INFO:[LightGBM] [Info] Total Bins 15045
2023-10-31 10:55:45,471:INFO:[LightGBM] [Info] Number of data points in the train set: 4842, number of used features: 59
2023-10-31 10:55:45,487:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.488435 -> initscore=-0.046270
2023-10-31 10:55:45,488:INFO:[LightGBM] [Info] Start training from score -0.046270
2023-10-31 10:55:45,488:INFO:[LightGBM] [Info] Number of positive: 2361, number of negative: 2481
2023-10-31 10:55:45,489:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001281 seconds.
2023-10-31 10:55:45,489:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 10:55:45,489:INFO:[LightGBM] [Info] Total Bins 15045
2023-10-31 10:55:45,490:INFO:[LightGBM] [Info] Number of data points in the train set: 4842, number of used features: 59
2023-10-31 10:55:45,490:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.487608 -> initscore=-0.049576
2023-10-31 10:55:45,490:INFO:[LightGBM] [Info] Start training from score -0.049576
2023-10-31 10:55:45,665:INFO:[LightGBM] [Info] Number of positive: 2343, number of negative: 2499
2023-10-31 10:55:45,667:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001314 seconds.
2023-10-31 10:55:45,668:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 10:55:45,668:INFO:[LightGBM] [Info] Total Bins 15045
2023-10-31 10:55:45,668:INFO:[LightGBM] [Info] Number of data points in the train set: 4842, number of used features: 59
2023-10-31 10:55:45,671:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.483891 -> initscore=-0.064458
2023-10-31 10:55:45,671:INFO:[LightGBM] [Info] Start training from score -0.064458
2023-10-31 10:55:45,958:INFO:[LightGBM] [Info] Number of positive: 2336, number of negative: 2506
2023-10-31 10:55:45,959:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001293 seconds.
2023-10-31 10:55:45,959:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 10:55:45,959:INFO:[LightGBM] [Info] Total Bins 15045
2023-10-31 10:55:45,961:INFO:[LightGBM] [Info] Number of data points in the train set: 4842, number of used features: 59
2023-10-31 10:55:45,962:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.482445 -> initscore=-0.070248
2023-10-31 10:55:45,962:INFO:[LightGBM] [Info] Start training from score -0.070248
2023-10-31 10:55:46,054:INFO:[LightGBM] [Info] Number of positive: 2380, number of negative: 2462
2023-10-31 10:55:46,056:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001335 seconds.
2023-10-31 10:55:46,056:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 10:55:46,056:INFO:[LightGBM] [Info] Total Bins 15045
2023-10-31 10:55:46,057:INFO:[LightGBM] [Info] Number of data points in the train set: 4842, number of used features: 59
2023-10-31 10:55:46,057:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.491532 -> initscore=-0.033874
2023-10-31 10:55:46,057:INFO:[LightGBM] [Info] Start training from score -0.033874
2023-10-31 10:55:46,103:INFO:[LightGBM] [Info] Number of positive: 2353, number of negative: 2488
2023-10-31 10:55:46,122:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000539 seconds.
2023-10-31 10:55:46,122:INFO:You can set `force_row_wise=true` to remove the overhead.
2023-10-31 10:55:46,122:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2023-10-31 10:55:46,122:INFO:[LightGBM] [Info] Total Bins 15045
2023-10-31 10:55:46,124:INFO:[LightGBM] [Info] Number of data points in the train set: 4841, number of used features: 59
2023-10-31 10:55:46,124:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.486057 -> initscore=-0.055788
2023-10-31 10:55:46,124:INFO:[LightGBM] [Info] Start training from score -0.055788
2023-10-31 10:55:46,240:INFO:[LightGBM] [Info] Number of positive: 2355, number of negative: 2487
2023-10-31 10:55:46,241:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001309 seconds.
2023-10-31 10:55:46,241:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 10:55:46,241:INFO:[LightGBM] [Info] Total Bins 15045
2023-10-31 10:55:46,252:INFO:[LightGBM] [Info] Number of data points in the train set: 4842, number of used features: 59
2023-10-31 10:55:46,262:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.486369 -> initscore=-0.054536
2023-10-31 10:55:46,263:INFO:[LightGBM] [Info] Start training from score -0.054536
2023-10-31 10:55:47,343:INFO:[LightGBM] [Info] Number of positive: 2362, number of negative: 2480
2023-10-31 10:55:47,344:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001290 seconds.
2023-10-31 10:55:47,344:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 10:55:47,345:INFO:[LightGBM] [Info] Total Bins 15045
2023-10-31 10:55:47,345:INFO:[LightGBM] [Info] Number of data points in the train set: 4842, number of used features: 59
2023-10-31 10:55:47,345:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.487815 -> initscore=-0.048750
2023-10-31 10:55:47,345:INFO:[LightGBM] [Info] Start training from score -0.048750
2023-10-31 10:55:47,502:INFO:[LightGBM] [Info] Number of positive: 2365, number of negative: 2477
2023-10-31 10:55:47,520:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017280 seconds.
2023-10-31 10:55:47,520:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 10:55:47,520:INFO:[LightGBM] [Info] Total Bins 15045
2023-10-31 10:55:47,520:INFO:[LightGBM] [Info] Number of data points in the train set: 4842, number of used features: 59
2023-10-31 10:55:47,520:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.488435 -> initscore=-0.046270
2023-10-31 10:55:47,520:INFO:[LightGBM] [Info] Start training from score -0.046270
2023-10-31 10:55:47,615:INFO:[LightGBM] [Info] Number of positive: 2343, number of negative: 2499
2023-10-31 10:55:47,617:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001301 seconds.
2023-10-31 10:55:47,617:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 10:55:47,617:INFO:[LightGBM] [Info] Total Bins 15045
2023-10-31 10:55:47,617:INFO:[LightGBM] [Info] Number of data points in the train set: 4842, number of used features: 59
2023-10-31 10:55:47,618:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.483891 -> initscore=-0.064458
2023-10-31 10:55:47,618:INFO:[LightGBM] [Info] Start training from score -0.064458
2023-10-31 10:55:47,815:INFO:[LightGBM] [Info] Number of positive: 2337, number of negative: 2505
2023-10-31 10:55:47,816:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001291 seconds.
2023-10-31 10:55:47,816:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 10:55:47,816:INFO:[LightGBM] [Info] Total Bins 15045
2023-10-31 10:55:47,817:INFO:[LightGBM] [Info] Number of data points in the train set: 4842, number of used features: 59
2023-10-31 10:55:47,817:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.482652 -> initscore=-0.069421
2023-10-31 10:55:47,817:INFO:[LightGBM] [Info] Start training from score -0.069421
2023-10-31 10:55:48,066:INFO:[LightGBM] [Info] Number of positive: 2356, number of negative: 2486
2023-10-31 10:55:48,067:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001343 seconds.
2023-10-31 10:55:48,067:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 10:55:48,067:INFO:[LightGBM] [Info] Total Bins 15045
2023-10-31 10:55:48,068:INFO:[LightGBM] [Info] Number of data points in the train set: 4842, number of used features: 59
2023-10-31 10:55:48,068:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.486576 -> initscore=-0.053710
2023-10-31 10:55:48,068:INFO:[LightGBM] [Info] Start training from score -0.053710
2023-10-31 10:55:48,154:INFO:[LightGBM] [Info] Number of positive: 2354, number of negative: 2488
2023-10-31 10:55:48,156:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001356 seconds.
2023-10-31 10:55:48,156:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 10:55:48,156:INFO:[LightGBM] [Info] Total Bins 15045
2023-10-31 10:55:48,156:INFO:[LightGBM] [Info] Number of data points in the train set: 4842, number of used features: 59
2023-10-31 10:55:48,160:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.486163 -> initscore=-0.055363
2023-10-31 10:55:48,160:INFO:[LightGBM] [Info] Start training from score -0.055363
2023-10-31 10:55:48,196:INFO:[LightGBM] [Info] Number of positive: 2355, number of negative: 2487
2023-10-31 10:55:48,201:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002099 seconds.
2023-10-31 10:55:48,201:INFO:You can set `force_row_wise=true` to remove the overhead.
2023-10-31 10:55:48,201:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2023-10-31 10:55:48,201:INFO:[LightGBM] [Info] Total Bins 15045
2023-10-31 10:55:48,202:INFO:[LightGBM] [Info] Number of data points in the train set: 4842, number of used features: 59
2023-10-31 10:55:48,202:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.486369 -> initscore=-0.054536
2023-10-31 10:55:48,202:INFO:[LightGBM] [Info] Start training from score -0.054536
2023-10-31 10:55:49,300:INFO:[LightGBM] [Info] Number of positive: 2362, number of negative: 2481
2023-10-31 10:55:49,300:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001334 seconds.
2023-10-31 10:55:49,300:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 10:55:49,300:INFO:[LightGBM] [Info] Total Bins 15045
2023-10-31 10:55:49,300:INFO:[LightGBM] [Info] Number of data points in the train set: 4843, number of used features: 59
2023-10-31 10:55:49,300:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.487714 -> initscore=-0.049153
2023-10-31 10:55:49,300:INFO:[LightGBM] [Info] Start training from score -0.049153
2023-10-31 10:55:49,795:INFO:[LightGBM] [Info] Number of positive: 2357, number of negative: 2485
2023-10-31 10:55:49,797:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001301 seconds.
2023-10-31 10:55:49,797:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 10:55:49,797:INFO:[LightGBM] [Info] Total Bins 15045
2023-10-31 10:55:49,798:INFO:[LightGBM] [Info] Number of data points in the train set: 4842, number of used features: 59
2023-10-31 10:55:49,798:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.486782 -> initscore=-0.052883
2023-10-31 10:55:49,798:INFO:[LightGBM] [Info] Start training from score -0.052883
2023-10-31 10:55:49,897:INFO:[LightGBM] [Info] Number of positive: 2343, number of negative: 2500
2023-10-31 10:55:49,909:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001354 seconds.
2023-10-31 10:55:49,909:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 10:55:49,909:INFO:[LightGBM] [Info] Total Bins 15045
2023-10-31 10:55:49,909:INFO:[LightGBM] [Info] Number of data points in the train set: 4843, number of used features: 59
2023-10-31 10:55:49,909:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.483791 -> initscore=-0.064859
2023-10-31 10:55:49,909:INFO:[LightGBM] [Info] Start training from score -0.064859
2023-10-31 10:55:49,934:INFO:[LightGBM] [Info] Number of positive: 2365, number of negative: 2478
2023-10-31 10:55:49,936:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001303 seconds.
2023-10-31 10:55:49,936:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 10:55:49,936:INFO:[LightGBM] [Info] Total Bins 15045
2023-10-31 10:55:49,937:INFO:[LightGBM] [Info] Number of data points in the train set: 4843, number of used features: 59
2023-10-31 10:55:49,937:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.488334 -> initscore=-0.046674
2023-10-31 10:55:49,937:INFO:[LightGBM] [Info] Start training from score -0.046674
2023-10-31 10:55:50,214:INFO:[LightGBM] [Info] Number of positive: 2337, number of negative: 2505
2023-10-31 10:55:50,214:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001334 seconds.
2023-10-31 10:55:50,214:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 10:55:50,214:INFO:[LightGBM] [Info] Total Bins 15045
2023-10-31 10:55:50,215:INFO:[LightGBM] [Info] Number of data points in the train set: 4842, number of used features: 59
2023-10-31 10:55:50,215:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.482652 -> initscore=-0.069421
2023-10-31 10:55:50,215:INFO:[LightGBM] [Info] Start training from score -0.069421
2023-10-31 10:55:50,284:INFO:[LightGBM] [Info] Number of positive: 2354, number of negative: 2488
2023-10-31 10:55:50,285:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001315 seconds.
2023-10-31 10:55:50,285:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 10:55:50,285:INFO:[LightGBM] [Info] Total Bins 15045
2023-10-31 10:55:50,286:INFO:[LightGBM] [Info] Number of data points in the train set: 4842, number of used features: 59
2023-10-31 10:55:50,286:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.486163 -> initscore=-0.055363
2023-10-31 10:55:50,286:INFO:[LightGBM] [Info] Start training from score -0.055363
2023-10-31 10:55:50,374:INFO:[LightGBM] [Info] Number of positive: 2355, number of negative: 2487
2023-10-31 10:55:50,374:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001307 seconds.
2023-10-31 10:55:50,374:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 10:55:50,374:INFO:[LightGBM] [Info] Total Bins 15045
2023-10-31 10:55:50,374:INFO:[LightGBM] [Info] Number of data points in the train set: 4842, number of used features: 59
2023-10-31 10:55:50,374:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.486369 -> initscore=-0.054536
2023-10-31 10:55:50,374:INFO:[LightGBM] [Info] Start training from score -0.054536
2023-10-31 10:55:51,302:INFO:[LightGBM] [Info] Number of positive: 2362, number of negative: 2481
2023-10-31 10:55:51,303:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001297 seconds.
2023-10-31 10:55:51,304:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 10:55:51,304:INFO:[LightGBM] [Info] Total Bins 15045
2023-10-31 10:55:51,304:INFO:[LightGBM] [Info] Number of data points in the train set: 4843, number of used features: 59
2023-10-31 10:55:51,304:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.487714 -> initscore=-0.049153
2023-10-31 10:55:51,304:INFO:[LightGBM] [Info] Start training from score -0.049153
2023-10-31 10:55:51,420:INFO:[LightGBM] [Info] Number of positive: 2355, number of negative: 2488
2023-10-31 10:55:51,421:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001345 seconds.
2023-10-31 10:55:51,421:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 10:55:51,421:INFO:[LightGBM] [Info] Total Bins 15045
2023-10-31 10:55:51,421:INFO:[LightGBM] [Info] Number of data points in the train set: 4843, number of used features: 59
2023-10-31 10:55:51,421:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.486269 -> initscore=-0.054938
2023-10-31 10:55:51,421:INFO:[LightGBM] [Info] Start training from score -0.054938
2023-10-31 10:55:51,426:INFO:[LightGBM] [Info] Number of positive: 2344, number of negative: 2499
2023-10-31 10:55:51,428:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001318 seconds.
2023-10-31 10:55:51,428:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 10:55:51,428:INFO:[LightGBM] [Info] Total Bins 15045
2023-10-31 10:55:51,428:INFO:[LightGBM] [Info] Number of data points in the train set: 4843, number of used features: 59
2023-10-31 10:55:51,428:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.483998 -> initscore=-0.064032
2023-10-31 10:55:51,428:INFO:[LightGBM] [Info] Start training from score -0.064032
2023-10-31 10:55:51,492:INFO:[LightGBM] [Info] Number of positive: 2357, number of negative: 2485
2023-10-31 10:55:51,493:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001299 seconds.
2023-10-31 10:55:51,494:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 10:55:51,494:INFO:[LightGBM] [Info] Total Bins 15045
2023-10-31 10:55:51,494:INFO:[LightGBM] [Info] Number of data points in the train set: 4842, number of used features: 59
2023-10-31 10:55:51,494:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.486782 -> initscore=-0.052883
2023-10-31 10:55:51,494:INFO:[LightGBM] [Info] Start training from score -0.052883
2023-10-31 10:55:51,648:INFO:[LightGBM] [Info] Number of positive: 2365, number of negative: 2478
2023-10-31 10:55:51,651:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001295 seconds.
2023-10-31 10:55:51,651:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 10:55:51,651:INFO:[LightGBM] [Info] Total Bins 15045
2023-10-31 10:55:51,651:INFO:[LightGBM] [Info] Number of data points in the train set: 4843, number of used features: 59
2023-10-31 10:55:51,651:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.488334 -> initscore=-0.046674
2023-10-31 10:55:51,651:INFO:[LightGBM] [Info] Start training from score -0.046674
2023-10-31 10:55:52,032:INFO:[LightGBM] [Info] Number of positive: 2337, number of negative: 2506
2023-10-31 10:55:52,033:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001327 seconds.
2023-10-31 10:55:52,033:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 10:55:52,033:INFO:[LightGBM] [Info] Total Bins 15045
2023-10-31 10:55:52,045:INFO:[LightGBM] [Info] Number of positive: 2354, number of negative: 2488
2023-10-31 10:55:52,046:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001298 seconds.
2023-10-31 10:55:52,046:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 10:55:52,047:INFO:[LightGBM] [Info] Total Bins 15045
2023-10-31 10:55:52,047:INFO:[LightGBM] [Info] Number of data points in the train set: 4842, number of used features: 59
2023-10-31 10:55:52,047:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.486163 -> initscore=-0.055363
2023-10-31 10:55:52,047:INFO:[LightGBM] [Info] Start training from score -0.055363
2023-10-31 10:55:52,057:INFO:[LightGBM] [Info] Number of data points in the train set: 4843, number of used features: 59
2023-10-31 10:55:52,057:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.482552 -> initscore=-0.069820
2023-10-31 10:55:52,057:INFO:[LightGBM] [Info] Start training from score -0.069820
2023-10-31 10:55:52,100:INFO:[LightGBM] [Info] Number of positive: 2356, number of negative: 2487
2023-10-31 10:55:52,102:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002082 seconds.
2023-10-31 10:55:52,102:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 10:55:52,102:INFO:[LightGBM] [Info] Total Bins 15045
2023-10-31 10:55:52,103:INFO:[LightGBM] [Info] Number of data points in the train set: 4843, number of used features: 59
2023-10-31 10:55:52,103:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.486475 -> initscore=-0.054112
2023-10-31 10:55:52,103:INFO:[LightGBM] [Info] Start training from score -0.054112
2023-10-31 10:55:53,144:INFO:[LightGBM] [Info] Number of positive: 2357, number of negative: 2486
2023-10-31 10:55:53,145:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001296 seconds.
2023-10-31 10:55:53,145:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 10:55:53,145:INFO:[LightGBM] [Info] Total Bins 15045
2023-10-31 10:55:53,145:INFO:[LightGBM] [Info] Number of data points in the train set: 4843, number of used features: 59
2023-10-31 10:55:53,145:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.486682 -> initscore=-0.053285
2023-10-31 10:55:53,145:INFO:[LightGBM] [Info] Start training from score -0.053285
2023-10-31 10:55:53,404:INFO:[LightGBM] [Info] Number of positive: 2337, number of negative: 2506
2023-10-31 10:55:53,405:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001333 seconds.
2023-10-31 10:55:53,405:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 10:55:53,430:INFO:[LightGBM] [Info] Total Bins 15045
2023-10-31 10:55:53,431:INFO:[LightGBM] [Info] Number of data points in the train set: 4843, number of used features: 59
2023-10-31 10:55:53,431:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.482552 -> initscore=-0.069820
2023-10-31 10:55:53,431:INFO:[LightGBM] [Info] Start training from score -0.069820
2023-10-31 10:55:54,333:INFO:[LightGBM] [Info] Number of positive: 2357, number of negative: 2486
2023-10-31 10:55:54,335:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001347 seconds.
2023-10-31 10:55:54,335:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 10:55:54,335:INFO:[LightGBM] [Info] Total Bins 15045
2023-10-31 10:55:54,335:INFO:[LightGBM] [Info] Number of data points in the train set: 4843, number of used features: 59
2023-10-31 10:55:54,335:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.486682 -> initscore=-0.053285
2023-10-31 10:55:54,335:INFO:[LightGBM] [Info] Start training from score -0.053285
2023-10-31 10:55:58,473:INFO:[LightGBM] [Info] Number of positive: 2940, number of negative: 3113
2023-10-31 10:55:58,474:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001644 seconds.
2023-10-31 10:55:58,475:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 10:55:58,475:INFO:[LightGBM] [Info] Total Bins 15045
2023-10-31 10:55:58,475:INFO:[LightGBM] [Info] Number of data points in the train set: 6053, number of used features: 59
2023-10-31 10:55:58,475:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.485710 -> initscore=-0.057177
2023-10-31 10:55:58,475:INFO:[LightGBM] [Info] Start training from score -0.057177
2023-10-31 10:56:17,470:INFO:[LightGBM] [Info] Number of positive: 2352, number of negative: 2490
2023-10-31 10:56:17,472:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001314 seconds.
2023-10-31 10:56:17,472:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 10:56:17,472:INFO:[LightGBM] [Info] Total Bins 15045
2023-10-31 10:56:17,473:INFO:[LightGBM] [Info] Number of data points in the train set: 4842, number of used features: 59
2023-10-31 10:56:17,473:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.485750 -> initscore=-0.057017
2023-10-31 10:56:17,473:INFO:[LightGBM] [Info] Start training from score -0.057017
2023-10-31 10:56:18,037:INFO:[LightGBM] [Info] Number of positive: 2352, number of negative: 2490
2023-10-31 10:56:18,039:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001392 seconds.
2023-10-31 10:56:18,039:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 10:56:18,039:INFO:[LightGBM] [Info] Total Bins 15045
2023-10-31 10:56:18,040:INFO:[LightGBM] [Info] Number of data points in the train set: 4842, number of used features: 59
2023-10-31 10:56:18,040:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.485750 -> initscore=-0.057017
2023-10-31 10:56:18,040:INFO:[LightGBM] [Info] Start training from score -0.057017
2023-10-31 10:56:18,460:INFO:[LightGBM] [Info] Number of positive: 2352, number of negative: 2490
2023-10-31 10:56:18,461:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001328 seconds.
2023-10-31 10:56:18,461:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 10:56:18,461:INFO:[LightGBM] [Info] Total Bins 15045
2023-10-31 10:56:18,462:INFO:[LightGBM] [Info] Number of data points in the train set: 4842, number of used features: 59
2023-10-31 10:56:18,462:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.485750 -> initscore=-0.057017
2023-10-31 10:56:18,462:INFO:[LightGBM] [Info] Start training from score -0.057017
2023-10-31 10:56:18,906:INFO:[LightGBM] [Info] Number of positive: 2352, number of negative: 2491
2023-10-31 10:56:18,907:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001313 seconds.
2023-10-31 10:56:18,907:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 10:56:18,908:INFO:[LightGBM] [Info] Total Bins 15045
2023-10-31 10:56:18,908:INFO:[LightGBM] [Info] Number of data points in the train set: 4843, number of used features: 59
2023-10-31 10:56:18,908:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.485649 -> initscore=-0.057418
2023-10-31 10:56:18,908:INFO:[LightGBM] [Info] Start training from score -0.057418
2023-10-31 10:56:19,391:INFO:[LightGBM] [Info] Number of positive: 2352, number of negative: 2491
2023-10-31 10:56:19,394:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001934 seconds.
2023-10-31 10:56:19,394:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 10:56:19,394:INFO:[LightGBM] [Info] Total Bins 15045
2023-10-31 10:56:19,396:INFO:[LightGBM] [Info] Number of data points in the train set: 4843, number of used features: 59
2023-10-31 10:56:19,396:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.485649 -> initscore=-0.057418
2023-10-31 10:56:19,396:INFO:[LightGBM] [Info] Start training from score -0.057418
2023-10-31 10:57:13,292:INFO:Calculating mean and std
2023-10-31 10:57:13,316:INFO:Creating metrics dataframe
2023-10-31 10:57:13,350:INFO:Finalizing model
2023-10-31 11:06:12,364:INFO:[LightGBM] [Info] Number of positive: 3312, number of negative: 3414
2023-10-31 11:06:12,625:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.259023 seconds.
2023-10-31 11:06:12,626:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 11:06:12,633:INFO:[LightGBM] [Info] Total Bins 1409640
2023-10-31 11:06:12,686:INFO:[LightGBM] [Info] Number of data points in the train set: 6726, number of used features: 5528
2023-10-31 11:06:12,692:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.492417 -> initscore=-0.030332
2023-10-31 11:06:12,692:INFO:[LightGBM] [Info] Start training from score -0.030332
2023-10-31 11:06:48,027:INFO:[LightGBM] [Info] Number of positive: 3312, number of negative: 3414
2023-10-31 11:06:48,028:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001799 seconds.
2023-10-31 11:06:48,029:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 11:06:48,029:INFO:[LightGBM] [Info] Total Bins 15045
2023-10-31 11:06:48,029:INFO:[LightGBM] [Info] Number of data points in the train set: 6726, number of used features: 59
2023-10-31 11:06:48,029:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.492417 -> initscore=-0.030332
2023-10-31 11:06:48,030:INFO:[LightGBM] [Info] Start training from score -0.030332
2023-10-31 11:06:51,288:INFO:[LightGBM] [Info] Number of positive: 2650, number of negative: 2731
2023-10-31 11:06:51,288:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001469 seconds.
2023-10-31 11:06:51,288:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 11:06:51,288:INFO:[LightGBM] [Info] Total Bins 15045
2023-10-31 11:06:51,288:INFO:[LightGBM] [Info] Number of positive: 2650, number of negative: 2731
2023-10-31 11:06:51,288:INFO:[LightGBM] [Info] Number of positive: 2650, number of negative: 2731
2023-10-31 11:06:51,288:INFO:[LightGBM] [Info] Number of data points in the train set: 5381, number of used features: 59
2023-10-31 11:06:51,288:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.492474 -> initscore=-0.030108
2023-10-31 11:06:51,288:INFO:[LightGBM] [Info] Start training from score -0.030108
2023-10-31 11:06:51,288:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001496 seconds.
2023-10-31 11:06:51,288:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 11:06:51,288:INFO:[LightGBM] [Info] Total Bins 15045
2023-10-31 11:06:51,288:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001534 seconds.
2023-10-31 11:06:51,288:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 11:06:51,288:INFO:[LightGBM] [Info] Total Bins 15045
2023-10-31 11:06:51,288:INFO:[LightGBM] [Info] Number of data points in the train set: 5381, number of used features: 59
2023-10-31 11:06:51,288:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.492474 -> initscore=-0.030108
2023-10-31 11:06:51,288:INFO:[LightGBM] [Info] Start training from score -0.030108
2023-10-31 11:06:51,288:INFO:[LightGBM] [Info] Number of data points in the train set: 5381, number of used features: 59
2023-10-31 11:06:51,288:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.492474 -> initscore=-0.030108
2023-10-31 11:06:51,288:INFO:[LightGBM] [Info] Start training from score -0.030108
2023-10-31 11:06:51,290:INFO:[LightGBM] [Info] Number of positive: 2649, number of negative: 2732
2023-10-31 11:06:51,290:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000792 seconds.
2023-10-31 11:06:51,290:INFO:You can set `force_row_wise=true` to remove the overhead.
2023-10-31 11:06:51,290:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2023-10-31 11:06:51,291:INFO:[LightGBM] [Info] Total Bins 15045
2023-10-31 11:06:51,299:INFO:[LightGBM] [Info] Number of data points in the train set: 5381, number of used features: 59
2023-10-31 11:06:51,299:INFO:[LightGBM] [Info] Number of positive: 2649, number of negative: 2731
2023-10-31 11:06:51,299:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.492288 -> initscore=-0.030852
2023-10-31 11:06:51,299:INFO:[LightGBM] [Info] Start training from score -0.030852
2023-10-31 11:06:51,299:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006179 seconds.
2023-10-31 11:06:51,299:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 11:06:51,299:INFO:[LightGBM] [Info] Total Bins 15045
2023-10-31 11:06:51,307:INFO:[LightGBM] [Info] Number of data points in the train set: 5380, number of used features: 59
2023-10-31 11:06:51,308:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.492379 -> initscore=-0.030486
2023-10-31 11:06:51,308:INFO:[LightGBM] [Info] Start training from score -0.030486
2023-10-31 11:06:59,610:INFO:Uploading results into container
2023-10-31 11:06:59,610:INFO:Uploading model into container now
2023-10-31 11:06:59,611:INFO:_master_model_container: 32
2023-10-31 11:06:59,611:INFO:_display_container: 14
2023-10-31 11:06:59,615:INFO:StackingClassifier(cv=5,
                   estimators=[('Extra Trees Classifier',
                                ExtraTreesClassifier(bootstrap=False,
                                                     ccp_alpha=0.0,
                                                     class_weight=None,
                                                     criterion='gini',
                                                     max_depth=None,
                                                     max_features='sqrt',
                                                     max_leaf_nodes=None,
                                                     max_samples=None,
                                                     min_impurity_decrease=0.0,
                                                     min_samples_leaf=1,
                                                     min_samples_split=2,
                                                     min_weight_fraction_leaf=0.0,
                                                     n_estimators=100,
                                                     n_jobs=-1, oob_score=F...
                                                        ccp_alpha=0.0,
                                                        class_weight=None,
                                                        criterion='gini',
                                                        max_depth=None,
                                                        max_features='sqrt',
                                                        max_leaf_nodes=None,
                                                        max_samples=None,
                                                        min_impurity_decrease=0.0,
                                                        min_samples_leaf=1,
                                                        min_samples_split=2,
                                                        min_weight_fraction_leaf=0.0,
                                                        n_estimators=100,
                                                        n_jobs=-1,
                                                        oob_score=False,
                                                        random_state=1784,
                                                        verbose=0,
                                                        warm_start=False),
                   n_jobs=-1, passthrough=True, stack_method='auto', verbose=0)
2023-10-31 11:06:59,615:INFO:create_model() successfully completed......................................
2023-10-31 11:06:59,897:INFO:SubProcess create_model() end ==================================
2023-10-31 11:06:59,904:INFO:_master_model_container: 32
2023-10-31 11:06:59,904:INFO:_display_container: 14
2023-10-31 11:06:59,907:INFO:StackingClassifier(cv=5,
                   estimators=[('Extra Trees Classifier',
                                ExtraTreesClassifier(bootstrap=False,
                                                     ccp_alpha=0.0,
                                                     class_weight=None,
                                                     criterion='gini',
                                                     max_depth=None,
                                                     max_features='sqrt',
                                                     max_leaf_nodes=None,
                                                     max_samples=None,
                                                     min_impurity_decrease=0.0,
                                                     min_samples_leaf=1,
                                                     min_samples_split=2,
                                                     min_weight_fraction_leaf=0.0,
                                                     n_estimators=100,
                                                     n_jobs=-1, oob_score=F...
                                                        ccp_alpha=0.0,
                                                        class_weight=None,
                                                        criterion='gini',
                                                        max_depth=None,
                                                        max_features='sqrt',
                                                        max_leaf_nodes=None,
                                                        max_samples=None,
                                                        min_impurity_decrease=0.0,
                                                        min_samples_leaf=1,
                                                        min_samples_split=2,
                                                        min_weight_fraction_leaf=0.0,
                                                        n_estimators=100,
                                                        n_jobs=-1,
                                                        oob_score=False,
                                                        random_state=1784,
                                                        verbose=0,
                                                        warm_start=False),
                   n_jobs=-1, passthrough=True, stack_method='auto', verbose=0)
2023-10-31 11:06:59,907:INFO:stack_models() successfully completed......................................
2023-10-31 11:16:14,953:INFO:Initializing plot_model()
2023-10-31 11:16:14,953:INFO:plot_model(plot=boundary, fold=None, verbose=True, display=None, display_format=None, estimator=StackingClassifier(cv=5,
                   estimators=[('Extra Trees Classifier',
                                ExtraTreesClassifier(bootstrap=False,
                                                     ccp_alpha=0.0,
                                                     class_weight=None,
                                                     criterion='gini',
                                                     max_depth=None,
                                                     max_features='sqrt',
                                                     max_leaf_nodes=None,
                                                     max_samples=None,
                                                     min_impurity_decrease=0.0,
                                                     min_samples_leaf=1,
                                                     min_samples_split=2,
                                                     min_weight_fraction_leaf=0.0,
                                                     n_estimators=100,
                                                     n_jobs=-1, oob_score=F...
                                                        ccp_alpha=0.0,
                                                        class_weight=None,
                                                        criterion='gini',
                                                        max_depth=None,
                                                        max_features='sqrt',
                                                        max_leaf_nodes=None,
                                                        max_samples=None,
                                                        min_impurity_decrease=0.0,
                                                        min_samples_leaf=1,
                                                        min_samples_split=2,
                                                        min_weight_fraction_leaf=0.0,
                                                        n_estimators=100,
                                                        n_jobs=-1,
                                                        oob_score=False,
                                                        random_state=1784,
                                                        verbose=0,
                                                        warm_start=False),
                   n_jobs=-1, passthrough=True, stack_method='auto', verbose=0), feature_name=None, fit_kwargs=None, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.classification.oop.ClassificationExperiment object at 0x2a279fcd0>, system=True)
2023-10-31 11:16:14,953:INFO:Checking exceptions
2023-10-31 11:16:14,984:INFO:Preloading libraries
2023-10-31 11:16:15,037:INFO:Copying training dataset
2023-10-31 11:16:15,037:INFO:Plot type: boundary
2023-10-31 11:16:15,792:INFO:Fitting StandardScaler()
2023-10-31 11:16:15,798:INFO:Fitting PCA()
2023-10-31 11:16:20,940:INFO:Fitting Model
2023-10-31 11:16:22,073:INFO:[LightGBM] [Info] Number of positive: 3312, number of negative: 3414
2023-10-31 11:16:22,073:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000078 seconds.
2023-10-31 11:16:22,073:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 11:16:22,073:INFO:[LightGBM] [Info] Total Bins 510
2023-10-31 11:16:22,073:INFO:[LightGBM] [Info] Number of data points in the train set: 6726, number of used features: 2
2023-10-31 11:16:22,073:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.492417 -> initscore=-0.030332
2023-10-31 11:16:22,073:INFO:[LightGBM] [Info] Start training from score -0.030332
2023-10-31 11:16:23,756:INFO:[LightGBM] [Info] Number of positive: 2649, number of negative: 2731
2023-10-31 11:16:23,757:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000061 seconds.
2023-10-31 11:16:23,757:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 11:16:23,757:INFO:[LightGBM] [Info] Total Bins 510
2023-10-31 11:16:23,757:INFO:[LightGBM] [Info] Number of data points in the train set: 5380, number of used features: 2
2023-10-31 11:16:23,757:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.492379 -> initscore=-0.030486
2023-10-31 11:16:23,757:INFO:[LightGBM] [Info] Start training from score -0.030486
2023-10-31 11:16:23,757:INFO:[LightGBM] [Info] Number of positive: 2649, number of negative: 2732
2023-10-31 11:16:23,757:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000057 seconds.
2023-10-31 11:16:23,757:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 11:16:23,757:INFO:[LightGBM] [Info] Total Bins 510
2023-10-31 11:16:23,757:INFO:[LightGBM] [Info] Number of data points in the train set: 5381, number of used features: 2
2023-10-31 11:16:23,757:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.492288 -> initscore=-0.030852
2023-10-31 11:16:23,757:INFO:[LightGBM] [Info] Start training from score -0.030852
2023-10-31 11:16:23,760:INFO:[LightGBM] [Info] Number of positive: 2650, number of negative: 2731
2023-10-31 11:16:23,760:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000060 seconds.
2023-10-31 11:16:23,760:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 11:16:23,760:INFO:[LightGBM] [Info] Total Bins 510
2023-10-31 11:16:23,760:INFO:[LightGBM] [Info] Number of data points in the train set: 5381, number of used features: 2
2023-10-31 11:16:23,760:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.492474 -> initscore=-0.030108
2023-10-31 11:16:23,760:INFO:[LightGBM] [Info] Start training from score -0.030108
2023-10-31 11:16:23,761:INFO:[LightGBM] [Info] Number of positive: 2650, number of negative: 2731
2023-10-31 11:16:23,761:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000059 seconds.
2023-10-31 11:16:23,761:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 11:16:23,761:INFO:[LightGBM] [Info] Total Bins 510
2023-10-31 11:16:23,761:INFO:[LightGBM] [Info] Number of data points in the train set: 5381, number of used features: 2
2023-10-31 11:16:23,761:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.492474 -> initscore=-0.030108
2023-10-31 11:16:23,761:INFO:[LightGBM] [Info] Start training from score -0.030108
2023-10-31 11:16:23,762:INFO:[LightGBM] [Info] Number of positive: 2650, number of negative: 2731
2023-10-31 11:16:23,762:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000058 seconds.
2023-10-31 11:16:23,762:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 11:16:23,762:INFO:[LightGBM] [Info] Total Bins 510
2023-10-31 11:16:23,762:INFO:[LightGBM] [Info] Number of data points in the train set: 5381, number of used features: 2
2023-10-31 11:16:23,765:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.492474 -> initscore=-0.030108
2023-10-31 11:16:23,765:INFO:[LightGBM] [Info] Start training from score -0.030108
2023-10-31 11:16:27,289:INFO:Visual Rendered Successfully
2023-10-31 11:16:27,402:INFO:plot_model() successfully completed......................................
2023-10-31 11:18:35,731:INFO:Initializing plot_model()
2023-10-31 11:18:35,732:INFO:plot_model(plot=auc, fold=None, verbose=True, display=None, display_format=None, estimator=StackingClassifier(cv=5,
                   estimators=[('Extra Trees Classifier',
                                ExtraTreesClassifier(bootstrap=False,
                                                     ccp_alpha=0.0,
                                                     class_weight=None,
                                                     criterion='gini',
                                                     max_depth=None,
                                                     max_features='sqrt',
                                                     max_leaf_nodes=None,
                                                     max_samples=None,
                                                     min_impurity_decrease=0.0,
                                                     min_samples_leaf=1,
                                                     min_samples_split=2,
                                                     min_weight_fraction_leaf=0.0,
                                                     n_estimators=100,
                                                     n_jobs=-1, oob_score=F...
                                                        ccp_alpha=0.0,
                                                        class_weight=None,
                                                        criterion='gini',
                                                        max_depth=None,
                                                        max_features='sqrt',
                                                        max_leaf_nodes=None,
                                                        max_samples=None,
                                                        min_impurity_decrease=0.0,
                                                        min_samples_leaf=1,
                                                        min_samples_split=2,
                                                        min_weight_fraction_leaf=0.0,
                                                        n_estimators=100,
                                                        n_jobs=-1,
                                                        oob_score=False,
                                                        random_state=1784,
                                                        verbose=0,
                                                        warm_start=False),
                   n_jobs=-1, passthrough=True, stack_method='auto', verbose=0), feature_name=None, fit_kwargs=None, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.classification.oop.ClassificationExperiment object at 0x2a279fcd0>, system=True)
2023-10-31 11:18:35,732:INFO:Checking exceptions
2023-10-31 11:18:35,741:INFO:Preloading libraries
2023-10-31 11:18:35,784:INFO:Copying training dataset
2023-10-31 11:18:35,784:INFO:Plot type: auc
2023-10-31 11:18:36,507:INFO:Fitting Model
2023-10-31 11:18:36,508:WARNING:X does not have valid feature names, but ExtraTreesClassifier was fitted with feature names

2023-10-31 11:18:36,508:INFO:Scoring test/hold-out set
2023-10-31 11:18:37,008:INFO:Visual Rendered Successfully
2023-10-31 11:18:37,096:INFO:plot_model() successfully completed......................................
2023-10-31 13:15:42,887:INFO:Initializing blend_models()
2023-10-31 13:15:42,887:INFO:blend_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x2a279fcd0>, estimator_list=[ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     n_estimators=100, n_jobs=-1, oob_score=False,
                     random_state=1784, verbose=0, warm_start=False), RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=100, n_jobs=-1, oob_score=False,
                       random_state=1784, verbose=0, warm_start=False), QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001), <catboost.core.CatBoostClassifier object at 0x2b25da2f0>, LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=1784, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)], fold=None, round=4, choose_better=False, optimize=AUC, method=soft, weights=None, fit_kwargs=None, groups=None, probability_threshold=None, verbose=True, return_train_score=False)
2023-10-31 13:15:42,887:INFO:Checking exceptions
2023-10-31 13:15:42,903:INFO:Importing libraries
2023-10-31 13:15:42,903:INFO:Copying training dataset
2023-10-31 13:15:42,906:INFO:Getting model names
2023-10-31 13:15:42,909:INFO:SubProcess create_model() called ==================================
2023-10-31 13:15:42,912:INFO:Initializing create_model()
2023-10-31 13:15:42,912:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x2a279fcd0>, estimator=VotingClassifier(estimators=[('Extra Trees Classifier',
                              ExtraTreesClassifier(bootstrap=False,
                                                   ccp_alpha=0.0,
                                                   class_weight=None,
                                                   criterion='gini',
                                                   max_depth=None,
                                                   max_features='sqrt',
                                                   max_leaf_nodes=None,
                                                   max_samples=None,
                                                   min_impurity_decrease=0.0,
                                                   min_samples_leaf=1,
                                                   min_samples_split=2,
                                                   min_weight_fraction_leaf=0.0,
                                                   n_estimators=100, n_jobs=-1,
                                                   oob_score=False,
                                                   ra...
                                             importance_type='split',
                                             learning_rate=0.1, max_depth=-1,
                                             min_child_samples=20,
                                             min_child_weight=0.001,
                                             min_split_gain=0.0,
                                             n_estimators=100, n_jobs=-1,
                                             num_leaves=31, objective=None,
                                             random_state=1784, reg_alpha=0.0,
                                             reg_lambda=0.0, subsample=1.0,
                                             subsample_for_bin=200000,
                                             subsample_freq=0))],
                 flatten_transform=True, n_jobs=-1, verbose=False,
                 voting='soft', weights=None), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x2b25e7c40>, model_only=True, return_train_score=False, kwargs={})
2023-10-31 13:15:42,912:INFO:Checking exceptions
2023-10-31 13:15:42,912:INFO:Importing libraries
2023-10-31 13:15:42,912:INFO:Copying training dataset
2023-10-31 13:15:42,926:INFO:Defining folds
2023-10-31 13:15:42,927:INFO:Declaring metric variables
2023-10-31 13:15:42,929:INFO:Importing untrained model
2023-10-31 13:15:42,929:INFO:Declaring custom model
2023-10-31 13:15:42,932:INFO:Voting Classifier Imported successfully
2023-10-31 13:15:42,937:INFO:Starting cross validation
2023-10-31 13:15:42,970:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-10-31 13:17:20,435:INFO:[LightGBM] [Info] Number of positive: 2954, number of negative: 3099
2023-10-31 13:17:21,171:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.739126 seconds.
2023-10-31 13:17:21,172:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 13:17:21,179:INFO:[LightGBM] [Info] Total Bins 1290810
2023-10-31 13:17:21,279:INFO:[LightGBM] [Info] Number of data points in the train set: 6053, number of used features: 5062
2023-10-31 13:17:21,300:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.488022 -> initscore=-0.047919
2023-10-31 13:17:21,301:INFO:[LightGBM] [Info] Start training from score -0.047919
2023-10-31 13:17:22,343:INFO:[LightGBM] [Info] Number of positive: 2921, number of negative: 3132
2023-10-31 13:17:23,024:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.679092 seconds.
2023-10-31 13:17:23,024:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 13:17:23,033:INFO:[LightGBM] [Info] Total Bins 1294890
2023-10-31 13:17:23,215:INFO:[LightGBM] [Info] Number of data points in the train set: 6053, number of used features: 5078
2023-10-31 13:17:23,216:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.482571 -> initscore=-0.069746
2023-10-31 13:17:23,216:INFO:[LightGBM] [Info] Start training from score -0.069746
2023-10-31 13:17:23,513:INFO:[LightGBM] [Info] Number of positive: 2975, number of negative: 3077
2023-10-31 13:17:24,300:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.783510 seconds.
2023-10-31 13:17:24,300:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 13:17:24,310:INFO:[LightGBM] [Info] Total Bins 1289280
2023-10-31 13:17:24,458:INFO:[LightGBM] [Info] Number of data points in the train set: 6052, number of used features: 5056
2023-10-31 13:17:24,471:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.491573 -> initscore=-0.033711
2023-10-31 13:17:24,472:INFO:[LightGBM] [Info] Start training from score -0.033711
2023-10-31 13:17:25,061:INFO:[LightGBM] [Info] Number of positive: 2952, number of negative: 3101
2023-10-31 13:17:25,280:INFO:[LightGBM] [Info] Number of positive: 2929, number of negative: 3124
2023-10-31 13:17:25,366:INFO:[LightGBM] [Info] Number of positive: 2944, number of negative: 3109
2023-10-31 13:17:25,631:INFO:[LightGBM] [Info] Number of positive: 2956, number of negative: 3097
2023-10-31 13:17:25,819:INFO:[LightGBM] [Info] Number of positive: 2942, number of negative: 3110
2023-10-31 13:17:25,901:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.618525 seconds.
2023-10-31 13:17:25,902:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 13:17:25,920:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.824631 seconds.
2023-10-31 13:17:25,920:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 13:17:25,921:INFO:[LightGBM] [Info] Total Bins 1291830
2023-10-31 13:17:25,921:INFO:[LightGBM] [Info] Total Bins 1286730
2023-10-31 13:17:26,277:INFO:[LightGBM] [Info] Number of data points in the train set: 6053, number of used features: 5066
2023-10-31 13:17:26,294:INFO:[LightGBM] [Info] Number of data points in the train set: 6053, number of used features: 5046
2023-10-31 13:17:26,316:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.483892 -> initscore=-0.064453
2023-10-31 13:17:26,316:INFO:[LightGBM] [Info] Start training from score -0.064453
2023-10-31 13:17:26,317:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.487692 -> initscore=-0.049242
2023-10-31 13:17:26,317:INFO:[LightGBM] [Info] Start training from score -0.049242
2023-10-31 13:17:26,317:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.930751 seconds.
2023-10-31 13:17:26,317:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 13:17:26,325:INFO:[LightGBM] [Info] Total Bins 1292085
2023-10-31 13:17:26,401:INFO:[LightGBM] [Info] Number of positive: 2946, number of negative: 3107
2023-10-31 13:17:26,498:INFO:[LightGBM] [Info] Number of data points in the train set: 6053, number of used features: 5067
2023-10-31 13:17:26,509:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.486370 -> initscore=-0.054532
2023-10-31 13:17:26,509:INFO:[LightGBM] [Info] Start training from score -0.054532
2023-10-31 13:17:26,611:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.959721 seconds.
2023-10-31 13:17:26,611:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 13:17:26,624:INFO:[LightGBM] [Info] Total Bins 1289790
2023-10-31 13:17:26,811:INFO:[LightGBM] [Info] Number of data points in the train set: 6053, number of used features: 5058
2023-10-31 13:17:26,828:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.488353 -> initscore=-0.046597
2023-10-31 13:17:26,829:INFO:[LightGBM] [Info] Start training from score -0.046597
2023-10-31 13:17:27,106:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 1.257144 seconds.
2023-10-31 13:17:27,109:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 13:17:27,126:INFO:[LightGBM] [Info] Total Bins 1294635
2023-10-31 13:17:27,454:INFO:[LightGBM] [Info] Number of data points in the train set: 6052, number of used features: 5077
2023-10-31 13:17:27,477:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 1.038181 seconds.
2023-10-31 13:17:27,479:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 13:17:27,483:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.486120 -> initscore=-0.055533
2023-10-31 13:17:27,484:INFO:[LightGBM] [Info] Start training from score -0.055533
2023-10-31 13:17:27,501:INFO:[LightGBM] [Info] Total Bins 1294890
2023-10-31 13:17:27,746:INFO:[LightGBM] [Info] Number of data points in the train set: 6053, number of used features: 5078
2023-10-31 13:17:27,769:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.486701 -> initscore=-0.053209
2023-10-31 13:17:27,769:INFO:[LightGBM] [Info] Start training from score -0.053209
2023-10-31 13:19:39,526:INFO:[LightGBM] [Info] Number of positive: 2940, number of negative: 3113
2023-10-31 13:19:40,241:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.704483 seconds.
2023-10-31 13:19:40,246:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 13:19:40,251:INFO:[LightGBM] [Info] Total Bins 1291830
2023-10-31 13:19:40,543:INFO:[LightGBM] [Info] Number of data points in the train set: 6053, number of used features: 5066
2023-10-31 13:19:40,557:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.485710 -> initscore=-0.057177
2023-10-31 13:19:40,558:INFO:[LightGBM] [Info] Start training from score -0.057177
2023-10-31 13:20:13,750:INFO:[LightGBM] [Info] Number of positive: 2954, number of negative: 3099
2023-10-31 13:20:13,758:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004358 seconds.
2023-10-31 13:20:13,759:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 13:20:13,759:INFO:[LightGBM] [Info] Total Bins 15045
2023-10-31 13:20:13,760:INFO:[LightGBM] [Info] Number of data points in the train set: 6053, number of used features: 59
2023-10-31 13:20:13,760:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.488022 -> initscore=-0.047919
2023-10-31 13:20:13,760:INFO:[LightGBM] [Info] Start training from score -0.047919
2023-10-31 13:20:14,432:INFO:[LightGBM] [Info] Number of positive: 2921, number of negative: 3132
2023-10-31 13:20:14,462:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.028961 seconds.
2023-10-31 13:20:14,462:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 13:20:14,462:INFO:[LightGBM] [Info] Total Bins 15045
2023-10-31 13:20:14,463:INFO:[LightGBM] [Info] Number of data points in the train set: 6053, number of used features: 59
2023-10-31 13:20:14,463:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.482571 -> initscore=-0.069746
2023-10-31 13:20:14,463:INFO:[LightGBM] [Info] Start training from score -0.069746
2023-10-31 13:20:16,599:INFO:[LightGBM] [Info] Number of positive: 2975, number of negative: 3077
2023-10-31 13:20:16,633:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.031893 seconds.
2023-10-31 13:20:16,633:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 13:20:16,633:INFO:[LightGBM] [Info] Total Bins 15045
2023-10-31 13:20:16,633:INFO:[LightGBM] [Info] Number of data points in the train set: 6052, number of used features: 59
2023-10-31 13:20:16,633:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.491573 -> initscore=-0.033711
2023-10-31 13:20:16,633:INFO:[LightGBM] [Info] Start training from score -0.033711
2023-10-31 13:20:17,862:INFO:[LightGBM] [Info] Number of positive: 2952, number of negative: 3101
2023-10-31 13:20:17,864:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001623 seconds.
2023-10-31 13:20:17,864:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 13:20:17,864:INFO:[LightGBM] [Info] Total Bins 15045
2023-10-31 13:20:17,864:INFO:[LightGBM] [Info] Number of positive: 2929, number of negative: 3124
2023-10-31 13:20:17,865:INFO:[LightGBM] [Info] Number of data points in the train set: 6053, number of used features: 59
2023-10-31 13:20:17,865:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.487692 -> initscore=-0.049242
2023-10-31 13:20:17,865:INFO:[LightGBM] [Info] Start training from score -0.049242
2023-10-31 13:20:17,869:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001674 seconds.
2023-10-31 13:20:17,871:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 13:20:17,871:INFO:[LightGBM] [Info] Total Bins 15045
2023-10-31 13:20:17,871:INFO:[LightGBM] [Info] Number of data points in the train set: 6053, number of used features: 59
2023-10-31 13:20:17,871:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.483892 -> initscore=-0.064453
2023-10-31 13:20:17,871:INFO:[LightGBM] [Info] Start training from score -0.064453
2023-10-31 13:20:18,997:INFO:[LightGBM] [Info] Number of positive: 2944, number of negative: 3109
2023-10-31 13:20:18,999:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007150 seconds.
2023-10-31 13:20:18,999:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 13:20:18,999:INFO:[LightGBM] [Info] Total Bins 15045
2023-10-31 13:20:18,999:INFO:[LightGBM] [Info] Number of data points in the train set: 6053, number of used features: 59
2023-10-31 13:20:19,003:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.486370 -> initscore=-0.054532
2023-10-31 13:20:19,005:INFO:[LightGBM] [Info] Start training from score -0.054532
2023-10-31 13:20:19,578:INFO:[LightGBM] [Info] Number of positive: 2956, number of negative: 3097
2023-10-31 13:20:19,580:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001601 seconds.
2023-10-31 13:20:19,580:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 13:20:19,580:INFO:[LightGBM] [Info] Total Bins 15045
2023-10-31 13:20:19,581:INFO:[LightGBM] [Info] Number of data points in the train set: 6053, number of used features: 59
2023-10-31 13:20:19,604:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.488353 -> initscore=-0.046597
2023-10-31 13:20:19,606:INFO:[LightGBM] [Info] Start training from score -0.046597
2023-10-31 13:20:20,004:INFO:[LightGBM] [Info] Number of positive: 2946, number of negative: 3107
2023-10-31 13:20:20,005:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001649 seconds.
2023-10-31 13:20:20,005:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 13:20:20,005:INFO:[LightGBM] [Info] Total Bins 15045
2023-10-31 13:20:20,006:INFO:[LightGBM] [Info] Number of data points in the train set: 6053, number of used features: 59
2023-10-31 13:20:20,006:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.486701 -> initscore=-0.053209
2023-10-31 13:20:20,006:INFO:[LightGBM] [Info] Start training from score -0.053209
2023-10-31 13:20:20,062:INFO:[LightGBM] [Info] Number of positive: 2942, number of negative: 3110
2023-10-31 13:20:20,067:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008571 seconds.
2023-10-31 13:20:20,067:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 13:20:20,067:INFO:[LightGBM] [Info] Total Bins 15045
2023-10-31 13:20:20,067:INFO:[LightGBM] [Info] Number of data points in the train set: 6052, number of used features: 59
2023-10-31 13:20:20,069:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.486120 -> initscore=-0.055533
2023-10-31 13:20:20,069:INFO:[LightGBM] [Info] Start training from score -0.055533
2023-10-31 13:20:46,004:INFO:[LightGBM] [Info] Number of positive: 2940, number of negative: 3113
2023-10-31 13:20:46,006:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001920 seconds.
2023-10-31 13:20:46,007:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 13:20:46,007:INFO:[LightGBM] [Info] Total Bins 15045
2023-10-31 13:20:46,007:INFO:[LightGBM] [Info] Number of data points in the train set: 6053, number of used features: 59
2023-10-31 13:20:46,008:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.485710 -> initscore=-0.057177
2023-10-31 13:20:46,008:INFO:[LightGBM] [Info] Start training from score -0.057177
2023-10-31 13:20:50,298:INFO:Calculating mean and std
2023-10-31 13:20:50,307:INFO:Creating metrics dataframe
2023-10-31 13:20:50,328:INFO:Finalizing model
2023-10-31 13:29:57,508:INFO:[LightGBM] [Info] Number of positive: 3312, number of negative: 3414
2023-10-31 13:29:57,785:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.272795 seconds.
2023-10-31 13:29:57,785:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 13:29:57,792:INFO:[LightGBM] [Info] Total Bins 1409640
2023-10-31 13:29:57,845:INFO:[LightGBM] [Info] Number of data points in the train set: 6726, number of used features: 5528
2023-10-31 13:29:57,851:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.492417 -> initscore=-0.030332
2023-10-31 13:29:57,851:INFO:[LightGBM] [Info] Start training from score -0.030332
2023-10-31 13:30:33,967:INFO:[LightGBM] [Info] Number of positive: 3312, number of negative: 3414
2023-10-31 13:30:33,968:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001811 seconds.
2023-10-31 13:30:33,969:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 13:30:33,969:INFO:[LightGBM] [Info] Total Bins 15045
2023-10-31 13:30:33,969:INFO:[LightGBM] [Info] Number of data points in the train set: 6726, number of used features: 59
2023-10-31 13:30:33,970:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.492417 -> initscore=-0.030332
2023-10-31 13:30:33,970:INFO:[LightGBM] [Info] Start training from score -0.030332
2023-10-31 13:30:36,724:INFO:Uploading results into container
2023-10-31 13:30:36,726:INFO:Uploading model into container now
2023-10-31 13:30:36,727:INFO:_master_model_container: 33
2023-10-31 13:30:36,727:INFO:_display_container: 15
2023-10-31 13:30:36,731:INFO:VotingClassifier(estimators=[('Extra Trees Classifier',
                              ExtraTreesClassifier(bootstrap=False,
                                                   ccp_alpha=0.0,
                                                   class_weight=None,
                                                   criterion='gini',
                                                   max_depth=None,
                                                   max_features='sqrt',
                                                   max_leaf_nodes=None,
                                                   max_samples=None,
                                                   min_impurity_decrease=0.0,
                                                   min_samples_leaf=1,
                                                   min_samples_split=2,
                                                   min_weight_fraction_leaf=0.0,
                                                   n_estimators=100, n_jobs=-1,
                                                   oob_score=False,
                                                   ra...
                                             importance_type='split',
                                             learning_rate=0.1, max_depth=-1,
                                             min_child_samples=20,
                                             min_child_weight=0.001,
                                             min_split_gain=0.0,
                                             n_estimators=100, n_jobs=-1,
                                             num_leaves=31, objective=None,
                                             random_state=1784, reg_alpha=0.0,
                                             reg_lambda=0.0, subsample=1.0,
                                             subsample_for_bin=200000,
                                             subsample_freq=0))],
                 flatten_transform=True, n_jobs=-1, verbose=False,
                 voting='soft', weights=None)
2023-10-31 13:30:36,731:INFO:create_model() successfully completed......................................
2023-10-31 13:30:37,116:INFO:SubProcess create_model() end ==================================
2023-10-31 13:30:37,122:INFO:_master_model_container: 33
2023-10-31 13:30:37,123:INFO:_display_container: 15
2023-10-31 13:30:37,125:INFO:VotingClassifier(estimators=[('Extra Trees Classifier',
                              ExtraTreesClassifier(bootstrap=False,
                                                   ccp_alpha=0.0,
                                                   class_weight=None,
                                                   criterion='gini',
                                                   max_depth=None,
                                                   max_features='sqrt',
                                                   max_leaf_nodes=None,
                                                   max_samples=None,
                                                   min_impurity_decrease=0.0,
                                                   min_samples_leaf=1,
                                                   min_samples_split=2,
                                                   min_weight_fraction_leaf=0.0,
                                                   n_estimators=100, n_jobs=-1,
                                                   oob_score=False,
                                                   ra...
                                             importance_type='split',
                                             learning_rate=0.1, max_depth=-1,
                                             min_child_samples=20,
                                             min_child_weight=0.001,
                                             min_split_gain=0.0,
                                             n_estimators=100, n_jobs=-1,
                                             num_leaves=31, objective=None,
                                             random_state=1784, reg_alpha=0.0,
                                             reg_lambda=0.0, subsample=1.0,
                                             subsample_for_bin=200000,
                                             subsample_freq=0))],
                 flatten_transform=True, n_jobs=-1, verbose=False,
                 voting='soft', weights=None)
2023-10-31 13:30:37,125:INFO:blend_models() successfully completed......................................
2023-10-31 13:30:37,218:INFO:Initializing blend_models()
2023-10-31 13:30:37,218:INFO:blend_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x2a279fcd0>, estimator_list=[ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     n_estimators=100, n_jobs=-1, oob_score=False,
                     random_state=1784, verbose=0, warm_start=False), RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=100, n_jobs=-1, oob_score=False,
                       random_state=1784, verbose=0, warm_start=False), QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001), <catboost.core.CatBoostClassifier object at 0x2b25da2f0>, LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=1784, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)], fold=None, round=4, choose_better=False, optimize=AUC, method=hard, weights=None, fit_kwargs=None, groups=None, probability_threshold=None, verbose=True, return_train_score=False)
2023-10-31 13:30:37,219:INFO:Checking exceptions
2023-10-31 13:30:37,257:INFO:Importing libraries
2023-10-31 13:30:37,257:INFO:Copying training dataset
2023-10-31 13:30:37,258:INFO:Getting model names
2023-10-31 13:30:37,260:INFO:SubProcess create_model() called ==================================
2023-10-31 13:30:37,262:INFO:Initializing create_model()
2023-10-31 13:30:37,262:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x2a279fcd0>, estimator=VotingClassifier(estimators=[('Extra Trees Classifier',
                              ExtraTreesClassifier(bootstrap=False,
                                                   ccp_alpha=0.0,
                                                   class_weight=None,
                                                   criterion='gini',
                                                   max_depth=None,
                                                   max_features='sqrt',
                                                   max_leaf_nodes=None,
                                                   max_samples=None,
                                                   min_impurity_decrease=0.0,
                                                   min_samples_leaf=1,
                                                   min_samples_split=2,
                                                   min_weight_fraction_leaf=0.0,
                                                   n_estimators=100, n_jobs=-1,
                                                   oob_score=False,
                                                   ra...
                                             importance_type='split',
                                             learning_rate=0.1, max_depth=-1,
                                             min_child_samples=20,
                                             min_child_weight=0.001,
                                             min_split_gain=0.0,
                                             n_estimators=100, n_jobs=-1,
                                             num_leaves=31, objective=None,
                                             random_state=1784, reg_alpha=0.0,
                                             reg_lambda=0.0, subsample=1.0,
                                             subsample_for_bin=200000,
                                             subsample_freq=0))],
                 flatten_transform=True, n_jobs=-1, verbose=False,
                 voting='hard', weights=None), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x148c4b4f0>, model_only=True, return_train_score=False, kwargs={})
2023-10-31 13:30:37,262:INFO:Checking exceptions
2023-10-31 13:30:37,263:INFO:Importing libraries
2023-10-31 13:30:37,263:INFO:Copying training dataset
2023-10-31 13:30:37,274:INFO:Defining folds
2023-10-31 13:30:37,274:INFO:Declaring metric variables
2023-10-31 13:30:37,275:INFO:Importing untrained model
2023-10-31 13:30:37,275:INFO:Declaring custom model
2023-10-31 13:30:37,278:INFO:Voting Classifier Imported successfully
2023-10-31 13:30:37,281:INFO:Starting cross validation
2023-10-31 13:30:37,778:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-10-31 13:31:54,889:INFO:Initializing plot_model()
2023-10-31 13:31:54,890:INFO:plot_model(plot=boundary, fold=None, verbose=True, display=None, display_format=None, estimator=VotingClassifier(estimators=[('Extra Trees Classifier',
                              ExtraTreesClassifier(bootstrap=False,
                                                   ccp_alpha=0.0,
                                                   class_weight=None,
                                                   criterion='gini',
                                                   max_depth=None,
                                                   max_features='sqrt',
                                                   max_leaf_nodes=None,
                                                   max_samples=None,
                                                   min_impurity_decrease=0.0,
                                                   min_samples_leaf=1,
                                                   min_samples_split=2,
                                                   min_weight_fraction_leaf=0.0,
                                                   n_estimators=100, n_jobs=-1,
                                                   oob_score=False,
                                                   ra...
                                             importance_type='split',
                                             learning_rate=0.1, max_depth=-1,
                                             min_child_samples=20,
                                             min_child_weight=0.001,
                                             min_split_gain=0.0,
                                             n_estimators=100, n_jobs=-1,
                                             num_leaves=31, objective=None,
                                             random_state=1784, reg_alpha=0.0,
                                             reg_lambda=0.0, subsample=1.0,
                                             subsample_for_bin=200000,
                                             subsample_freq=0))],
                 flatten_transform=True, n_jobs=-1, verbose=False,
                 voting='soft', weights=None), feature_name=None, fit_kwargs=None, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.classification.oop.ClassificationExperiment object at 0x2a279fcd0>, system=True)
2023-10-31 13:31:54,891:INFO:Checking exceptions
2023-10-31 13:31:54,901:INFO:Preloading libraries
2023-10-31 13:31:54,945:INFO:Copying training dataset
2023-10-31 13:31:54,945:INFO:Plot type: boundary
2023-10-31 13:31:55,362:INFO:Fitting StandardScaler()
2023-10-31 13:31:55,367:INFO:Fitting PCA()
2023-10-31 13:31:59,394:INFO:Fitting Model
2023-10-31 13:32:00,857:INFO:[LightGBM] [Info] Number of positive: 3312, number of negative: 3414
2023-10-31 13:32:00,857:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000078 seconds.
2023-10-31 13:32:00,857:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 13:32:00,857:INFO:[LightGBM] [Info] Total Bins 510
2023-10-31 13:32:00,857:INFO:[LightGBM] [Info] Number of data points in the train set: 6726, number of used features: 2
2023-10-31 13:32:00,857:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.492417 -> initscore=-0.030332
2023-10-31 13:32:00,857:INFO:[LightGBM] [Info] Start training from score -0.030332
2023-10-31 13:32:03,641:INFO:Visual Rendered Successfully
2023-10-31 13:32:03,759:INFO:plot_model() successfully completed......................................
2023-10-31 13:32:03,782:INFO:Initializing plot_model()
2023-10-31 13:32:03,782:INFO:plot_model(plot=auc, fold=None, verbose=True, display=None, display_format=None, estimator=VotingClassifier(estimators=[('Extra Trees Classifier',
                              ExtraTreesClassifier(bootstrap=False,
                                                   ccp_alpha=0.0,
                                                   class_weight=None,
                                                   criterion='gini',
                                                   max_depth=None,
                                                   max_features='sqrt',
                                                   max_leaf_nodes=None,
                                                   max_samples=None,
                                                   min_impurity_decrease=0.0,
                                                   min_samples_leaf=1,
                                                   min_samples_split=2,
                                                   min_weight_fraction_leaf=0.0,
                                                   n_estimators=100, n_jobs=-1,
                                                   oob_score=False,
                                                   ra...
                                             importance_type='split',
                                             learning_rate=0.1, max_depth=-1,
                                             min_child_samples=20,
                                             min_child_weight=0.001,
                                             min_split_gain=0.0,
                                             n_estimators=100, n_jobs=-1,
                                             num_leaves=31, objective=None,
                                             random_state=1784, reg_alpha=0.0,
                                             reg_lambda=0.0, subsample=1.0,
                                             subsample_for_bin=200000,
                                             subsample_freq=0))],
                 flatten_transform=True, n_jobs=-1, verbose=False,
                 voting='soft', weights=None), feature_name=None, fit_kwargs=None, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.classification.oop.ClassificationExperiment object at 0x2a279fcd0>, system=True)
2023-10-31 13:32:03,782:INFO:Checking exceptions
2023-10-31 13:32:03,790:INFO:Preloading libraries
2023-10-31 13:32:03,822:INFO:Copying training dataset
2023-10-31 13:32:03,823:INFO:Plot type: auc
2023-10-31 13:32:04,564:INFO:Fitting Model
2023-10-31 13:32:04,564:WARNING:X does not have valid feature names, but ExtraTreesClassifier was fitted with feature names

2023-10-31 13:32:04,565:INFO:Scoring test/hold-out set
2023-10-31 13:32:04,909:INFO:Visual Rendered Successfully
2023-10-31 13:32:05,001:INFO:plot_model() successfully completed......................................
2023-10-31 13:32:05,034:INFO:Initializing blend_models()
2023-10-31 13:32:05,034:INFO:blend_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x2a279fcd0>, estimator_list=[ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     n_estimators=100, n_jobs=-1, oob_score=False,
                     random_state=1784, verbose=0, warm_start=False), RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=100, n_jobs=-1, oob_score=False,
                       random_state=1784, verbose=0, warm_start=False), QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001), <catboost.core.CatBoostClassifier object at 0x2b25da2f0>, LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=1784, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)], fold=None, round=4, choose_better=False, optimize=AUC, method=hard, weights=None, fit_kwargs=None, groups=None, probability_threshold=None, verbose=True, return_train_score=False)
2023-10-31 13:32:05,034:INFO:Checking exceptions
2023-10-31 13:32:05,048:INFO:Importing libraries
2023-10-31 13:32:05,048:INFO:Copying training dataset
2023-10-31 13:32:05,050:INFO:Getting model names
2023-10-31 13:32:05,052:INFO:SubProcess create_model() called ==================================
2023-10-31 13:32:05,055:INFO:Initializing create_model()
2023-10-31 13:32:05,055:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x2a279fcd0>, estimator=VotingClassifier(estimators=[('Extra Trees Classifier',
                              ExtraTreesClassifier(bootstrap=False,
                                                   ccp_alpha=0.0,
                                                   class_weight=None,
                                                   criterion='gini',
                                                   max_depth=None,
                                                   max_features='sqrt',
                                                   max_leaf_nodes=None,
                                                   max_samples=None,
                                                   min_impurity_decrease=0.0,
                                                   min_samples_leaf=1,
                                                   min_samples_split=2,
                                                   min_weight_fraction_leaf=0.0,
                                                   n_estimators=100, n_jobs=-1,
                                                   oob_score=False,
                                                   ra...
                                             importance_type='split',
                                             learning_rate=0.1, max_depth=-1,
                                             min_child_samples=20,
                                             min_child_weight=0.001,
                                             min_split_gain=0.0,
                                             n_estimators=100, n_jobs=-1,
                                             num_leaves=31, objective=None,
                                             random_state=1784, reg_alpha=0.0,
                                             reg_lambda=0.0, subsample=1.0,
                                             subsample_for_bin=200000,
                                             subsample_freq=0))],
                 flatten_transform=True, n_jobs=-1, verbose=False,
                 voting='hard', weights=None), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x2a0b3ead0>, model_only=True, return_train_score=False, kwargs={})
2023-10-31 13:32:05,055:INFO:Checking exceptions
2023-10-31 13:32:05,055:INFO:Importing libraries
2023-10-31 13:32:05,055:INFO:Copying training dataset
2023-10-31 13:32:05,068:INFO:Defining folds
2023-10-31 13:32:05,068:INFO:Declaring metric variables
2023-10-31 13:32:05,070:INFO:Importing untrained model
2023-10-31 13:32:05,070:INFO:Declaring custom model
2023-10-31 13:32:05,072:INFO:Voting Classifier Imported successfully
2023-10-31 13:32:05,075:INFO:Starting cross validation
2023-10-31 13:32:05,105:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-10-31 13:36:25,030:WARNING:/Users/tanmaysharma/projects/Jak2Biotech/.venv/lib/python3.10/site-packages/pycaret/internal/metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/Users/tanmaysharma/projects/Jak2Biotech/.venv/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/tanmaysharma/projects/Jak2Biotech/.venv/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 182, in _score
    return super()._score(
  File "/Users/tanmaysharma/projects/Jak2Biotech/.venv/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "/Users/tanmaysharma/projects/Jak2Biotech/.venv/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "/Users/tanmaysharma/projects/Jak2Biotech/.venv/lib/python3.10/site-packages/sklearn/utils/_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "/Users/tanmaysharma/projects/Jak2Biotech/.venv/lib/python3.10/site-packages/sklearn/ensemble/_voting.py", line 383, in _check_voting
    raise AttributeError(
AttributeError: predict_proba is not available when voting='hard'

  warnings.warn(

2023-10-31 13:36:25,203:WARNING:/Users/tanmaysharma/projects/Jak2Biotech/.venv/lib/python3.10/site-packages/pycaret/internal/metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/Users/tanmaysharma/projects/Jak2Biotech/.venv/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/tanmaysharma/projects/Jak2Biotech/.venv/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 182, in _score
    return super()._score(
  File "/Users/tanmaysharma/projects/Jak2Biotech/.venv/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "/Users/tanmaysharma/projects/Jak2Biotech/.venv/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "/Users/tanmaysharma/projects/Jak2Biotech/.venv/lib/python3.10/site-packages/sklearn/utils/_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "/Users/tanmaysharma/projects/Jak2Biotech/.venv/lib/python3.10/site-packages/sklearn/ensemble/_voting.py", line 383, in _check_voting
    raise AttributeError(
AttributeError: predict_proba is not available when voting='hard'

  warnings.warn(

2023-10-31 13:36:25,583:WARNING:/Users/tanmaysharma/projects/Jak2Biotech/.venv/lib/python3.10/site-packages/pycaret/internal/metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/Users/tanmaysharma/projects/Jak2Biotech/.venv/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/tanmaysharma/projects/Jak2Biotech/.venv/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 182, in _score
    return super()._score(
  File "/Users/tanmaysharma/projects/Jak2Biotech/.venv/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "/Users/tanmaysharma/projects/Jak2Biotech/.venv/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "/Users/tanmaysharma/projects/Jak2Biotech/.venv/lib/python3.10/site-packages/sklearn/utils/_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "/Users/tanmaysharma/projects/Jak2Biotech/.venv/lib/python3.10/site-packages/sklearn/ensemble/_voting.py", line 383, in _check_voting
    raise AttributeError(
AttributeError: predict_proba is not available when voting='hard'

  warnings.warn(

2023-10-31 13:36:26,493:WARNING:/Users/tanmaysharma/projects/Jak2Biotech/.venv/lib/python3.10/site-packages/pycaret/internal/metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/Users/tanmaysharma/projects/Jak2Biotech/.venv/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/tanmaysharma/projects/Jak2Biotech/.venv/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 182, in _score
    return super()._score(
  File "/Users/tanmaysharma/projects/Jak2Biotech/.venv/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "/Users/tanmaysharma/projects/Jak2Biotech/.venv/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "/Users/tanmaysharma/projects/Jak2Biotech/.venv/lib/python3.10/site-packages/sklearn/utils/_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "/Users/tanmaysharma/projects/Jak2Biotech/.venv/lib/python3.10/site-packages/sklearn/ensemble/_voting.py", line 383, in _check_voting
    raise AttributeError(
AttributeError: predict_proba is not available when voting='hard'

  warnings.warn(

2023-10-31 13:36:27,588:WARNING:/Users/tanmaysharma/projects/Jak2Biotech/.venv/lib/python3.10/site-packages/pycaret/internal/metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/Users/tanmaysharma/projects/Jak2Biotech/.venv/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/tanmaysharma/projects/Jak2Biotech/.venv/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 182, in _score
    return super()._score(
  File "/Users/tanmaysharma/projects/Jak2Biotech/.venv/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "/Users/tanmaysharma/projects/Jak2Biotech/.venv/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "/Users/tanmaysharma/projects/Jak2Biotech/.venv/lib/python3.10/site-packages/sklearn/utils/_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "/Users/tanmaysharma/projects/Jak2Biotech/.venv/lib/python3.10/site-packages/sklearn/ensemble/_voting.py", line 383, in _check_voting
    raise AttributeError(
AttributeError: predict_proba is not available when voting='hard'

  warnings.warn(

2023-10-31 13:36:27,629:WARNING:/Users/tanmaysharma/projects/Jak2Biotech/.venv/lib/python3.10/site-packages/pycaret/internal/metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/Users/tanmaysharma/projects/Jak2Biotech/.venv/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/tanmaysharma/projects/Jak2Biotech/.venv/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 182, in _score
    return super()._score(
  File "/Users/tanmaysharma/projects/Jak2Biotech/.venv/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "/Users/tanmaysharma/projects/Jak2Biotech/.venv/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "/Users/tanmaysharma/projects/Jak2Biotech/.venv/lib/python3.10/site-packages/sklearn/utils/_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "/Users/tanmaysharma/projects/Jak2Biotech/.venv/lib/python3.10/site-packages/sklearn/ensemble/_voting.py", line 383, in _check_voting
    raise AttributeError(
AttributeError: predict_proba is not available when voting='hard'

  warnings.warn(

2023-10-31 13:36:27,657:WARNING:/Users/tanmaysharma/projects/Jak2Biotech/.venv/lib/python3.10/site-packages/pycaret/internal/metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/Users/tanmaysharma/projects/Jak2Biotech/.venv/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/tanmaysharma/projects/Jak2Biotech/.venv/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 182, in _score
    return super()._score(
  File "/Users/tanmaysharma/projects/Jak2Biotech/.venv/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "/Users/tanmaysharma/projects/Jak2Biotech/.venv/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "/Users/tanmaysharma/projects/Jak2Biotech/.venv/lib/python3.10/site-packages/sklearn/utils/_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "/Users/tanmaysharma/projects/Jak2Biotech/.venv/lib/python3.10/site-packages/sklearn/ensemble/_voting.py", line 383, in _check_voting
    raise AttributeError(
AttributeError: predict_proba is not available when voting='hard'

  warnings.warn(

2023-10-31 13:36:27,797:WARNING:/Users/tanmaysharma/projects/Jak2Biotech/.venv/lib/python3.10/site-packages/pycaret/internal/metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/Users/tanmaysharma/projects/Jak2Biotech/.venv/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/tanmaysharma/projects/Jak2Biotech/.venv/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 182, in _score
    return super()._score(
  File "/Users/tanmaysharma/projects/Jak2Biotech/.venv/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "/Users/tanmaysharma/projects/Jak2Biotech/.venv/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "/Users/tanmaysharma/projects/Jak2Biotech/.venv/lib/python3.10/site-packages/sklearn/utils/_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "/Users/tanmaysharma/projects/Jak2Biotech/.venv/lib/python3.10/site-packages/sklearn/ensemble/_voting.py", line 383, in _check_voting
    raise AttributeError(
AttributeError: predict_proba is not available when voting='hard'

  warnings.warn(

2023-10-31 13:36:27,844:WARNING:/Users/tanmaysharma/projects/Jak2Biotech/.venv/lib/python3.10/site-packages/pycaret/internal/metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/Users/tanmaysharma/projects/Jak2Biotech/.venv/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/tanmaysharma/projects/Jak2Biotech/.venv/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 182, in _score
    return super()._score(
  File "/Users/tanmaysharma/projects/Jak2Biotech/.venv/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "/Users/tanmaysharma/projects/Jak2Biotech/.venv/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "/Users/tanmaysharma/projects/Jak2Biotech/.venv/lib/python3.10/site-packages/sklearn/utils/_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "/Users/tanmaysharma/projects/Jak2Biotech/.venv/lib/python3.10/site-packages/sklearn/ensemble/_voting.py", line 383, in _check_voting
    raise AttributeError(
AttributeError: predict_proba is not available when voting='hard'

  warnings.warn(

2023-10-31 13:36:40,005:WARNING:/Users/tanmaysharma/projects/Jak2Biotech/.venv/lib/python3.10/site-packages/pycaret/internal/metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/Users/tanmaysharma/projects/Jak2Biotech/.venv/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/tanmaysharma/projects/Jak2Biotech/.venv/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 182, in _score
    return super()._score(
  File "/Users/tanmaysharma/projects/Jak2Biotech/.venv/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "/Users/tanmaysharma/projects/Jak2Biotech/.venv/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "/Users/tanmaysharma/projects/Jak2Biotech/.venv/lib/python3.10/site-packages/sklearn/utils/_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "/Users/tanmaysharma/projects/Jak2Biotech/.venv/lib/python3.10/site-packages/sklearn/ensemble/_voting.py", line 383, in _check_voting
    raise AttributeError(
AttributeError: predict_proba is not available when voting='hard'

  warnings.warn(

2023-10-31 13:36:40,120:INFO:Calculating mean and std
2023-10-31 13:36:40,125:INFO:Creating metrics dataframe
2023-10-31 13:36:40,135:INFO:Finalizing model
2023-10-31 13:45:52,285:INFO:[LightGBM] [Info] Number of positive: 3312, number of negative: 3414
2023-10-31 13:45:52,577:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.290048 seconds.
2023-10-31 13:45:52,577:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 13:45:52,584:INFO:[LightGBM] [Info] Total Bins 1409640
2023-10-31 13:45:52,638:INFO:[LightGBM] [Info] Number of data points in the train set: 6726, number of used features: 5528
2023-10-31 13:45:52,644:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.492417 -> initscore=-0.030332
2023-10-31 13:45:52,644:INFO:[LightGBM] [Info] Start training from score -0.030332
2023-10-31 13:46:28,146:INFO:[LightGBM] [Info] Number of positive: 3312, number of negative: 3414
2023-10-31 13:46:28,148:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001869 seconds.
2023-10-31 13:46:28,148:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 13:46:28,148:INFO:[LightGBM] [Info] Total Bins 15045
2023-10-31 13:46:28,149:INFO:[LightGBM] [Info] Number of data points in the train set: 6726, number of used features: 59
2023-10-31 13:46:28,149:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.492417 -> initscore=-0.030332
2023-10-31 13:46:28,149:INFO:[LightGBM] [Info] Start training from score -0.030332
2023-10-31 13:46:30,845:INFO:Uploading results into container
2023-10-31 13:46:30,846:INFO:Uploading model into container now
2023-10-31 13:46:30,847:INFO:_master_model_container: 34
2023-10-31 13:46:30,847:INFO:_display_container: 16
2023-10-31 13:46:30,849:INFO:VotingClassifier(estimators=[('Extra Trees Classifier',
                              ExtraTreesClassifier(bootstrap=False,
                                                   ccp_alpha=0.0,
                                                   class_weight=None,
                                                   criterion='gini',
                                                   max_depth=None,
                                                   max_features='sqrt',
                                                   max_leaf_nodes=None,
                                                   max_samples=None,
                                                   min_impurity_decrease=0.0,
                                                   min_samples_leaf=1,
                                                   min_samples_split=2,
                                                   min_weight_fraction_leaf=0.0,
                                                   n_estimators=100, n_jobs=-1,
                                                   oob_score=False,
                                                   ra...
                                             importance_type='split',
                                             learning_rate=0.1, max_depth=-1,
                                             min_child_samples=20,
                                             min_child_weight=0.001,
                                             min_split_gain=0.0,
                                             n_estimators=100, n_jobs=-1,
                                             num_leaves=31, objective=None,
                                             random_state=1784, reg_alpha=0.0,
                                             reg_lambda=0.0, subsample=1.0,
                                             subsample_for_bin=200000,
                                             subsample_freq=0))],
                 flatten_transform=True, n_jobs=-1, verbose=False,
                 voting='hard', weights=None)
2023-10-31 13:46:30,849:INFO:create_model() successfully completed......................................
2023-10-31 13:46:30,968:INFO:SubProcess create_model() end ==================================
2023-10-31 13:46:30,973:INFO:_master_model_container: 34
2023-10-31 13:46:30,973:INFO:_display_container: 16
2023-10-31 13:46:30,975:INFO:VotingClassifier(estimators=[('Extra Trees Classifier',
                              ExtraTreesClassifier(bootstrap=False,
                                                   ccp_alpha=0.0,
                                                   class_weight=None,
                                                   criterion='gini',
                                                   max_depth=None,
                                                   max_features='sqrt',
                                                   max_leaf_nodes=None,
                                                   max_samples=None,
                                                   min_impurity_decrease=0.0,
                                                   min_samples_leaf=1,
                                                   min_samples_split=2,
                                                   min_weight_fraction_leaf=0.0,
                                                   n_estimators=100, n_jobs=-1,
                                                   oob_score=False,
                                                   ra...
                                             importance_type='split',
                                             learning_rate=0.1, max_depth=-1,
                                             min_child_samples=20,
                                             min_child_weight=0.001,
                                             min_split_gain=0.0,
                                             n_estimators=100, n_jobs=-1,
                                             num_leaves=31, objective=None,
                                             random_state=1784, reg_alpha=0.0,
                                             reg_lambda=0.0, subsample=1.0,
                                             subsample_for_bin=200000,
                                             subsample_freq=0))],
                 flatten_transform=True, n_jobs=-1, verbose=False,
                 voting='hard', weights=None)
2023-10-31 13:46:30,976:INFO:blend_models() successfully completed......................................
2023-10-31 14:03:09,565:INFO:Initializing evaluate_model()
2023-10-31 14:03:09,565:INFO:evaluate_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x2a279fcd0>, estimator=VotingClassifier(estimators=[('Extra Trees Classifier',
                              ExtraTreesClassifier(bootstrap=False,
                                                   ccp_alpha=0.0,
                                                   class_weight=None,
                                                   criterion='gini',
                                                   max_depth=None,
                                                   max_features='sqrt',
                                                   max_leaf_nodes=None,
                                                   max_samples=None,
                                                   min_impurity_decrease=0.0,
                                                   min_samples_leaf=1,
                                                   min_samples_split=2,
                                                   min_weight_fraction_leaf=0.0,
                                                   n_estimators=100, n_jobs=-1,
                                                   oob_score=False,
                                                   ra...
                                             importance_type='split',
                                             learning_rate=0.1, max_depth=-1,
                                             min_child_samples=20,
                                             min_child_weight=0.001,
                                             min_split_gain=0.0,
                                             n_estimators=100, n_jobs=-1,
                                             num_leaves=31, objective=None,
                                             random_state=1784, reg_alpha=0.0,
                                             reg_lambda=0.0, subsample=1.0,
                                             subsample_for_bin=200000,
                                             subsample_freq=0))],
                 flatten_transform=True, n_jobs=-1, verbose=False,
                 voting='hard', weights=None), fold=None, fit_kwargs=None, plot_kwargs=None, feature_name=None, groups=None)
2023-10-31 14:03:09,591:INFO:Initializing plot_model()
2023-10-31 14:03:09,591:INFO:plot_model(plot=pipeline, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), verbose=False, display=None, display_format=None, estimator=VotingClassifier(estimators=[('Extra Trees Classifier',
                              ExtraTreesClassifier(bootstrap=False,
                                                   ccp_alpha=0.0,
                                                   class_weight=None,
                                                   criterion='gini',
                                                   max_depth=None,
                                                   max_features='sqrt',
                                                   max_leaf_nodes=None,
                                                   max_samples=None,
                                                   min_impurity_decrease=0.0,
                                                   min_samples_leaf=1,
                                                   min_samples_split=2,
                                                   min_weight_fraction_leaf=0.0,
                                                   n_estimators=100, n_jobs=-1,
                                                   oob_score=False,
                                                   ra...
                                             importance_type='split',
                                             learning_rate=0.1, max_depth=-1,
                                             min_child_samples=20,
                                             min_child_weight=0.001,
                                             min_split_gain=0.0,
                                             n_estimators=100, n_jobs=-1,
                                             num_leaves=31, objective=None,
                                             random_state=1784, reg_alpha=0.0,
                                             reg_lambda=0.0, subsample=1.0,
                                             subsample_for_bin=200000,
                                             subsample_freq=0))],
                 flatten_transform=True, n_jobs=-1, verbose=False,
                 voting='hard', weights=None), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.classification.oop.ClassificationExperiment object at 0x2a279fcd0>, system=True)
2023-10-31 14:03:09,591:INFO:Checking exceptions
2023-10-31 14:03:09,599:INFO:Preloading libraries
2023-10-31 14:03:09,641:INFO:Copying training dataset
2023-10-31 14:03:09,641:INFO:Plot type: pipeline
2023-10-31 14:03:09,768:INFO:Visual Rendered Successfully
2023-10-31 14:03:09,884:INFO:plot_model() successfully completed......................................
2023-10-31 14:03:27,095:INFO:Initializing plot_model()
2023-10-31 14:03:27,096:INFO:plot_model(plot=dimension, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), verbose=False, display=None, display_format=None, estimator=VotingClassifier(estimators=[('Extra Trees Classifier',
                              ExtraTreesClassifier(bootstrap=False,
                                                   ccp_alpha=0.0,
                                                   class_weight=None,
                                                   criterion='gini',
                                                   max_depth=None,
                                                   max_features='sqrt',
                                                   max_leaf_nodes=None,
                                                   max_samples=None,
                                                   min_impurity_decrease=0.0,
                                                   min_samples_leaf=1,
                                                   min_samples_split=2,
                                                   min_weight_fraction_leaf=0.0,
                                                   n_estimators=100, n_jobs=-1,
                                                   oob_score=False,
                                                   ra...
                                             importance_type='split',
                                             learning_rate=0.1, max_depth=-1,
                                             min_child_samples=20,
                                             min_child_weight=0.001,
                                             min_split_gain=0.0,
                                             n_estimators=100, n_jobs=-1,
                                             num_leaves=31, objective=None,
                                             random_state=1784, reg_alpha=0.0,
                                             reg_lambda=0.0, subsample=1.0,
                                             subsample_for_bin=200000,
                                             subsample_freq=0))],
                 flatten_transform=True, n_jobs=-1, verbose=False,
                 voting='hard', weights=None), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.classification.oop.ClassificationExperiment object at 0x2a279fcd0>, system=True)
2023-10-31 14:03:27,096:INFO:Checking exceptions
2023-10-31 14:03:27,101:INFO:Preloading libraries
2023-10-31 14:03:27,132:INFO:Copying training dataset
2023-10-31 14:03:27,132:INFO:Plot type: dimension
2023-10-31 14:03:27,400:INFO:Fitting StandardScaler()
2023-10-31 14:03:27,581:INFO:Fitting PCA()
2023-10-31 14:03:30,294:INFO:Fitting & Transforming Model
2023-10-31 14:03:30,472:INFO:Visual Rendered Successfully
2023-10-31 14:03:30,562:INFO:plot_model() successfully completed......................................
2023-10-31 14:03:33,110:INFO:Initializing plot_model()
2023-10-31 14:03:33,110:INFO:plot_model(plot=pipeline, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), verbose=False, display=None, display_format=None, estimator=VotingClassifier(estimators=[('Extra Trees Classifier',
                              ExtraTreesClassifier(bootstrap=False,
                                                   ccp_alpha=0.0,
                                                   class_weight=None,
                                                   criterion='gini',
                                                   max_depth=None,
                                                   max_features='sqrt',
                                                   max_leaf_nodes=None,
                                                   max_samples=None,
                                                   min_impurity_decrease=0.0,
                                                   min_samples_leaf=1,
                                                   min_samples_split=2,
                                                   min_weight_fraction_leaf=0.0,
                                                   n_estimators=100, n_jobs=-1,
                                                   oob_score=False,
                                                   ra...
                                             importance_type='split',
                                             learning_rate=0.1, max_depth=-1,
                                             min_child_samples=20,
                                             min_child_weight=0.001,
                                             min_split_gain=0.0,
                                             n_estimators=100, n_jobs=-1,
                                             num_leaves=31, objective=None,
                                             random_state=1784, reg_alpha=0.0,
                                             reg_lambda=0.0, subsample=1.0,
                                             subsample_for_bin=200000,
                                             subsample_freq=0))],
                 flatten_transform=True, n_jobs=-1, verbose=False,
                 voting='hard', weights=None), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.classification.oop.ClassificationExperiment object at 0x2a279fcd0>, system=True)
2023-10-31 14:03:33,110:INFO:Checking exceptions
2023-10-31 14:03:33,118:INFO:Preloading libraries
2023-10-31 14:03:33,159:INFO:Copying training dataset
2023-10-31 14:03:33,159:INFO:Plot type: pipeline
2023-10-31 14:03:33,265:INFO:Visual Rendered Successfully
2023-10-31 14:03:33,355:INFO:plot_model() successfully completed......................................
2023-10-31 14:03:33,914:INFO:Initializing plot_model()
2023-10-31 14:03:33,914:INFO:plot_model(plot=parameter, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), verbose=False, display=None, display_format=None, estimator=VotingClassifier(estimators=[('Extra Trees Classifier',
                              ExtraTreesClassifier(bootstrap=False,
                                                   ccp_alpha=0.0,
                                                   class_weight=None,
                                                   criterion='gini',
                                                   max_depth=None,
                                                   max_features='sqrt',
                                                   max_leaf_nodes=None,
                                                   max_samples=None,
                                                   min_impurity_decrease=0.0,
                                                   min_samples_leaf=1,
                                                   min_samples_split=2,
                                                   min_weight_fraction_leaf=0.0,
                                                   n_estimators=100, n_jobs=-1,
                                                   oob_score=False,
                                                   ra...
                                             importance_type='split',
                                             learning_rate=0.1, max_depth=-1,
                                             min_child_samples=20,
                                             min_child_weight=0.001,
                                             min_split_gain=0.0,
                                             n_estimators=100, n_jobs=-1,
                                             num_leaves=31, objective=None,
                                             random_state=1784, reg_alpha=0.0,
                                             reg_lambda=0.0, subsample=1.0,
                                             subsample_for_bin=200000,
                                             subsample_freq=0))],
                 flatten_transform=True, n_jobs=-1, verbose=False,
                 voting='hard', weights=None), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.classification.oop.ClassificationExperiment object at 0x2a279fcd0>, system=True)
2023-10-31 14:03:33,914:INFO:Checking exceptions
2023-10-31 14:03:33,923:INFO:Preloading libraries
2023-10-31 14:03:33,961:INFO:Copying training dataset
2023-10-31 14:03:33,961:INFO:Plot type: parameter
2023-10-31 14:03:33,964:INFO:Visual Rendered Successfully
2023-10-31 14:03:34,053:INFO:plot_model() successfully completed......................................
2023-10-31 14:03:36,823:INFO:Initializing plot_model()
2023-10-31 14:03:36,824:INFO:plot_model(plot=auc, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), verbose=False, display=None, display_format=None, estimator=VotingClassifier(estimators=[('Extra Trees Classifier',
                              ExtraTreesClassifier(bootstrap=False,
                                                   ccp_alpha=0.0,
                                                   class_weight=None,
                                                   criterion='gini',
                                                   max_depth=None,
                                                   max_features='sqrt',
                                                   max_leaf_nodes=None,
                                                   max_samples=None,
                                                   min_impurity_decrease=0.0,
                                                   min_samples_leaf=1,
                                                   min_samples_split=2,
                                                   min_weight_fraction_leaf=0.0,
                                                   n_estimators=100, n_jobs=-1,
                                                   oob_score=False,
                                                   ra...
                                             importance_type='split',
                                             learning_rate=0.1, max_depth=-1,
                                             min_child_samples=20,
                                             min_child_weight=0.001,
                                             min_split_gain=0.0,
                                             n_estimators=100, n_jobs=-1,
                                             num_leaves=31, objective=None,
                                             random_state=1784, reg_alpha=0.0,
                                             reg_lambda=0.0, subsample=1.0,
                                             subsample_for_bin=200000,
                                             subsample_freq=0))],
                 flatten_transform=True, n_jobs=-1, verbose=False,
                 voting='hard', weights=None), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.classification.oop.ClassificationExperiment object at 0x2a279fcd0>, system=True)
2023-10-31 14:03:36,824:INFO:Checking exceptions
2023-10-31 14:03:42,283:INFO:Initializing plot_model()
2023-10-31 14:03:42,284:INFO:plot_model(plot=confusion_matrix, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), verbose=False, display=None, display_format=None, estimator=VotingClassifier(estimators=[('Extra Trees Classifier',
                              ExtraTreesClassifier(bootstrap=False,
                                                   ccp_alpha=0.0,
                                                   class_weight=None,
                                                   criterion='gini',
                                                   max_depth=None,
                                                   max_features='sqrt',
                                                   max_leaf_nodes=None,
                                                   max_samples=None,
                                                   min_impurity_decrease=0.0,
                                                   min_samples_leaf=1,
                                                   min_samples_split=2,
                                                   min_weight_fraction_leaf=0.0,
                                                   n_estimators=100, n_jobs=-1,
                                                   oob_score=False,
                                                   ra...
                                             importance_type='split',
                                             learning_rate=0.1, max_depth=-1,
                                             min_child_samples=20,
                                             min_child_weight=0.001,
                                             min_split_gain=0.0,
                                             n_estimators=100, n_jobs=-1,
                                             num_leaves=31, objective=None,
                                             random_state=1784, reg_alpha=0.0,
                                             reg_lambda=0.0, subsample=1.0,
                                             subsample_for_bin=200000,
                                             subsample_freq=0))],
                 flatten_transform=True, n_jobs=-1, verbose=False,
                 voting='hard', weights=None), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.classification.oop.ClassificationExperiment object at 0x2a279fcd0>, system=True)
2023-10-31 14:03:42,284:INFO:Checking exceptions
2023-10-31 14:03:42,292:INFO:Preloading libraries
2023-10-31 14:03:42,329:INFO:Copying training dataset
2023-10-31 14:03:42,329:INFO:Plot type: confusion_matrix
2023-10-31 14:03:43,046:INFO:Fitting Model
2023-10-31 14:03:43,046:WARNING:X does not have valid feature names, but ExtraTreesClassifier was fitted with feature names

2023-10-31 14:03:43,046:INFO:Scoring test/hold-out set
2023-10-31 14:03:43,417:INFO:Visual Rendered Successfully
2023-10-31 14:03:43,560:INFO:plot_model() successfully completed......................................
2023-10-31 14:03:47,289:INFO:Initializing plot_model()
2023-10-31 14:03:47,289:INFO:plot_model(plot=threshold, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), verbose=False, display=None, display_format=None, estimator=VotingClassifier(estimators=[('Extra Trees Classifier',
                              ExtraTreesClassifier(bootstrap=False,
                                                   ccp_alpha=0.0,
                                                   class_weight=None,
                                                   criterion='gini',
                                                   max_depth=None,
                                                   max_features='sqrt',
                                                   max_leaf_nodes=None,
                                                   max_samples=None,
                                                   min_impurity_decrease=0.0,
                                                   min_samples_leaf=1,
                                                   min_samples_split=2,
                                                   min_weight_fraction_leaf=0.0,
                                                   n_estimators=100, n_jobs=-1,
                                                   oob_score=False,
                                                   ra...
                                             importance_type='split',
                                             learning_rate=0.1, max_depth=-1,
                                             min_child_samples=20,
                                             min_child_weight=0.001,
                                             min_split_gain=0.0,
                                             n_estimators=100, n_jobs=-1,
                                             num_leaves=31, objective=None,
                                             random_state=1784, reg_alpha=0.0,
                                             reg_lambda=0.0, subsample=1.0,
                                             subsample_for_bin=200000,
                                             subsample_freq=0))],
                 flatten_transform=True, n_jobs=-1, verbose=False,
                 voting='hard', weights=None), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.classification.oop.ClassificationExperiment object at 0x2a279fcd0>, system=True)
2023-10-31 14:03:47,290:INFO:Checking exceptions
2023-10-31 14:03:47,298:INFO:Preloading libraries
2023-10-31 14:03:47,336:INFO:Copying training dataset
2023-10-31 14:03:47,336:INFO:Plot type: threshold
2023-10-31 14:03:48,604:INFO:Initializing plot_model()
2023-10-31 14:03:48,604:INFO:plot_model(plot=pr, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), verbose=False, display=None, display_format=None, estimator=VotingClassifier(estimators=[('Extra Trees Classifier',
                              ExtraTreesClassifier(bootstrap=False,
                                                   ccp_alpha=0.0,
                                                   class_weight=None,
                                                   criterion='gini',
                                                   max_depth=None,
                                                   max_features='sqrt',
                                                   max_leaf_nodes=None,
                                                   max_samples=None,
                                                   min_impurity_decrease=0.0,
                                                   min_samples_leaf=1,
                                                   min_samples_split=2,
                                                   min_weight_fraction_leaf=0.0,
                                                   n_estimators=100, n_jobs=-1,
                                                   oob_score=False,
                                                   ra...
                                             importance_type='split',
                                             learning_rate=0.1, max_depth=-1,
                                             min_child_samples=20,
                                             min_child_weight=0.001,
                                             min_split_gain=0.0,
                                             n_estimators=100, n_jobs=-1,
                                             num_leaves=31, objective=None,
                                             random_state=1784, reg_alpha=0.0,
                                             reg_lambda=0.0, subsample=1.0,
                                             subsample_for_bin=200000,
                                             subsample_freq=0))],
                 flatten_transform=True, n_jobs=-1, verbose=False,
                 voting='hard', weights=None), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.classification.oop.ClassificationExperiment object at 0x2a279fcd0>, system=True)
2023-10-31 14:03:48,605:INFO:Checking exceptions
2023-10-31 14:03:48,613:INFO:Preloading libraries
2023-10-31 14:03:48,656:INFO:Copying training dataset
2023-10-31 14:03:48,656:INFO:Plot type: pr
2023-10-31 14:03:49,376:INFO:Fitting Model
2023-10-31 14:03:49,376:WARNING:X does not have valid feature names, but ExtraTreesClassifier was fitted with feature names

2023-10-31 14:03:49,376:INFO:Scoring test/hold-out set
2023-10-31 14:03:56,420:INFO:Initializing plot_model()
2023-10-31 14:03:56,420:INFO:plot_model(plot=error, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), verbose=False, display=None, display_format=None, estimator=VotingClassifier(estimators=[('Extra Trees Classifier',
                              ExtraTreesClassifier(bootstrap=False,
                                                   ccp_alpha=0.0,
                                                   class_weight=None,
                                                   criterion='gini',
                                                   max_depth=None,
                                                   max_features='sqrt',
                                                   max_leaf_nodes=None,
                                                   max_samples=None,
                                                   min_impurity_decrease=0.0,
                                                   min_samples_leaf=1,
                                                   min_samples_split=2,
                                                   min_weight_fraction_leaf=0.0,
                                                   n_estimators=100, n_jobs=-1,
                                                   oob_score=False,
                                                   ra...
                                             importance_type='split',
                                             learning_rate=0.1, max_depth=-1,
                                             min_child_samples=20,
                                             min_child_weight=0.001,
                                             min_split_gain=0.0,
                                             n_estimators=100, n_jobs=-1,
                                             num_leaves=31, objective=None,
                                             random_state=1784, reg_alpha=0.0,
                                             reg_lambda=0.0, subsample=1.0,
                                             subsample_for_bin=200000,
                                             subsample_freq=0))],
                 flatten_transform=True, n_jobs=-1, verbose=False,
                 voting='hard', weights=None), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.classification.oop.ClassificationExperiment object at 0x2a279fcd0>, system=True)
2023-10-31 14:03:56,420:INFO:Checking exceptions
2023-10-31 14:03:56,426:INFO:Preloading libraries
2023-10-31 14:03:56,459:INFO:Copying training dataset
2023-10-31 14:03:56,459:INFO:Plot type: error
2023-10-31 14:03:57,175:INFO:Fitting Model
2023-10-31 14:03:57,175:WARNING:X does not have valid feature names, but ExtraTreesClassifier was fitted with feature names

2023-10-31 14:03:57,175:INFO:Scoring test/hold-out set
2023-10-31 14:03:57,516:INFO:Visual Rendered Successfully
2023-10-31 14:03:57,633:INFO:plot_model() successfully completed......................................
2023-10-31 14:04:11,178:INFO:Initializing plot_model()
2023-10-31 14:04:11,178:INFO:plot_model(plot=tree, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), verbose=False, display=None, display_format=None, estimator=VotingClassifier(estimators=[('Extra Trees Classifier',
                              ExtraTreesClassifier(bootstrap=False,
                                                   ccp_alpha=0.0,
                                                   class_weight=None,
                                                   criterion='gini',
                                                   max_depth=None,
                                                   max_features='sqrt',
                                                   max_leaf_nodes=None,
                                                   max_samples=None,
                                                   min_impurity_decrease=0.0,
                                                   min_samples_leaf=1,
                                                   min_samples_split=2,
                                                   min_weight_fraction_leaf=0.0,
                                                   n_estimators=100, n_jobs=-1,
                                                   oob_score=False,
                                                   ra...
                                             importance_type='split',
                                             learning_rate=0.1, max_depth=-1,
                                             min_child_samples=20,
                                             min_child_weight=0.001,
                                             min_split_gain=0.0,
                                             n_estimators=100, n_jobs=-1,
                                             num_leaves=31, objective=None,
                                             random_state=1784, reg_alpha=0.0,
                                             reg_lambda=0.0, subsample=1.0,
                                             subsample_for_bin=200000,
                                             subsample_freq=0))],
                 flatten_transform=True, n_jobs=-1, verbose=False,
                 voting='hard', weights=None), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.classification.oop.ClassificationExperiment object at 0x2a279fcd0>, system=True)
2023-10-31 14:04:11,178:INFO:Checking exceptions
2023-10-31 14:04:12,338:INFO:Initializing plot_model()
2023-10-31 14:04:12,338:INFO:plot_model(plot=gain, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), verbose=False, display=None, display_format=None, estimator=VotingClassifier(estimators=[('Extra Trees Classifier',
                              ExtraTreesClassifier(bootstrap=False,
                                                   ccp_alpha=0.0,
                                                   class_weight=None,
                                                   criterion='gini',
                                                   max_depth=None,
                                                   max_features='sqrt',
                                                   max_leaf_nodes=None,
                                                   max_samples=None,
                                                   min_impurity_decrease=0.0,
                                                   min_samples_leaf=1,
                                                   min_samples_split=2,
                                                   min_weight_fraction_leaf=0.0,
                                                   n_estimators=100, n_jobs=-1,
                                                   oob_score=False,
                                                   ra...
                                             importance_type='split',
                                             learning_rate=0.1, max_depth=-1,
                                             min_child_samples=20,
                                             min_child_weight=0.001,
                                             min_split_gain=0.0,
                                             n_estimators=100, n_jobs=-1,
                                             num_leaves=31, objective=None,
                                             random_state=1784, reg_alpha=0.0,
                                             reg_lambda=0.0, subsample=1.0,
                                             subsample_for_bin=200000,
                                             subsample_freq=0))],
                 flatten_transform=True, n_jobs=-1, verbose=False,
                 voting='hard', weights=None), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.classification.oop.ClassificationExperiment object at 0x2a279fcd0>, system=True)
2023-10-31 14:04:12,339:INFO:Checking exceptions
2023-10-31 14:04:12,346:INFO:Preloading libraries
2023-10-31 14:04:12,384:INFO:Copying training dataset
2023-10-31 14:04:12,384:INFO:Plot type: gain
2023-10-31 14:04:12,385:INFO:Generating predictions / predict_proba on X_test
2023-10-31 14:04:13,152:INFO:Initializing plot_model()
2023-10-31 14:04:13,152:INFO:plot_model(plot=learning, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), verbose=False, display=None, display_format=None, estimator=VotingClassifier(estimators=[('Extra Trees Classifier',
                              ExtraTreesClassifier(bootstrap=False,
                                                   ccp_alpha=0.0,
                                                   class_weight=None,
                                                   criterion='gini',
                                                   max_depth=None,
                                                   max_features='sqrt',
                                                   max_leaf_nodes=None,
                                                   max_samples=None,
                                                   min_impurity_decrease=0.0,
                                                   min_samples_leaf=1,
                                                   min_samples_split=2,
                                                   min_weight_fraction_leaf=0.0,
                                                   n_estimators=100, n_jobs=-1,
                                                   oob_score=False,
                                                   ra...
                                             importance_type='split',
                                             learning_rate=0.1, max_depth=-1,
                                             min_child_samples=20,
                                             min_child_weight=0.001,
                                             min_split_gain=0.0,
                                             n_estimators=100, n_jobs=-1,
                                             num_leaves=31, objective=None,
                                             random_state=1784, reg_alpha=0.0,
                                             reg_lambda=0.0, subsample=1.0,
                                             subsample_for_bin=200000,
                                             subsample_freq=0))],
                 flatten_transform=True, n_jobs=-1, verbose=False,
                 voting='hard', weights=None), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.classification.oop.ClassificationExperiment object at 0x2a279fcd0>, system=True)
2023-10-31 14:04:13,152:INFO:Checking exceptions
2023-10-31 14:04:13,160:INFO:Preloading libraries
2023-10-31 14:04:13,197:INFO:Copying training dataset
2023-10-31 14:04:13,197:INFO:Plot type: learning
2023-10-31 14:04:13,922:INFO:Fitting Model
2023-10-31 14:04:15,760:INFO:[LightGBM] [Info] Number of positive: 1368, number of negative: 1389
2023-10-31 14:04:15,767:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000807 seconds.
2023-10-31 14:04:15,767:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 14:04:15,767:INFO:[LightGBM] [Info] Total Bins 15045
2023-10-31 14:04:15,767:INFO:[LightGBM] [Info] Number of data points in the train set: 2757, number of used features: 59
2023-10-31 14:04:15,767:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496192 -> initscore=-0.015234
2023-10-31 14:04:15,767:INFO:[LightGBM] [Info] Start training from score -0.015234
2023-10-31 14:04:15,767:INFO:[LightGBM] [Info] Number of positive: 1148, number of negative: 1138
2023-10-31 14:04:15,779:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001023 seconds.
2023-10-31 14:04:15,779:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 14:04:15,779:INFO:[LightGBM] [Info] Total Bins 15045
2023-10-31 14:04:15,779:INFO:[LightGBM] [Info] Number of data points in the train set: 2286, number of used features: 59
2023-10-31 14:04:15,779:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.502187 -> initscore=0.008749
2023-10-31 14:04:15,779:INFO:[LightGBM] [Info] Start training from score 0.008749
2023-10-31 14:04:15,798:INFO:[LightGBM] [Info] Number of positive: 2065, number of negative: 2104
2023-10-31 14:04:15,799:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001136 seconds.
2023-10-31 14:04:15,799:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 14:04:15,799:INFO:[LightGBM] [Info] Total Bins 15045
2023-10-31 14:04:15,799:INFO:[LightGBM] [Info] Number of data points in the train set: 4169, number of used features: 59
2023-10-31 14:04:15,799:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495323 -> initscore=-0.018710
2023-10-31 14:04:15,799:INFO:[LightGBM] [Info] Start training from score -0.018710
2023-10-31 14:04:15,800:INFO:[LightGBM] [Info] Number of positive: 1588, number of negative: 1640
2023-10-31 14:04:15,800:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000862 seconds.
2023-10-31 14:04:15,800:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 14:04:15,800:INFO:[LightGBM] [Info] Total Bins 15045
2023-10-31 14:04:15,800:INFO:[LightGBM] [Info] Number of data points in the train set: 3228, number of used features: 59
2023-10-31 14:04:15,800:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.491945 -> initscore=-0.032221
2023-10-31 14:04:15,800:INFO:[LightGBM] [Info] Start training from score -0.032221
2023-10-31 14:04:15,818:INFO:[LightGBM] [Info] Number of positive: 2290, number of negative: 2350
2023-10-31 14:04:15,819:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001334 seconds.
2023-10-31 14:04:15,819:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 14:04:15,820:INFO:[LightGBM] [Info] Total Bins 15045
2023-10-31 14:04:15,820:INFO:[LightGBM] [Info] Number of positive: 2980, number of negative: 3073
2023-10-31 14:04:15,820:INFO:[LightGBM] [Info] Number of positive: 1820, number of negative: 1879
2023-10-31 14:04:15,821:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001678 seconds.
2023-10-31 14:04:15,821:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 14:04:15,821:INFO:[LightGBM] [Info] Total Bins 15045
2023-10-31 14:04:15,822:INFO:[LightGBM] [Info] Number of data points in the train set: 6053, number of used features: 59
2023-10-31 14:04:15,823:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002926 seconds.
2023-10-31 14:04:15,823:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 14:04:15,823:INFO:[LightGBM] [Info] Total Bins 15045
2023-10-31 14:04:15,824:INFO:[LightGBM] [Info] Number of data points in the train set: 3699, number of used features: 59
2023-10-31 14:04:15,824:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.492025 -> initscore=-0.031903
2023-10-31 14:04:15,824:INFO:[LightGBM] [Info] Start training from score -0.031903
2023-10-31 14:04:15,829:INFO:[LightGBM] [Info] Number of data points in the train set: 4640, number of used features: 59
2023-10-31 14:04:15,829:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.493534 -> initscore=-0.025864
2023-10-31 14:04:15,829:INFO:[LightGBM] [Info] Start training from score -0.025864
2023-10-31 14:04:15,841:INFO:[LightGBM] [Info] Number of positive: 909, number of negative: 906
2023-10-31 14:04:15,842:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000525 seconds.
2023-10-31 14:04:15,842:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 14:04:15,842:INFO:[LightGBM] [Info] Total Bins 15045
2023-10-31 14:04:15,843:INFO:[LightGBM] [Info] Number of data points in the train set: 1815, number of used features: 59
2023-10-31 14:04:15,843:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500826 -> initscore=0.003306
2023-10-31 14:04:15,843:INFO:[LightGBM] [Info] Start training from score 0.003306
2023-10-31 14:04:15,847:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.492318 -> initscore=-0.030731
2023-10-31 14:04:15,847:INFO:[LightGBM] [Info] Start training from score -0.030731
2023-10-31 14:04:15,853:INFO:[LightGBM] [Info] Number of positive: 2521, number of negative: 2590
2023-10-31 14:04:15,854:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001538 seconds.
2023-10-31 14:04:15,854:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 14:04:15,855:INFO:[LightGBM] [Info] Total Bins 15045
2023-10-31 14:04:15,855:INFO:[LightGBM] [Info] Number of data points in the train set: 5111, number of used features: 59
2023-10-31 14:04:15,855:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.493250 -> initscore=-0.027002
2023-10-31 14:04:15,855:INFO:[LightGBM] [Info] Start training from score -0.027002
2023-10-31 14:04:15,872:INFO:[LightGBM] [Info] Number of positive: 2747, number of negative: 2835
2023-10-31 14:04:15,873:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001536 seconds.
2023-10-31 14:04:15,873:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 14:04:15,873:INFO:[LightGBM] [Info] Total Bins 15045
2023-10-31 14:04:15,876:INFO:[LightGBM] [Info] Number of data points in the train set: 5582, number of used features: 59
2023-10-31 14:04:15,877:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.492118 -> initscore=-0.031533
2023-10-31 14:04:15,877:INFO:[LightGBM] [Info] Start training from score -0.031533
2023-10-31 14:04:30,534:INFO:[LightGBM] [Info] Number of positive: 1148, number of negative: 1138
2023-10-31 14:04:30,535:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000643 seconds.
2023-10-31 14:04:30,535:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 14:04:30,535:INFO:[LightGBM] [Info] Total Bins 15045
2023-10-31 14:04:30,536:INFO:[LightGBM] [Info] Number of data points in the train set: 2286, number of used features: 59
2023-10-31 14:04:30,536:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.502187 -> initscore=0.008749
2023-10-31 14:04:30,536:INFO:[LightGBM] [Info] Start training from score 0.008749
2023-10-31 14:04:30,539:INFO:[LightGBM] [Info] Number of positive: 909, number of negative: 906
2023-10-31 14:04:30,539:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000507 seconds.
2023-10-31 14:04:30,539:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 14:04:30,539:INFO:[LightGBM] [Info] Total Bins 15045
2023-10-31 14:04:30,540:INFO:[LightGBM] [Info] Number of data points in the train set: 1815, number of used features: 59
2023-10-31 14:04:30,541:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500826 -> initscore=0.003306
2023-10-31 14:04:30,541:INFO:[LightGBM] [Info] Start training from score 0.003306
2023-10-31 14:04:30,881:INFO:[LightGBM] [Info] Number of positive: 1368, number of negative: 1389
2023-10-31 14:04:30,882:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000770 seconds.
2023-10-31 14:04:30,882:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 14:04:30,882:INFO:[LightGBM] [Info] Total Bins 15045
2023-10-31 14:04:30,882:INFO:[LightGBM] [Info] Number of data points in the train set: 2757, number of used features: 59
2023-10-31 14:04:30,884:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496192 -> initscore=-0.015234
2023-10-31 14:04:30,884:INFO:[LightGBM] [Info] Start training from score -0.015234
2023-10-31 14:04:31,671:INFO:[LightGBM] [Info] Number of positive: 1588, number of negative: 1640
2023-10-31 14:04:31,673:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000878 seconds.
2023-10-31 14:04:31,673:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 14:04:31,673:INFO:[LightGBM] [Info] Total Bins 15045
2023-10-31 14:04:31,673:INFO:[LightGBM] [Info] Number of data points in the train set: 3228, number of used features: 59
2023-10-31 14:04:31,674:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.491945 -> initscore=-0.032221
2023-10-31 14:04:31,674:INFO:[LightGBM] [Info] Start training from score -0.032221
2023-10-31 14:04:32,038:INFO:[LightGBM] [Info] Number of positive: 1820, number of negative: 1879
2023-10-31 14:04:32,039:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001115 seconds.
2023-10-31 14:04:32,040:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 14:04:32,040:INFO:[LightGBM] [Info] Total Bins 15045
2023-10-31 14:04:32,044:INFO:[LightGBM] [Info] Number of data points in the train set: 3699, number of used features: 59
2023-10-31 14:04:32,044:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.492025 -> initscore=-0.031903
2023-10-31 14:04:32,044:INFO:[LightGBM] [Info] Start training from score -0.031903
2023-10-31 14:04:32,411:INFO:[LightGBM] [Info] Number of positive: 2065, number of negative: 2104
2023-10-31 14:04:32,411:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001124 seconds.
2023-10-31 14:04:32,411:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 14:04:32,411:INFO:[LightGBM] [Info] Total Bins 15045
2023-10-31 14:04:32,411:INFO:[LightGBM] [Info] Number of data points in the train set: 4169, number of used features: 59
2023-10-31 14:04:32,411:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495323 -> initscore=-0.018710
2023-10-31 14:04:32,411:INFO:[LightGBM] [Info] Start training from score -0.018710
2023-10-31 14:04:33,572:INFO:[LightGBM] [Info] Number of positive: 2290, number of negative: 2350
2023-10-31 14:04:33,573:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001371 seconds.
2023-10-31 14:04:33,573:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 14:04:33,573:INFO:[LightGBM] [Info] Total Bins 15045
2023-10-31 14:04:33,576:INFO:[LightGBM] [Info] Number of data points in the train set: 4640, number of used features: 59
2023-10-31 14:04:33,576:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.493534 -> initscore=-0.025864
2023-10-31 14:04:33,576:INFO:[LightGBM] [Info] Start training from score -0.025864
2023-10-31 14:04:33,707:INFO:[LightGBM] [Info] Number of positive: 2521, number of negative: 2590
2023-10-31 14:04:33,710:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001359 seconds.
2023-10-31 14:04:33,710:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 14:04:33,710:INFO:[LightGBM] [Info] Total Bins 15045
2023-10-31 14:04:33,710:INFO:[LightGBM] [Info] Number of data points in the train set: 5111, number of used features: 59
2023-10-31 14:04:33,710:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.493250 -> initscore=-0.027002
2023-10-31 14:04:33,710:INFO:[LightGBM] [Info] Start training from score -0.027002
2023-10-31 14:04:34,402:INFO:[LightGBM] [Info] Number of positive: 2747, number of negative: 2835
2023-10-31 14:04:34,404:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001482 seconds.
2023-10-31 14:04:34,404:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 14:04:34,404:INFO:[LightGBM] [Info] Total Bins 15045
2023-10-31 14:04:34,404:INFO:[LightGBM] [Info] Number of data points in the train set: 5582, number of used features: 59
2023-10-31 14:04:34,404:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.492118 -> initscore=-0.031533
2023-10-31 14:04:34,404:INFO:[LightGBM] [Info] Start training from score -0.031533
2023-10-31 14:04:34,416:INFO:[LightGBM] [Info] Number of positive: 2980, number of negative: 3073
2023-10-31 14:04:34,418:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001624 seconds.
2023-10-31 14:04:34,420:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 14:04:34,420:INFO:[LightGBM] [Info] Total Bins 15045
2023-10-31 14:04:34,425:INFO:[LightGBM] [Info] Number of data points in the train set: 6053, number of used features: 59
2023-10-31 14:04:34,432:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.492318 -> initscore=-0.030731
2023-10-31 14:04:34,432:INFO:[LightGBM] [Info] Start training from score -0.030731
2023-10-31 14:04:45,642:INFO:[LightGBM] [Info] Number of positive: 910, number of negative: 905
2023-10-31 14:04:45,643:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000536 seconds.
2023-10-31 14:04:45,643:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 14:04:45,643:INFO:[LightGBM] [Info] Total Bins 15045
2023-10-31 14:04:45,644:INFO:[LightGBM] [Info] Number of data points in the train set: 1815, number of used features: 59
2023-10-31 14:04:45,644:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501377 -> initscore=0.005510
2023-10-31 14:04:45,644:INFO:[LightGBM] [Info] Start training from score 0.005510
2023-10-31 14:04:45,760:INFO:[LightGBM] [Info] Number of positive: 1149, number of negative: 1137
2023-10-31 14:04:45,760:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000629 seconds.
2023-10-31 14:04:45,760:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 14:04:45,760:INFO:[LightGBM] [Info] Total Bins 15045
2023-10-31 14:04:45,760:INFO:[LightGBM] [Info] Number of data points in the train set: 2286, number of used features: 59
2023-10-31 14:04:45,760:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.502625 -> initscore=0.010499
2023-10-31 14:04:45,760:INFO:[LightGBM] [Info] Start training from score 0.010499
2023-10-31 14:04:46,603:INFO:[LightGBM] [Info] Number of positive: 1369, number of negative: 1388
2023-10-31 14:04:46,606:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003318 seconds.
2023-10-31 14:04:46,606:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 14:04:46,606:INFO:[LightGBM] [Info] Total Bins 15045
2023-10-31 14:04:46,607:INFO:[LightGBM] [Info] Number of data points in the train set: 2757, number of used features: 59
2023-10-31 14:04:46,616:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496554 -> initscore=-0.013783
2023-10-31 14:04:46,617:INFO:[LightGBM] [Info] Start training from score -0.013783
2023-10-31 14:04:48,856:INFO:[LightGBM] [Info] Number of positive: 1589, number of negative: 1639
2023-10-31 14:04:48,856:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000902 seconds.
2023-10-31 14:04:48,856:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 14:04:48,856:INFO:[LightGBM] [Info] Total Bins 15045
2023-10-31 14:04:48,856:INFO:[LightGBM] [Info] Number of data points in the train set: 3228, number of used features: 59
2023-10-31 14:04:48,856:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.492255 -> initscore=-0.030981
2023-10-31 14:04:48,856:INFO:[LightGBM] [Info] Start training from score -0.030981
2023-10-31 14:04:48,913:INFO:[LightGBM] [Info] Number of positive: 1821, number of negative: 1878
2023-10-31 14:04:48,915:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002567 seconds.
2023-10-31 14:04:48,915:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 14:04:48,915:INFO:[LightGBM] [Info] Total Bins 15045
2023-10-31 14:04:48,916:INFO:[LightGBM] [Info] Number of data points in the train set: 3699, number of used features: 59
2023-10-31 14:04:48,916:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.492295 -> initscore=-0.030822
2023-10-31 14:04:48,916:INFO:[LightGBM] [Info] Start training from score -0.030822
2023-10-31 14:04:49,825:INFO:[LightGBM] [Info] Number of positive: 2066, number of negative: 2103
2023-10-31 14:04:49,825:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001162 seconds.
2023-10-31 14:04:49,825:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 14:04:49,825:INFO:[LightGBM] [Info] Total Bins 15045
2023-10-31 14:04:49,825:INFO:[LightGBM] [Info] Number of data points in the train set: 4169, number of used features: 59
2023-10-31 14:04:49,829:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495562 -> initscore=-0.017751
2023-10-31 14:04:49,829:INFO:[LightGBM] [Info] Start training from score -0.017751
2023-10-31 14:04:52,164:INFO:[LightGBM] [Info] Number of positive: 2291, number of negative: 2349
2023-10-31 14:04:52,165:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001262 seconds.
2023-10-31 14:04:52,165:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 14:04:52,165:INFO:[LightGBM] [Info] Total Bins 15045
2023-10-31 14:04:52,166:INFO:[LightGBM] [Info] Number of data points in the train set: 4640, number of used features: 59
2023-10-31 14:04:52,166:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.493750 -> initscore=-0.025001
2023-10-31 14:04:52,166:INFO:[LightGBM] [Info] Start training from score -0.025001
2023-10-31 14:04:52,821:INFO:[LightGBM] [Info] Number of positive: 2522, number of negative: 2589
2023-10-31 14:04:52,823:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001406 seconds.
2023-10-31 14:04:52,823:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 14:04:52,823:INFO:[LightGBM] [Info] Total Bins 15045
2023-10-31 14:04:52,824:INFO:[LightGBM] [Info] Number of data points in the train set: 5111, number of used features: 59
2023-10-31 14:04:52,824:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.493446 -> initscore=-0.026219
2023-10-31 14:04:52,824:INFO:[LightGBM] [Info] Start training from score -0.026219
2023-10-31 14:04:53,823:INFO:[LightGBM] [Info] Number of positive: 2748, number of negative: 2834
2023-10-31 14:04:53,825:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001500 seconds.
2023-10-31 14:04:53,825:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 14:04:53,825:INFO:[LightGBM] [Info] Total Bins 15045
2023-10-31 14:04:53,825:INFO:[LightGBM] [Info] Number of data points in the train set: 5582, number of used features: 59
2023-10-31 14:04:53,825:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.492297 -> initscore=-0.030816
2023-10-31 14:04:53,825:INFO:[LightGBM] [Info] Start training from score -0.030816
2023-10-31 14:04:53,827:INFO:[LightGBM] [Info] Number of positive: 2981, number of negative: 3072
2023-10-31 14:04:53,829:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001634 seconds.
2023-10-31 14:04:53,829:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 14:04:53,829:INFO:[LightGBM] [Info] Total Bins 15045
2023-10-31 14:04:53,829:INFO:[LightGBM] [Info] Number of data points in the train set: 6053, number of used features: 59
2023-10-31 14:04:53,829:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.492483 -> initscore=-0.030070
2023-10-31 14:04:53,829:INFO:[LightGBM] [Info] Start training from score -0.030070
2023-10-31 14:05:00,783:INFO:[LightGBM] [Info] Number of positive: 895, number of negative: 920
2023-10-31 14:05:00,784:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000526 seconds.
2023-10-31 14:05:00,784:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 14:05:00,784:INFO:[LightGBM] [Info] Total Bins 15045
2023-10-31 14:05:00,785:INFO:[LightGBM] [Info] Number of data points in the train set: 1815, number of used features: 59
2023-10-31 14:05:00,785:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.493113 -> initscore=-0.027550
2023-10-31 14:05:00,785:INFO:[LightGBM] [Info] Start training from score -0.027550
2023-10-31 14:05:01,046:INFO:[LightGBM] [Info] Number of positive: 1149, number of negative: 1137
2023-10-31 14:05:01,047:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000636 seconds.
2023-10-31 14:05:01,047:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 14:05:01,047:INFO:[LightGBM] [Info] Total Bins 15045
2023-10-31 14:05:01,053:INFO:[LightGBM] [Info] Number of data points in the train set: 2286, number of used features: 59
2023-10-31 14:05:01,053:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.502625 -> initscore=0.010499
2023-10-31 14:05:01,053:INFO:[LightGBM] [Info] Start training from score 0.010499
2023-10-31 14:05:03,583:INFO:[LightGBM] [Info] Number of positive: 1369, number of negative: 1388
2023-10-31 14:05:03,584:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000772 seconds.
2023-10-31 14:05:03,584:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 14:05:03,584:INFO:[LightGBM] [Info] Total Bins 15045
2023-10-31 14:05:03,584:INFO:[LightGBM] [Info] Number of data points in the train set: 2757, number of used features: 59
2023-10-31 14:05:03,584:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496554 -> initscore=-0.013783
2023-10-31 14:05:03,584:INFO:[LightGBM] [Info] Start training from score -0.013783
2023-10-31 14:05:06,017:INFO:[LightGBM] [Info] Number of positive: 1589, number of negative: 1639
2023-10-31 14:05:06,017:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003969 seconds.
2023-10-31 14:05:06,017:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 14:05:06,017:INFO:[LightGBM] [Info] Total Bins 15045
2023-10-31 14:05:06,017:INFO:[LightGBM] [Info] Number of data points in the train set: 3228, number of used features: 59
2023-10-31 14:05:06,018:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.492255 -> initscore=-0.030981
2023-10-31 14:05:06,018:INFO:[LightGBM] [Info] Start training from score -0.030981
2023-10-31 14:05:06,375:INFO:[LightGBM] [Info] Number of positive: 1821, number of negative: 1878
2023-10-31 14:05:06,388:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004679 seconds.
2023-10-31 14:05:06,388:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 14:05:06,388:INFO:[LightGBM] [Info] Total Bins 15045
2023-10-31 14:05:06,388:INFO:[LightGBM] [Info] Number of data points in the train set: 3699, number of used features: 59
2023-10-31 14:05:06,388:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.492295 -> initscore=-0.030822
2023-10-31 14:05:06,388:INFO:[LightGBM] [Info] Start training from score -0.030822
2023-10-31 14:05:06,956:INFO:[LightGBM] [Info] Number of positive: 2066, number of negative: 2103
2023-10-31 14:05:06,957:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001132 seconds.
2023-10-31 14:05:06,957:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 14:05:06,957:INFO:[LightGBM] [Info] Total Bins 15045
2023-10-31 14:05:06,958:INFO:[LightGBM] [Info] Number of data points in the train set: 4169, number of used features: 59
2023-10-31 14:05:06,958:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495562 -> initscore=-0.017751
2023-10-31 14:05:06,958:INFO:[LightGBM] [Info] Start training from score -0.017751
2023-10-31 14:05:10,001:INFO:[LightGBM] [Info] Number of positive: 2291, number of negative: 2349
2023-10-31 14:05:10,003:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001263 seconds.
2023-10-31 14:05:10,003:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 14:05:10,003:INFO:[LightGBM] [Info] Total Bins 15045
2023-10-31 14:05:10,004:INFO:[LightGBM] [Info] Number of data points in the train set: 4640, number of used features: 59
2023-10-31 14:05:10,004:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.493750 -> initscore=-0.025001
2023-10-31 14:05:10,004:INFO:[LightGBM] [Info] Start training from score -0.025001
2023-10-31 14:05:10,727:INFO:[LightGBM] [Info] Number of positive: 2522, number of negative: 2589
2023-10-31 14:05:10,730:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002885 seconds.
2023-10-31 14:05:10,731:INFO:You can set `force_row_wise=true` to remove the overhead.
2023-10-31 14:05:10,731:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2023-10-31 14:05:10,732:INFO:[LightGBM] [Info] Total Bins 15045
2023-10-31 14:05:10,732:INFO:[LightGBM] [Info] Number of data points in the train set: 5111, number of used features: 59
2023-10-31 14:05:10,732:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.493446 -> initscore=-0.026219
2023-10-31 14:05:10,732:INFO:[LightGBM] [Info] Start training from score -0.026219
2023-10-31 14:05:10,924:INFO:[LightGBM] [Info] Number of positive: 2748, number of negative: 2834
2023-10-31 14:05:10,925:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001476 seconds.
2023-10-31 14:05:10,925:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 14:05:10,925:INFO:[LightGBM] [Info] Total Bins 15045
2023-10-31 14:05:10,925:INFO:[LightGBM] [Info] Number of data points in the train set: 5582, number of used features: 59
2023-10-31 14:05:10,925:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.492297 -> initscore=-0.030816
2023-10-31 14:05:10,925:INFO:[LightGBM] [Info] Start training from score -0.030816
2023-10-31 14:05:12,978:INFO:[LightGBM] [Info] Number of positive: 2981, number of negative: 3072
2023-10-31 14:05:12,999:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.020246 seconds.
2023-10-31 14:05:12,999:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 14:05:12,999:INFO:[LightGBM] [Info] Total Bins 15045
2023-10-31 14:05:13,000:INFO:[LightGBM] [Info] Number of data points in the train set: 6053, number of used features: 59
2023-10-31 14:05:13,001:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.492483 -> initscore=-0.030070
2023-10-31 14:05:13,001:INFO:[LightGBM] [Info] Start training from score -0.030070
2023-10-31 14:05:15,504:INFO:[LightGBM] [Info] Number of positive: 895, number of negative: 920
2023-10-31 14:05:15,504:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000595 seconds.
2023-10-31 14:05:15,504:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 14:05:15,504:INFO:[LightGBM] [Info] Total Bins 15045
2023-10-31 14:05:15,504:INFO:[LightGBM] [Info] Number of data points in the train set: 1815, number of used features: 59
2023-10-31 14:05:15,504:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.493113 -> initscore=-0.027550
2023-10-31 14:05:15,504:INFO:[LightGBM] [Info] Start training from score -0.027550
2023-10-31 14:05:15,733:INFO:[LightGBM] [Info] Number of positive: 1143, number of negative: 1143
2023-10-31 14:05:15,733:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000619 seconds.
2023-10-31 14:05:15,734:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 14:05:15,734:INFO:[LightGBM] [Info] Total Bins 15045
2023-10-31 14:05:15,734:INFO:[LightGBM] [Info] Number of data points in the train set: 2286, number of used features: 59
2023-10-31 14:05:15,734:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000
2023-10-31 14:05:19,807:INFO:[LightGBM] [Info] Number of positive: 1369, number of negative: 1388
2023-10-31 14:05:19,810:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000836 seconds.
2023-10-31 14:05:19,811:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 14:05:19,811:INFO:[LightGBM] [Info] Total Bins 15045
2023-10-31 14:05:19,811:INFO:[LightGBM] [Info] Number of data points in the train set: 2757, number of used features: 59
2023-10-31 14:05:19,811:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496554 -> initscore=-0.013783
2023-10-31 14:05:19,811:INFO:[LightGBM] [Info] Start training from score -0.013783
2023-10-31 14:05:22,947:INFO:[LightGBM] [Info] Number of positive: 1589, number of negative: 1639
2023-10-31 14:05:22,948:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000866 seconds.
2023-10-31 14:05:22,948:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 14:05:22,948:INFO:[LightGBM] [Info] Total Bins 15045
2023-10-31 14:05:22,948:INFO:[LightGBM] [Info] Number of data points in the train set: 3228, number of used features: 59
2023-10-31 14:05:22,948:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.492255 -> initscore=-0.030981
2023-10-31 14:05:22,948:INFO:[LightGBM] [Info] Start training from score -0.030981
2023-10-31 14:05:23,228:INFO:[LightGBM] [Info] Number of positive: 1821, number of negative: 1878
2023-10-31 14:05:23,228:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001074 seconds.
2023-10-31 14:05:23,228:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 14:05:23,228:INFO:[LightGBM] [Info] Total Bins 15045
2023-10-31 14:05:23,229:INFO:[LightGBM] [Info] Number of data points in the train set: 3699, number of used features: 59
2023-10-31 14:05:23,229:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.492295 -> initscore=-0.030822
2023-10-31 14:05:23,229:INFO:[LightGBM] [Info] Start training from score -0.030822
2023-10-31 14:05:24,247:INFO:[LightGBM] [Info] Number of positive: 2066, number of negative: 2103
2023-10-31 14:05:24,248:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001114 seconds.
2023-10-31 14:05:24,248:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 14:05:24,248:INFO:[LightGBM] [Info] Total Bins 15045
2023-10-31 14:05:24,248:INFO:[LightGBM] [Info] Number of data points in the train set: 4169, number of used features: 59
2023-10-31 14:05:24,248:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495562 -> initscore=-0.017751
2023-10-31 14:05:24,249:INFO:[LightGBM] [Info] Start training from score -0.017751
2023-10-31 14:05:28,126:INFO:[LightGBM] [Info] Number of positive: 2291, number of negative: 2349
2023-10-31 14:05:28,128:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001287 seconds.
2023-10-31 14:05:28,128:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 14:05:28,128:INFO:[LightGBM] [Info] Total Bins 15045
2023-10-31 14:05:28,128:INFO:[LightGBM] [Info] Number of data points in the train set: 4640, number of used features: 59
2023-10-31 14:05:28,129:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.493750 -> initscore=-0.025001
2023-10-31 14:05:28,129:INFO:[LightGBM] [Info] Start training from score -0.025001
2023-10-31 14:05:29,327:INFO:[LightGBM] [Info] Number of positive: 2522, number of negative: 2589
2023-10-31 14:05:29,328:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001401 seconds.
2023-10-31 14:05:29,329:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 14:05:29,329:INFO:[LightGBM] [Info] Total Bins 15045
2023-10-31 14:05:29,329:INFO:[LightGBM] [Info] Number of data points in the train set: 5111, number of used features: 59
2023-10-31 14:05:29,329:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.493446 -> initscore=-0.026219
2023-10-31 14:05:29,329:INFO:[LightGBM] [Info] Start training from score -0.026219
2023-10-31 14:05:29,756:INFO:[LightGBM] [Info] Number of positive: 2748, number of negative: 2834
2023-10-31 14:05:29,758:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001498 seconds.
2023-10-31 14:05:29,758:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 14:05:29,758:INFO:[LightGBM] [Info] Total Bins 15045
2023-10-31 14:05:29,758:INFO:[LightGBM] [Info] Number of data points in the train set: 5582, number of used features: 59
2023-10-31 14:05:29,758:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.492297 -> initscore=-0.030816
2023-10-31 14:05:29,758:INFO:[LightGBM] [Info] Start training from score -0.030816
2023-10-31 14:05:30,021:INFO:[LightGBM] [Info] Number of positive: 2981, number of negative: 3072
2023-10-31 14:05:30,022:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001724 seconds.
2023-10-31 14:05:30,023:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 14:05:30,023:INFO:[LightGBM] [Info] Total Bins 15045
2023-10-31 14:05:30,023:INFO:[LightGBM] [Info] Number of data points in the train set: 6053, number of used features: 59
2023-10-31 14:05:30,026:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.492483 -> initscore=-0.030070
2023-10-31 14:05:30,026:INFO:[LightGBM] [Info] Start training from score -0.030070
2023-10-31 14:05:30,308:INFO:[LightGBM] [Info] Number of positive: 895, number of negative: 920
2023-10-31 14:05:30,308:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000527 seconds.
2023-10-31 14:05:30,309:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 14:05:30,309:INFO:[LightGBM] [Info] Total Bins 15045
2023-10-31 14:05:30,309:INFO:[LightGBM] [Info] Number of data points in the train set: 1815, number of used features: 59
2023-10-31 14:05:30,309:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.493113 -> initscore=-0.027550
2023-10-31 14:05:30,309:INFO:[LightGBM] [Info] Start training from score -0.027550
2023-10-31 14:05:32,019:INFO:[LightGBM] [Info] Number of positive: 1143, number of negative: 1143
2023-10-31 14:05:32,020:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000645 seconds.
2023-10-31 14:05:32,020:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 14:05:32,020:INFO:[LightGBM] [Info] Total Bins 15045
2023-10-31 14:05:32,020:INFO:[LightGBM] [Info] Number of data points in the train set: 2286, number of used features: 59
2023-10-31 14:05:32,029:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000
2023-10-31 14:05:35,412:INFO:[LightGBM] [Info] Number of positive: 1376, number of negative: 1381
2023-10-31 14:05:35,413:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000770 seconds.
2023-10-31 14:05:35,413:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 14:05:35,414:INFO:[LightGBM] [Info] Total Bins 15045
2023-10-31 14:05:35,414:INFO:[LightGBM] [Info] Number of data points in the train set: 2757, number of used features: 59
2023-10-31 14:05:35,414:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499093 -> initscore=-0.003627
2023-10-31 14:05:35,414:INFO:[LightGBM] [Info] Start training from score -0.003627
2023-10-31 14:05:38,331:INFO:[LightGBM] [Info] Number of positive: 1598, number of negative: 1630
2023-10-31 14:05:38,332:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000911 seconds.
2023-10-31 14:05:38,332:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 14:05:38,332:INFO:[LightGBM] [Info] Total Bins 15045
2023-10-31 14:05:38,333:INFO:[LightGBM] [Info] Number of data points in the train set: 3228, number of used features: 59
2023-10-31 14:05:38,333:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495043 -> initscore=-0.019827
2023-10-31 14:05:38,333:INFO:[LightGBM] [Info] Start training from score -0.019827
2023-10-31 14:05:38,398:INFO:[LightGBM] [Info] Number of positive: 1821, number of negative: 1878
2023-10-31 14:05:38,399:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001041 seconds.
2023-10-31 14:05:38,400:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 14:05:38,400:INFO:[LightGBM] [Info] Total Bins 15045
2023-10-31 14:05:38,400:INFO:[LightGBM] [Info] Number of data points in the train set: 3699, number of used features: 59
2023-10-31 14:05:38,400:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.492295 -> initscore=-0.030822
2023-10-31 14:05:38,400:INFO:[LightGBM] [Info] Start training from score -0.030822
2023-10-31 14:05:40,943:INFO:[LightGBM] [Info] Number of positive: 2066, number of negative: 2103
2023-10-31 14:05:40,943:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001160 seconds.
2023-10-31 14:05:40,943:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 14:05:40,947:INFO:[LightGBM] [Info] Total Bins 15045
2023-10-31 14:05:40,947:INFO:[LightGBM] [Info] Number of data points in the train set: 4169, number of used features: 59
2023-10-31 14:05:40,947:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495562 -> initscore=-0.017751
2023-10-31 14:05:40,947:INFO:[LightGBM] [Info] Start training from score -0.017751
2023-10-31 14:05:45,016:INFO:[LightGBM] [Info] Number of positive: 2291, number of negative: 2349
2023-10-31 14:05:45,018:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001371 seconds.
2023-10-31 14:05:45,018:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 14:05:45,018:INFO:[LightGBM] [Info] Total Bins 15045
2023-10-31 14:05:45,018:INFO:[LightGBM] [Info] Number of data points in the train set: 4640, number of used features: 59
2023-10-31 14:05:45,018:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.493750 -> initscore=-0.025001
2023-10-31 14:05:45,019:INFO:[LightGBM] [Info] Start training from score -0.025001
2023-10-31 14:05:45,530:INFO:[LightGBM] [Info] Number of positive: 2522, number of negative: 2589
2023-10-31 14:05:45,530:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001069 seconds.
2023-10-31 14:05:45,531:INFO:You can set `force_row_wise=true` to remove the overhead.
2023-10-31 14:05:45,531:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2023-10-31 14:05:45,531:INFO:[LightGBM] [Info] Total Bins 15045
2023-10-31 14:05:45,531:INFO:[LightGBM] [Info] Number of data points in the train set: 5111, number of used features: 59
2023-10-31 14:05:45,532:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.493446 -> initscore=-0.026219
2023-10-31 14:05:45,532:INFO:[LightGBM] [Info] Start training from score -0.026219
2023-10-31 14:05:47,723:INFO:[LightGBM] [Info] Number of positive: 2748, number of negative: 2834
2023-10-31 14:05:47,725:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001510 seconds.
2023-10-31 14:05:47,725:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 14:05:47,725:INFO:[LightGBM] [Info] Total Bins 15045
2023-10-31 14:05:47,726:INFO:[LightGBM] [Info] Number of data points in the train set: 5582, number of used features: 59
2023-10-31 14:05:47,726:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.492297 -> initscore=-0.030816
2023-10-31 14:05:47,726:INFO:[LightGBM] [Info] Start training from score -0.030816
2023-10-31 14:05:48,192:INFO:[LightGBM] [Info] Number of positive: 2981, number of negative: 3072
2023-10-31 14:05:48,193:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001644 seconds.
2023-10-31 14:05:48,193:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 14:05:48,193:INFO:[LightGBM] [Info] Total Bins 15045
2023-10-31 14:05:48,194:INFO:[LightGBM] [Info] Number of data points in the train set: 6053, number of used features: 59
2023-10-31 14:05:48,195:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.492483 -> initscore=-0.030070
2023-10-31 14:05:48,195:INFO:[LightGBM] [Info] Start training from score -0.030070
2023-10-31 14:05:48,552:INFO:[LightGBM] [Info] Number of positive: 895, number of negative: 920
2023-10-31 14:05:48,553:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001099 seconds.
2023-10-31 14:05:48,553:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 14:05:48,554:INFO:[LightGBM] [Info] Total Bins 15045
2023-10-31 14:05:48,554:INFO:[LightGBM] [Info] Number of data points in the train set: 1815, number of used features: 59
2023-10-31 14:05:48,554:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.493113 -> initscore=-0.027550
2023-10-31 14:05:48,554:INFO:[LightGBM] [Info] Start training from score -0.027550
2023-10-31 14:05:49,064:INFO:[LightGBM] [Info] Number of positive: 1143, number of negative: 1143
2023-10-31 14:05:49,065:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000629 seconds.
2023-10-31 14:05:49,065:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 14:05:49,065:INFO:[LightGBM] [Info] Total Bins 15045
2023-10-31 14:05:49,065:INFO:[LightGBM] [Info] Number of data points in the train set: 2286, number of used features: 59
2023-10-31 14:05:49,065:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000
2023-10-31 14:05:51,618:INFO:[LightGBM] [Info] Number of positive: 1376, number of negative: 1381
2023-10-31 14:05:51,618:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000786 seconds.
2023-10-31 14:05:51,618:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 14:05:51,618:INFO:[LightGBM] [Info] Total Bins 15045
2023-10-31 14:05:51,618:INFO:[LightGBM] [Info] Number of data points in the train set: 2757, number of used features: 59
2023-10-31 14:05:51,618:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499093 -> initscore=-0.003627
2023-10-31 14:05:51,619:INFO:[LightGBM] [Info] Start training from score -0.003627
2023-10-31 14:05:53,870:INFO:[LightGBM] [Info] Number of positive: 1598, number of negative: 1630
2023-10-31 14:05:53,870:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000869 seconds.
2023-10-31 14:05:53,870:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 14:05:53,870:INFO:[LightGBM] [Info] Total Bins 15045
2023-10-31 14:05:53,870:INFO:[LightGBM] [Info] Number of data points in the train set: 3228, number of used features: 59
2023-10-31 14:05:53,870:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495043 -> initscore=-0.019827
2023-10-31 14:05:53,870:INFO:[LightGBM] [Info] Start training from score -0.019827
2023-10-31 14:05:54,986:INFO:[LightGBM] [Info] Number of positive: 1825, number of negative: 1874
2023-10-31 14:05:54,988:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001080 seconds.
2023-10-31 14:05:54,988:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 14:05:54,988:INFO:[LightGBM] [Info] Total Bins 15045
2023-10-31 14:05:54,988:INFO:[LightGBM] [Info] Number of data points in the train set: 3699, number of used features: 59
2023-10-31 14:05:54,988:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.493377 -> initscore=-0.026495
2023-10-31 14:05:54,988:INFO:[LightGBM] [Info] Start training from score -0.026495
2023-10-31 14:05:57,993:INFO:[LightGBM] [Info] Number of positive: 2065, number of negative: 2104
2023-10-31 14:05:57,993:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003525 seconds.
2023-10-31 14:05:57,993:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 14:05:57,993:INFO:[LightGBM] [Info] Total Bins 15045
2023-10-31 14:05:57,998:INFO:[LightGBM] [Info] Number of data points in the train set: 4169, number of used features: 59
2023-10-31 14:05:57,998:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495323 -> initscore=-0.018710
2023-10-31 14:05:57,998:INFO:[LightGBM] [Info] Start training from score -0.018710
2023-10-31 14:06:02,130:INFO:[LightGBM] [Info] Number of positive: 2521, number of negative: 2590
2023-10-31 14:06:02,131:INFO:[LightGBM] [Info] Number of positive: 2291, number of negative: 2349
2023-10-31 14:06:02,132:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001401 seconds.
2023-10-31 14:06:02,132:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 14:06:02,132:INFO:[LightGBM] [Info] Total Bins 15045
2023-10-31 14:06:02,132:INFO:[LightGBM] [Info] Number of data points in the train set: 5111, number of used features: 59
2023-10-31 14:06:02,133:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.493250 -> initscore=-0.027002
2023-10-31 14:06:02,133:INFO:[LightGBM] [Info] Start training from score -0.027002
2023-10-31 14:06:02,133:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001273 seconds.
2023-10-31 14:06:02,133:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 14:06:02,133:INFO:[LightGBM] [Info] Total Bins 15045
2023-10-31 14:06:02,133:INFO:[LightGBM] [Info] Number of data points in the train set: 4640, number of used features: 59
2023-10-31 14:06:02,134:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.493750 -> initscore=-0.025001
2023-10-31 14:06:02,135:INFO:[LightGBM] [Info] Start training from score -0.025001
2023-10-31 14:06:03,629:INFO:[LightGBM] [Info] Number of positive: 2747, number of negative: 2835
2023-10-31 14:06:03,632:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001526 seconds.
2023-10-31 14:06:03,632:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 14:06:03,633:INFO:[LightGBM] [Info] Total Bins 15045
2023-10-31 14:06:03,633:INFO:[LightGBM] [Info] Number of data points in the train set: 5582, number of used features: 59
2023-10-31 14:06:03,633:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.492118 -> initscore=-0.031533
2023-10-31 14:06:03,633:INFO:[LightGBM] [Info] Start training from score -0.031533
2023-10-31 14:06:04,882:INFO:[LightGBM] [Info] Number of positive: 2981, number of negative: 3072
2023-10-31 14:06:04,883:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001666 seconds.
2023-10-31 14:06:04,883:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 14:06:04,883:INFO:[LightGBM] [Info] Total Bins 15045
2023-10-31 14:06:04,883:INFO:[LightGBM] [Info] Number of data points in the train set: 6053, number of used features: 59
2023-10-31 14:06:04,883:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.492483 -> initscore=-0.030070
2023-10-31 14:06:04,883:INFO:[LightGBM] [Info] Start training from score -0.030070
2023-10-31 14:06:06,250:INFO:[LightGBM] [Info] Number of positive: 895, number of negative: 920
2023-10-31 14:06:06,251:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000508 seconds.
2023-10-31 14:06:06,251:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 14:06:06,251:INFO:[LightGBM] [Info] Total Bins 15045
2023-10-31 14:06:06,252:INFO:[LightGBM] [Info] Number of data points in the train set: 1815, number of used features: 59
2023-10-31 14:06:06,256:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.493113 -> initscore=-0.027550
2023-10-31 14:06:06,256:INFO:[LightGBM] [Info] Start training from score -0.027550
2023-10-31 14:06:06,285:INFO:[LightGBM] [Info] Number of positive: 1376, number of negative: 1381
2023-10-31 14:06:06,286:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000760 seconds.
2023-10-31 14:06:06,286:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 14:06:06,286:INFO:[LightGBM] [Info] Total Bins 15045
2023-10-31 14:06:06,286:INFO:[LightGBM] [Info] Number of data points in the train set: 2757, number of used features: 59
2023-10-31 14:06:06,286:INFO:[LightGBM] [Info] Number of positive: 1143, number of negative: 1143
2023-10-31 14:06:06,286:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499093 -> initscore=-0.003627
2023-10-31 14:06:06,286:INFO:[LightGBM] [Info] Start training from score -0.003627
2023-10-31 14:06:06,286:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000650 seconds.
2023-10-31 14:06:06,286:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 14:06:06,286:INFO:[LightGBM] [Info] Total Bins 15045
2023-10-31 14:06:06,298:INFO:[LightGBM] [Info] Number of data points in the train set: 2286, number of used features: 59
2023-10-31 14:06:06,301:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000
2023-10-31 14:06:09,207:INFO:[LightGBM] [Info] Number of positive: 1598, number of negative: 1630
2023-10-31 14:06:09,207:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000914 seconds.
2023-10-31 14:06:09,207:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 14:06:09,208:INFO:[LightGBM] [Info] Total Bins 15045
2023-10-31 14:06:09,208:INFO:[LightGBM] [Info] Number of data points in the train set: 3228, number of used features: 59
2023-10-31 14:06:09,208:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495043 -> initscore=-0.019827
2023-10-31 14:06:09,208:INFO:[LightGBM] [Info] Start training from score -0.019827
2023-10-31 14:06:10,408:INFO:[LightGBM] [Info] Number of positive: 1825, number of negative: 1874
2023-10-31 14:06:10,410:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001429 seconds.
2023-10-31 14:06:10,410:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 14:06:10,410:INFO:[LightGBM] [Info] Total Bins 15045
2023-10-31 14:06:10,411:INFO:[LightGBM] [Info] Number of data points in the train set: 3699, number of used features: 59
2023-10-31 14:06:10,411:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.493377 -> initscore=-0.026495
2023-10-31 14:06:10,411:INFO:[LightGBM] [Info] Start training from score -0.026495
2023-10-31 14:06:14,728:INFO:[LightGBM] [Info] Number of positive: 2059, number of negative: 2110
2023-10-31 14:06:14,728:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001163 seconds.
2023-10-31 14:06:14,728:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 14:06:14,728:INFO:[LightGBM] [Info] Total Bins 15045
2023-10-31 14:06:14,728:INFO:[LightGBM] [Info] Number of data points in the train set: 4169, number of used features: 59
2023-10-31 14:06:14,728:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.493883 -> initscore=-0.024468
2023-10-31 14:06:14,728:INFO:[LightGBM] [Info] Start training from score -0.024468
2023-10-31 14:06:19,744:INFO:[LightGBM] [Info] Number of positive: 2283, number of negative: 2357
2023-10-31 14:06:19,745:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001268 seconds.
2023-10-31 14:06:19,745:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 14:06:19,745:INFO:[LightGBM] [Info] Total Bins 15045
2023-10-31 14:06:19,746:INFO:[LightGBM] [Info] Number of data points in the train set: 4640, number of used features: 59
2023-10-31 14:06:19,746:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.492026 -> initscore=-0.031899
2023-10-31 14:06:19,747:INFO:[LightGBM] [Info] Start training from score -0.031899
2023-10-31 14:06:19,990:INFO:[LightGBM] [Info] Number of positive: 2521, number of negative: 2590
2023-10-31 14:06:19,993:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001539 seconds.
2023-10-31 14:06:19,994:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 14:06:19,994:INFO:[LightGBM] [Info] Total Bins 15045
2023-10-31 14:06:19,994:INFO:[LightGBM] [Info] Number of data points in the train set: 5111, number of used features: 59
2023-10-31 14:06:19,995:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.493250 -> initscore=-0.027002
2023-10-31 14:06:19,995:INFO:[LightGBM] [Info] Start training from score -0.027002
2023-10-31 14:06:21,595:INFO:[LightGBM] [Info] Number of positive: 2747, number of negative: 2835
2023-10-31 14:06:21,595:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001491 seconds.
2023-10-31 14:06:21,595:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 14:06:21,595:INFO:[LightGBM] [Info] Total Bins 15045
2023-10-31 14:06:21,597:INFO:[LightGBM] [Info] Number of data points in the train set: 5582, number of used features: 59
2023-10-31 14:06:21,598:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.492118 -> initscore=-0.031533
2023-10-31 14:06:21,598:INFO:[LightGBM] [Info] Start training from score -0.031533
2023-10-31 14:06:22,183:INFO:[LightGBM] [Info] Number of positive: 2981, number of negative: 3072
2023-10-31 14:06:22,184:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001646 seconds.
2023-10-31 14:06:22,184:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 14:06:22,184:INFO:[LightGBM] [Info] Total Bins 15045
2023-10-31 14:06:22,185:INFO:[LightGBM] [Info] Number of data points in the train set: 6053, number of used features: 59
2023-10-31 14:06:22,185:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.492483 -> initscore=-0.030070
2023-10-31 14:06:22,185:INFO:[LightGBM] [Info] Start training from score -0.030070
2023-10-31 14:06:22,996:INFO:[LightGBM] [Info] Number of positive: 895, number of negative: 920
2023-10-31 14:06:22,996:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000498 seconds.
2023-10-31 14:06:22,997:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 14:06:22,997:INFO:[LightGBM] [Info] Total Bins 15045
2023-10-31 14:06:22,997:INFO:[LightGBM] [Info] Number of data points in the train set: 1815, number of used features: 59
2023-10-31 14:06:23,003:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.493113 -> initscore=-0.027550
2023-10-31 14:06:23,003:INFO:[LightGBM] [Info] Start training from score -0.027550
2023-10-31 14:06:23,094:INFO:[LightGBM] [Info] Number of positive: 1143, number of negative: 1143
2023-10-31 14:06:23,095:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000644 seconds.
2023-10-31 14:06:23,095:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 14:06:23,095:INFO:[LightGBM] [Info] Total Bins 15045
2023-10-31 14:06:23,096:INFO:[LightGBM] [Info] Number of data points in the train set: 2286, number of used features: 59
2023-10-31 14:06:23,100:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000
2023-10-31 14:06:24,988:INFO:[LightGBM] [Info] Number of positive: 1376, number of negative: 1381
2023-10-31 14:06:24,989:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000756 seconds.
2023-10-31 14:06:24,989:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 14:06:24,989:INFO:[LightGBM] [Info] Total Bins 15045
2023-10-31 14:06:24,990:INFO:[LightGBM] [Info] Number of data points in the train set: 2757, number of used features: 59
2023-10-31 14:06:24,990:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499093 -> initscore=-0.003627
2023-10-31 14:06:24,990:INFO:[LightGBM] [Info] Start training from score -0.003627
2023-10-31 14:06:25,812:INFO:[LightGBM] [Info] Number of positive: 1598, number of negative: 1630
2023-10-31 14:06:25,812:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000865 seconds.
2023-10-31 14:06:25,812:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 14:06:25,812:INFO:[LightGBM] [Info] Total Bins 15045
2023-10-31 14:06:25,812:INFO:[LightGBM] [Info] Number of data points in the train set: 3228, number of used features: 59
2023-10-31 14:06:25,812:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495043 -> initscore=-0.019827
2023-10-31 14:06:25,813:INFO:[LightGBM] [Info] Start training from score -0.019827
2023-10-31 14:06:28,152:INFO:[LightGBM] [Info] Number of positive: 1825, number of negative: 1874
2023-10-31 14:06:28,154:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001821 seconds.
2023-10-31 14:06:28,154:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 14:06:28,155:INFO:[LightGBM] [Info] Total Bins 15045
2023-10-31 14:06:28,155:INFO:[LightGBM] [Info] Number of data points in the train set: 3699, number of used features: 59
2023-10-31 14:06:28,156:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.493377 -> initscore=-0.026495
2023-10-31 14:06:28,156:INFO:[LightGBM] [Info] Start training from score -0.026495
2023-10-31 14:06:31,983:INFO:[LightGBM] [Info] Number of positive: 2059, number of negative: 2110
2023-10-31 14:06:31,984:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001134 seconds.
2023-10-31 14:06:31,984:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 14:06:31,984:INFO:[LightGBM] [Info] Total Bins 15045
2023-10-31 14:06:31,985:INFO:[LightGBM] [Info] Number of data points in the train set: 4169, number of used features: 59
2023-10-31 14:06:31,986:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.493883 -> initscore=-0.024468
2023-10-31 14:06:31,986:INFO:[LightGBM] [Info] Start training from score -0.024468
2023-10-31 14:06:36,464:INFO:[LightGBM] [Info] Number of positive: 2283, number of negative: 2357
2023-10-31 14:06:36,465:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001274 seconds.
2023-10-31 14:06:36,465:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 14:06:36,465:INFO:[LightGBM] [Info] Total Bins 15045
2023-10-31 14:06:36,466:INFO:[LightGBM] [Info] Number of data points in the train set: 4640, number of used features: 59
2023-10-31 14:06:36,466:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.492026 -> initscore=-0.031899
2023-10-31 14:06:36,466:INFO:[LightGBM] [Info] Start training from score -0.031899
2023-10-31 14:06:38,408:INFO:[LightGBM] [Info] Number of positive: 2525, number of negative: 2586
2023-10-31 14:06:38,409:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001393 seconds.
2023-10-31 14:06:38,409:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 14:06:38,409:INFO:[LightGBM] [Info] Total Bins 15045
2023-10-31 14:06:38,411:INFO:[LightGBM] [Info] Number of data points in the train set: 5111, number of used features: 59
2023-10-31 14:06:38,411:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.494032 -> initscore=-0.023871
2023-10-31 14:06:38,411:INFO:[LightGBM] [Info] Start training from score -0.023871
2023-10-31 14:06:38,542:INFO:[LightGBM] [Info] Number of positive: 2747, number of negative: 2835
2023-10-31 14:06:38,543:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001529 seconds.
2023-10-31 14:06:38,543:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 14:06:38,543:INFO:[LightGBM] [Info] Total Bins 15045
2023-10-31 14:06:38,544:INFO:[LightGBM] [Info] Number of data points in the train set: 5582, number of used features: 59
2023-10-31 14:06:38,544:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.492118 -> initscore=-0.031533
2023-10-31 14:06:38,544:INFO:[LightGBM] [Info] Start training from score -0.031533
2023-10-31 14:06:39,253:INFO:[LightGBM] [Info] Number of positive: 2981, number of negative: 3072
2023-10-31 14:06:39,255:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001625 seconds.
2023-10-31 14:06:39,255:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 14:06:39,255:INFO:[LightGBM] [Info] Total Bins 15045
2023-10-31 14:06:39,255:INFO:[LightGBM] [Info] Number of data points in the train set: 6053, number of used features: 59
2023-10-31 14:06:39,255:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.492483 -> initscore=-0.030070
2023-10-31 14:06:39,256:INFO:[LightGBM] [Info] Start training from score -0.030070
2023-10-31 14:06:40,377:INFO:[LightGBM] [Info] Number of positive: 895, number of negative: 920
2023-10-31 14:06:40,377:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000524 seconds.
2023-10-31 14:06:40,377:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 14:06:40,377:INFO:[LightGBM] [Info] Total Bins 15045
2023-10-31 14:06:40,377:INFO:[LightGBM] [Info] Number of data points in the train set: 1815, number of used features: 59
2023-10-31 14:06:40,377:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.493113 -> initscore=-0.027550
2023-10-31 14:06:40,377:INFO:[LightGBM] [Info] Start training from score -0.027550
2023-10-31 14:06:41,051:INFO:[LightGBM] [Info] Number of positive: 1143, number of negative: 1143
2023-10-31 14:06:41,052:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000647 seconds.
2023-10-31 14:06:41,052:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 14:06:41,052:INFO:[LightGBM] [Info] Total Bins 15045
2023-10-31 14:06:41,052:INFO:[LightGBM] [Info] Number of data points in the train set: 2286, number of used features: 59
2023-10-31 14:06:41,053:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000
2023-10-31 14:06:41,591:INFO:[LightGBM] [Info] Number of positive: 1376, number of negative: 1381
2023-10-31 14:06:41,592:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000746 seconds.
2023-10-31 14:06:41,592:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 14:06:41,592:INFO:[LightGBM] [Info] Total Bins 15045
2023-10-31 14:06:41,593:INFO:[LightGBM] [Info] Number of data points in the train set: 2757, number of used features: 59
2023-10-31 14:06:41,594:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499093 -> initscore=-0.003627
2023-10-31 14:06:41,594:INFO:[LightGBM] [Info] Start training from score -0.003627
2023-10-31 14:06:42,632:INFO:[LightGBM] [Info] Number of positive: 1598, number of negative: 1630
2023-10-31 14:06:42,635:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002866 seconds.
2023-10-31 14:06:42,636:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 14:06:42,636:INFO:[LightGBM] [Info] Total Bins 15045
2023-10-31 14:06:42,636:INFO:[LightGBM] [Info] Number of data points in the train set: 3228, number of used features: 59
2023-10-31 14:06:42,636:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495043 -> initscore=-0.019827
2023-10-31 14:06:42,636:INFO:[LightGBM] [Info] Start training from score -0.019827
2023-10-31 14:06:44,310:INFO:[LightGBM] [Info] Number of positive: 1825, number of negative: 1874
2023-10-31 14:06:44,311:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001047 seconds.
2023-10-31 14:06:44,311:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 14:06:44,311:INFO:[LightGBM] [Info] Total Bins 15045
2023-10-31 14:06:44,312:INFO:[LightGBM] [Info] Number of data points in the train set: 3699, number of used features: 59
2023-10-31 14:06:44,312:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.493377 -> initscore=-0.026495
2023-10-31 14:06:44,312:INFO:[LightGBM] [Info] Start training from score -0.026495
2023-10-31 14:06:49,699:INFO:[LightGBM] [Info] Number of positive: 2059, number of negative: 2110
2023-10-31 14:06:49,700:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001236 seconds.
2023-10-31 14:06:49,700:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 14:06:49,700:INFO:[LightGBM] [Info] Total Bins 15045
2023-10-31 14:06:49,701:INFO:[LightGBM] [Info] Number of data points in the train set: 4169, number of used features: 59
2023-10-31 14:06:49,701:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.493883 -> initscore=-0.024468
2023-10-31 14:06:49,701:INFO:[LightGBM] [Info] Start training from score -0.024468
2023-10-31 14:06:53,671:INFO:[LightGBM] [Info] Number of positive: 2283, number of negative: 2357
2023-10-31 14:06:53,672:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001261 seconds.
2023-10-31 14:06:53,673:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 14:06:53,673:INFO:[LightGBM] [Info] Total Bins 15045
2023-10-31 14:06:53,673:INFO:[LightGBM] [Info] Number of data points in the train set: 4640, number of used features: 59
2023-10-31 14:06:53,675:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.492026 -> initscore=-0.031899
2023-10-31 14:06:53,676:INFO:[LightGBM] [Info] Start training from score -0.031899
2023-10-31 14:06:54,894:INFO:[LightGBM] [Info] Number of positive: 2525, number of negative: 2586
2023-10-31 14:06:54,897:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002338 seconds.
2023-10-31 14:06:54,898:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 14:06:54,898:INFO:[LightGBM] [Info] Total Bins 15045
2023-10-31 14:06:54,898:INFO:[LightGBM] [Info] Number of data points in the train set: 5111, number of used features: 59
2023-10-31 14:06:54,899:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.494032 -> initscore=-0.023871
2023-10-31 14:06:54,899:INFO:[LightGBM] [Info] Start training from score -0.023871
2023-10-31 14:06:55,471:INFO:[LightGBM] [Info] Number of positive: 2761, number of negative: 2821
2023-10-31 14:06:55,471:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001529 seconds.
2023-10-31 14:06:55,472:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 14:06:55,472:INFO:[LightGBM] [Info] Total Bins 15045
2023-10-31 14:06:55,472:INFO:[LightGBM] [Info] Number of data points in the train set: 5582, number of used features: 59
2023-10-31 14:06:55,472:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.494626 -> initscore=-0.021498
2023-10-31 14:06:55,472:INFO:[LightGBM] [Info] Start training from score -0.021498
2023-10-31 14:06:55,963:INFO:[LightGBM] [Info] Number of positive: 2981, number of negative: 3072
2023-10-31 14:06:55,976:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009282 seconds.
2023-10-31 14:06:55,977:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-10-31 14:06:55,977:INFO:[LightGBM] [Info] Total Bins 15045
2023-10-31 14:06:55,977:INFO:[LightGBM] [Info] Number of data points in the train set: 6053, number of used features: 59
2023-10-31 14:06:55,977:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.492483 -> initscore=-0.030070
2023-10-31 14:06:55,977:INFO:[LightGBM] [Info] Start training from score -0.030070
2023-10-31 14:07:04,792:INFO:Visual Rendered Successfully
2023-10-31 14:07:04,959:INFO:plot_model() successfully completed......................................
2023-10-31 14:07:04,984:INFO:Initializing evaluate_model()
2023-10-31 14:07:04,984:INFO:evaluate_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x2a279fcd0>, estimator=VotingClassifier(estimators=[('Extra Trees Classifier',
                              ExtraTreesClassifier(bootstrap=False,
                                                   ccp_alpha=0.0,
                                                   class_weight=None,
                                                   criterion='gini',
                                                   max_depth=None,
                                                   max_features='sqrt',
                                                   max_leaf_nodes=None,
                                                   max_samples=None,
                                                   min_impurity_decrease=0.0,
                                                   min_samples_leaf=1,
                                                   min_samples_split=2,
                                                   min_weight_fraction_leaf=0.0,
                                                   n_estimators=100, n_jobs=-1,
                                                   oob_score=False,
                                                   ra...
                                             importance_type='split',
                                             learning_rate=0.1, max_depth=-1,
                                             min_child_samples=20,
                                             min_child_weight=0.001,
                                             min_split_gain=0.0,
                                             n_estimators=100, n_jobs=-1,
                                             num_leaves=31, objective=None,
                                             random_state=1784, reg_alpha=0.0,
                                             reg_lambda=0.0, subsample=1.0,
                                             subsample_for_bin=200000,
                                             subsample_freq=0))],
                 flatten_transform=True, n_jobs=-1, verbose=False,
                 voting='soft', weights=None), fold=None, fit_kwargs=None, plot_kwargs=None, feature_name=None, groups=None)
2023-10-31 14:07:04,996:INFO:Initializing plot_model()
2023-10-31 14:07:04,996:INFO:plot_model(plot=pipeline, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), verbose=False, display=None, display_format=None, estimator=VotingClassifier(estimators=[('Extra Trees Classifier',
                              ExtraTreesClassifier(bootstrap=False,
                                                   ccp_alpha=0.0,
                                                   class_weight=None,
                                                   criterion='gini',
                                                   max_depth=None,
                                                   max_features='sqrt',
                                                   max_leaf_nodes=None,
                                                   max_samples=None,
                                                   min_impurity_decrease=0.0,
                                                   min_samples_leaf=1,
                                                   min_samples_split=2,
                                                   min_weight_fraction_leaf=0.0,
                                                   n_estimators=100, n_jobs=-1,
                                                   oob_score=False,
                                                   ra...
                                             importance_type='split',
                                             learning_rate=0.1, max_depth=-1,
                                             min_child_samples=20,
                                             min_child_weight=0.001,
                                             min_split_gain=0.0,
                                             n_estimators=100, n_jobs=-1,
                                             num_leaves=31, objective=None,
                                             random_state=1784, reg_alpha=0.0,
                                             reg_lambda=0.0, subsample=1.0,
                                             subsample_for_bin=200000,
                                             subsample_freq=0))],
                 flatten_transform=True, n_jobs=-1, verbose=False,
                 voting='soft', weights=None), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.classification.oop.ClassificationExperiment object at 0x2a279fcd0>, system=True)
2023-10-31 14:07:04,996:INFO:Checking exceptions
2023-10-31 14:07:05,001:INFO:Preloading libraries
2023-10-31 14:07:05,057:INFO:Copying training dataset
2023-10-31 14:07:05,057:INFO:Plot type: pipeline
2023-10-31 14:07:05,165:INFO:Visual Rendered Successfully
2023-10-31 14:07:05,257:INFO:plot_model() successfully completed......................................
2023-10-31 14:07:05,268:INFO:Initializing plot_model()
2023-10-31 14:07:05,268:INFO:plot_model(plot=pipeline, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), verbose=False, display=None, display_format=None, estimator=VotingClassifier(estimators=[('Extra Trees Classifier',
                              ExtraTreesClassifier(bootstrap=False,
                                                   ccp_alpha=0.0,
                                                   class_weight=None,
                                                   criterion='gini',
                                                   max_depth=None,
                                                   max_features='sqrt',
                                                   max_leaf_nodes=None,
                                                   max_samples=None,
                                                   min_impurity_decrease=0.0,
                                                   min_samples_leaf=1,
                                                   min_samples_split=2,
                                                   min_weight_fraction_leaf=0.0,
                                                   n_estimators=100, n_jobs=-1,
                                                   oob_score=False,
                                                   ra...
                                             importance_type='split',
                                             learning_rate=0.1, max_depth=-1,
                                             min_child_samples=20,
                                             min_child_weight=0.001,
                                             min_split_gain=0.0,
                                             n_estimators=100, n_jobs=-1,
                                             num_leaves=31, objective=None,
                                             random_state=1784, reg_alpha=0.0,
                                             reg_lambda=0.0, subsample=1.0,
                                             subsample_for_bin=200000,
                                             subsample_freq=0))],
                 flatten_transform=True, n_jobs=-1, verbose=False,
                 voting='hard', weights=None), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.classification.oop.ClassificationExperiment object at 0x2a279fcd0>, system=True)
2023-10-31 14:07:05,268:INFO:Checking exceptions
2023-10-31 14:07:05,272:INFO:Preloading libraries
2023-10-31 14:07:05,310:INFO:Copying training dataset
2023-10-31 14:07:05,310:INFO:Plot type: pipeline
2023-10-31 14:07:05,420:INFO:Visual Rendered Successfully
2023-10-31 14:07:05,512:INFO:plot_model() successfully completed......................................
2023-10-31 14:08:40,913:INFO:Initializing plot_model()
2023-10-31 14:08:40,914:INFO:plot_model(plot=parameter, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), verbose=False, display=None, display_format=None, estimator=VotingClassifier(estimators=[('Extra Trees Classifier',
                              ExtraTreesClassifier(bootstrap=False,
                                                   ccp_alpha=0.0,
                                                   class_weight=None,
                                                   criterion='gini',
                                                   max_depth=None,
                                                   max_features='sqrt',
                                                   max_leaf_nodes=None,
                                                   max_samples=None,
                                                   min_impurity_decrease=0.0,
                                                   min_samples_leaf=1,
                                                   min_samples_split=2,
                                                   min_weight_fraction_leaf=0.0,
                                                   n_estimators=100, n_jobs=-1,
                                                   oob_score=False,
                                                   ra...
                                             importance_type='split',
                                             learning_rate=0.1, max_depth=-1,
                                             min_child_samples=20,
                                             min_child_weight=0.001,
                                             min_split_gain=0.0,
                                             n_estimators=100, n_jobs=-1,
                                             num_leaves=31, objective=None,
                                             random_state=1784, reg_alpha=0.0,
                                             reg_lambda=0.0, subsample=1.0,
                                             subsample_for_bin=200000,
                                             subsample_freq=0))],
                 flatten_transform=True, n_jobs=-1, verbose=False,
                 voting='hard', weights=None), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.classification.oop.ClassificationExperiment object at 0x2a279fcd0>, system=True)
2023-10-31 14:08:40,914:INFO:Checking exceptions
2023-10-31 14:08:40,922:INFO:Preloading libraries
2023-10-31 14:08:40,958:INFO:Copying training dataset
2023-10-31 14:08:40,958:INFO:Plot type: parameter
2023-10-31 14:08:40,961:INFO:Visual Rendered Successfully
2023-10-31 14:08:41,054:INFO:plot_model() successfully completed......................................
2023-10-31 14:08:45,779:INFO:Initializing plot_model()
2023-10-31 14:08:45,779:INFO:plot_model(plot=confusion_matrix, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), verbose=False, display=None, display_format=None, estimator=VotingClassifier(estimators=[('Extra Trees Classifier',
                              ExtraTreesClassifier(bootstrap=False,
                                                   ccp_alpha=0.0,
                                                   class_weight=None,
                                                   criterion='gini',
                                                   max_depth=None,
                                                   max_features='sqrt',
                                                   max_leaf_nodes=None,
                                                   max_samples=None,
                                                   min_impurity_decrease=0.0,
                                                   min_samples_leaf=1,
                                                   min_samples_split=2,
                                                   min_weight_fraction_leaf=0.0,
                                                   n_estimators=100, n_jobs=-1,
                                                   oob_score=False,
                                                   ra...
                                             importance_type='split',
                                             learning_rate=0.1, max_depth=-1,
                                             min_child_samples=20,
                                             min_child_weight=0.001,
                                             min_split_gain=0.0,
                                             n_estimators=100, n_jobs=-1,
                                             num_leaves=31, objective=None,
                                             random_state=1784, reg_alpha=0.0,
                                             reg_lambda=0.0, subsample=1.0,
                                             subsample_for_bin=200000,
                                             subsample_freq=0))],
                 flatten_transform=True, n_jobs=-1, verbose=False,
                 voting='hard', weights=None), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.classification.oop.ClassificationExperiment object at 0x2a279fcd0>, system=True)
2023-10-31 14:08:45,779:INFO:Checking exceptions
2023-10-31 14:08:45,787:INFO:Preloading libraries
2023-10-31 14:08:45,827:INFO:Copying training dataset
2023-10-31 14:08:45,827:INFO:Plot type: confusion_matrix
2023-10-31 14:08:46,583:INFO:Fitting Model
2023-10-31 14:08:46,583:WARNING:X does not have valid feature names, but ExtraTreesClassifier was fitted with feature names

2023-10-31 14:08:46,583:INFO:Scoring test/hold-out set
2023-10-31 14:08:46,907:INFO:Visual Rendered Successfully
2023-10-31 14:08:47,029:INFO:plot_model() successfully completed......................................
2023-10-31 14:08:50,529:INFO:Initializing plot_model()
2023-10-31 14:08:50,529:INFO:plot_model(plot=confusion_matrix, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), verbose=False, display=None, display_format=None, estimator=VotingClassifier(estimators=[('Extra Trees Classifier',
                              ExtraTreesClassifier(bootstrap=False,
                                                   ccp_alpha=0.0,
                                                   class_weight=None,
                                                   criterion='gini',
                                                   max_depth=None,
                                                   max_features='sqrt',
                                                   max_leaf_nodes=None,
                                                   max_samples=None,
                                                   min_impurity_decrease=0.0,
                                                   min_samples_leaf=1,
                                                   min_samples_split=2,
                                                   min_weight_fraction_leaf=0.0,
                                                   n_estimators=100, n_jobs=-1,
                                                   oob_score=False,
                                                   ra...
                                             importance_type='split',
                                             learning_rate=0.1, max_depth=-1,
                                             min_child_samples=20,
                                             min_child_weight=0.001,
                                             min_split_gain=0.0,
                                             n_estimators=100, n_jobs=-1,
                                             num_leaves=31, objective=None,
                                             random_state=1784, reg_alpha=0.0,
                                             reg_lambda=0.0, subsample=1.0,
                                             subsample_for_bin=200000,
                                             subsample_freq=0))],
                 flatten_transform=True, n_jobs=-1, verbose=False,
                 voting='soft', weights=None), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.classification.oop.ClassificationExperiment object at 0x2a279fcd0>, system=True)
2023-10-31 14:08:50,529:INFO:Checking exceptions
2023-10-31 14:08:50,537:INFO:Preloading libraries
2023-10-31 14:08:50,574:INFO:Copying training dataset
2023-10-31 14:08:50,574:INFO:Plot type: confusion_matrix
2023-10-31 14:08:51,290:INFO:Fitting Model
2023-10-31 14:08:51,290:WARNING:X does not have valid feature names, but ExtraTreesClassifier was fitted with feature names

2023-10-31 14:08:51,290:INFO:Scoring test/hold-out set
2023-10-31 14:08:51,599:INFO:Visual Rendered Successfully
2023-10-31 14:08:51,704:INFO:plot_model() successfully completed......................................
2023-10-31 14:09:58,717:INFO:Initializing plot_model()
2023-10-31 14:09:58,718:INFO:plot_model(plot=pr, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), verbose=False, display=None, display_format=None, estimator=VotingClassifier(estimators=[('Extra Trees Classifier',
                              ExtraTreesClassifier(bootstrap=False,
                                                   ccp_alpha=0.0,
                                                   class_weight=None,
                                                   criterion='gini',
                                                   max_depth=None,
                                                   max_features='sqrt',
                                                   max_leaf_nodes=None,
                                                   max_samples=None,
                                                   min_impurity_decrease=0.0,
                                                   min_samples_leaf=1,
                                                   min_samples_split=2,
                                                   min_weight_fraction_leaf=0.0,
                                                   n_estimators=100, n_jobs=-1,
                                                   oob_score=False,
                                                   ra...
                                             importance_type='split',
                                             learning_rate=0.1, max_depth=-1,
                                             min_child_samples=20,
                                             min_child_weight=0.001,
                                             min_split_gain=0.0,
                                             n_estimators=100, n_jobs=-1,
                                             num_leaves=31, objective=None,
                                             random_state=1784, reg_alpha=0.0,
                                             reg_lambda=0.0, subsample=1.0,
                                             subsample_for_bin=200000,
                                             subsample_freq=0))],
                 flatten_transform=True, n_jobs=-1, verbose=False,
                 voting='hard', weights=None), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.classification.oop.ClassificationExperiment object at 0x2a279fcd0>, system=True)
2023-10-31 14:09:58,718:INFO:Checking exceptions
2023-10-31 14:09:58,725:INFO:Preloading libraries
2023-10-31 14:09:58,763:INFO:Copying training dataset
2023-10-31 14:09:58,763:INFO:Plot type: pr
2023-10-31 14:09:59,487:INFO:Fitting Model
2023-10-31 14:09:59,487:WARNING:X does not have valid feature names, but ExtraTreesClassifier was fitted with feature names

2023-10-31 14:09:59,487:INFO:Scoring test/hold-out set
2023-10-31 14:10:00,823:INFO:Initializing plot_model()
2023-10-31 14:10:00,823:INFO:plot_model(plot=confusion_matrix, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), verbose=False, display=None, display_format=None, estimator=VotingClassifier(estimators=[('Extra Trees Classifier',
                              ExtraTreesClassifier(bootstrap=False,
                                                   ccp_alpha=0.0,
                                                   class_weight=None,
                                                   criterion='gini',
                                                   max_depth=None,
                                                   max_features='sqrt',
                                                   max_leaf_nodes=None,
                                                   max_samples=None,
                                                   min_impurity_decrease=0.0,
                                                   min_samples_leaf=1,
                                                   min_samples_split=2,
                                                   min_weight_fraction_leaf=0.0,
                                                   n_estimators=100, n_jobs=-1,
                                                   oob_score=False,
                                                   ra...
                                             importance_type='split',
                                             learning_rate=0.1, max_depth=-1,
                                             min_child_samples=20,
                                             min_child_weight=0.001,
                                             min_split_gain=0.0,
                                             n_estimators=100, n_jobs=-1,
                                             num_leaves=31, objective=None,
                                             random_state=1784, reg_alpha=0.0,
                                             reg_lambda=0.0, subsample=1.0,
                                             subsample_for_bin=200000,
                                             subsample_freq=0))],
                 flatten_transform=True, n_jobs=-1, verbose=False,
                 voting='hard', weights=None), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.classification.oop.ClassificationExperiment object at 0x2a279fcd0>, system=True)
2023-10-31 14:10:00,823:INFO:Checking exceptions
2023-10-31 14:10:00,829:INFO:Preloading libraries
2023-10-31 14:10:00,863:INFO:Copying training dataset
2023-10-31 14:10:00,863:INFO:Plot type: confusion_matrix
2023-10-31 14:10:01,582:INFO:Fitting Model
2023-10-31 14:10:01,582:WARNING:X does not have valid feature names, but ExtraTreesClassifier was fitted with feature names

2023-10-31 14:10:01,583:INFO:Scoring test/hold-out set
2023-10-31 14:10:01,919:INFO:Visual Rendered Successfully
2023-10-31 14:10:02,021:INFO:plot_model() successfully completed......................................
2023-10-31 14:10:25,107:INFO:Initializing finalize_model()
2023-10-31 14:10:25,107:INFO:finalize_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x2a279fcd0>, estimator=VotingClassifier(estimators=[('Extra Trees Classifier',
                              ExtraTreesClassifier(bootstrap=False,
                                                   ccp_alpha=0.0,
                                                   class_weight=None,
                                                   criterion='gini',
                                                   max_depth=None,
                                                   max_features='sqrt',
                                                   max_leaf_nodes=None,
                                                   max_samples=None,
                                                   min_impurity_decrease=0.0,
                                                   min_samples_leaf=1,
                                                   min_samples_split=2,
                                                   min_weight_fraction_leaf=0.0,
                                                   n_estimators=100, n_jobs=-1,
                                                   oob_score=False,
                                                   ra...
                                             importance_type='split',
                                             learning_rate=0.1, max_depth=-1,
                                             min_child_samples=20,
                                             min_child_weight=0.001,
                                             min_split_gain=0.0,
                                             n_estimators=100, n_jobs=-1,
                                             num_leaves=31, objective=None,
                                             random_state=1784, reg_alpha=0.0,
                                             reg_lambda=0.0, subsample=1.0,
                                             subsample_for_bin=200000,
                                             subsample_freq=0))],
                 flatten_transform=True, n_jobs=-1, verbose=False,
                 voting='hard', weights=None), fit_kwargs=None, groups=None, model_only=False, experiment_custom_tags=None)
2023-10-31 14:10:25,110:INFO:Finalizing VotingClassifier(estimators=[('Extra Trees Classifier',
                              ExtraTreesClassifier(bootstrap=False,
                                                   ccp_alpha=0.0,
                                                   class_weight=None,
                                                   criterion='gini',
                                                   max_depth=None,
                                                   max_features='sqrt',
                                                   max_leaf_nodes=None,
                                                   max_samples=None,
                                                   min_impurity_decrease=0.0,
                                                   min_samples_leaf=1,
                                                   min_samples_split=2,
                                                   min_weight_fraction_leaf=0.0,
                                                   n_estimators=100, n_jobs=-1,
                                                   oob_score=False,
                                                   ra...
                                             importance_type='split',
                                             learning_rate=0.1, max_depth=-1,
                                             min_child_samples=20,
                                             min_child_weight=0.001,
                                             min_split_gain=0.0,
                                             n_estimators=100, n_jobs=-1,
                                             num_leaves=31, objective=None,
                                             random_state=1784, reg_alpha=0.0,
                                             reg_lambda=0.0, subsample=1.0,
                                             subsample_for_bin=200000,
                                             subsample_freq=0))],
                 flatten_transform=True, n_jobs=-1, verbose=False,
                 voting='hard', weights=None)
2023-10-31 14:10:25,121:INFO:Initializing create_model()
2023-10-31 14:10:25,122:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x2a279fcd0>, estimator=VotingClassifier(estimators=[('Extra Trees Classifier',
                              ExtraTreesClassifier(bootstrap=False,
                                                   ccp_alpha=0.0,
                                                   class_weight=None,
                                                   criterion='gini',
                                                   max_depth=None,
                                                   max_features='sqrt',
                                                   max_leaf_nodes=None,
                                                   max_samples=None,
                                                   min_impurity_decrease=0.0,
                                                   min_samples_leaf=1,
                                                   min_samples_split=2,
                                                   min_weight_fraction_leaf=0.0,
                                                   n_estimators=100, n_jobs=-1,
                                                   oob_score=False,
                                                   ra...
                                             importance_type='split',
                                             learning_rate=0.1, max_depth=-1,
                                             min_child_samples=20,
                                             min_child_weight=0.001,
                                             min_split_gain=0.0,
                                             n_estimators=100, n_jobs=-1,
                                             num_leaves=31, objective=None,
                                             random_state=1784, reg_alpha=0.0,
                                             reg_lambda=0.0, subsample=1.0,
                                             subsample_for_bin=200000,
                                             subsample_freq=0))],
                 flatten_transform=True, n_jobs=-1, verbose=False,
                 voting='hard', weights=None), fold=None, round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=False, metrics=None, display=None, model_only=False, return_train_score=False, kwargs={})
2023-10-31 14:10:25,122:INFO:Checking exceptions
2023-10-31 14:10:25,123:INFO:Importing libraries
2023-10-31 14:10:25,123:INFO:Copying training dataset
2023-10-31 14:10:25,123:INFO:Defining folds
2023-10-31 14:10:25,123:INFO:Declaring metric variables
2023-10-31 14:10:25,124:INFO:Importing untrained model
2023-10-31 14:10:25,124:INFO:Declaring custom model
2023-10-31 14:10:25,125:INFO:Voting Classifier Imported successfully
2023-10-31 14:10:25,156:INFO:Cross validation set to False
2023-10-31 14:10:25,156:INFO:Fitting Model
2023-10-31 14:27:03,851:INFO:Initializing interpret_model()
2023-10-31 14:27:03,852:INFO:interpret_model(estimator=VotingClassifier(estimators=[('Extra Trees Classifier',
                              ExtraTreesClassifier(bootstrap=False,
                                                   ccp_alpha=0.0,
                                                   class_weight=None,
                                                   criterion='gini',
                                                   max_depth=None,
                                                   max_features='sqrt',
                                                   max_leaf_nodes=None,
                                                   max_samples=None,
                                                   min_impurity_decrease=0.0,
                                                   min_samples_leaf=1,
                                                   min_samples_split=2,
                                                   min_weight_fraction_leaf=0.0,
                                                   n_estimators=100, n_jobs=-1,
                                                   oob_score=False,
                                                   ra...
                                             importance_type='split',
                                             learning_rate=0.1, max_depth=-1,
                                             min_child_samples=20,
                                             min_child_weight=0.001,
                                             min_split_gain=0.0,
                                             n_estimators=100, n_jobs=-1,
                                             num_leaves=31, objective=None,
                                             random_state=1784, reg_alpha=0.0,
                                             reg_lambda=0.0, subsample=1.0,
                                             subsample_for_bin=200000,
                                             subsample_freq=0))],
                 flatten_transform=True, n_jobs=-1, verbose=False,
                 voting='hard', weights=None), use_train_data=False, X_new_sample=None, y_new_sample=None, feature=None, kwargs={}, observation=None, plot=summary, save=False, self=<pycaret.classification.oop.ClassificationExperiment object at 0x2a279fcd0>)
2023-10-31 14:27:03,852:INFO:Checking exceptions
2023-10-31 14:27:03,852:INFO:Soft dependency imported: shap: 0.43.0
